        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2412)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2412)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
22/06/09 12:25:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
22/06/09 12:25:51 INFO SparkContext: Running Spark version 3.0.0
22/06/09 12:25:51 INFO ResourceUtils: ==============================================================
22/06/09 12:25:51 INFO ResourceUtils: Resources for spark.driver:

22/06/09 12:25:51 INFO ResourceUtils: ==============================================================
22/06/09 12:25:51 INFO SparkContext: Submitted application: Spark%20Pipeline.py
22/06/09 12:25:51 INFO SecurityManager: Changing view acls to: Malcolm Lewis
22/06/09 12:25:51 INFO SecurityManager: Changing modify acls to: Malcolm Lewis
22/06/09 12:25:51 INFO SecurityManager: Changing view acls groups to:
22/06/09 12:25:51 INFO SecurityManager: Changing modify acls groups to:
22/06/09 12:25:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Malcolm Lewis); groups with view permissions: Set(); users  with modify permissions: Set(Malcolm Lewis); groups with modify permissions: Set()
22/06/09 12:25:53 INFO Utils: Successfully started service 'sparkDriver' on port 55513.
22/06/09 12:25:53 INFO SparkEnv: Registering MapOutputTracker
22/06/09 12:25:53 INFO SparkEnv: Registering BlockManagerMaster
22/06/09 12:25:53 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
22/06/09 12:25:53 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
22/06/09 12:25:53 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
22/06/09 12:25:53 INFO DiskBlockManager: Created local directory at C:\Users\Malcolm Lewis\AppData\Local\Temp\blockmgr-58a7ba63-912d-4b24-ab36-a694cba93c3b
22/06/09 12:25:53 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
22/06/09 12:25:53 INFO SparkEnv: Registering OutputCommitCoordinator
22/06/09 12:25:53 INFO Utils: Successfully started service 'SparkUI' on port 4040.
22/06/09 12:25:54 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://MalcolmSurfacePro4.myfiosgateway.com:4040
22/06/09 12:25:54 INFO Executor: Starting executor ID driver on host MalcolmSurfacePro4.myfiosgateway.com
22/06/09 12:25:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55538.
22/06/09 12:25:54 INFO NettyBlockTransferService: Server created on MalcolmSurfacePro4.myfiosgateway.com:55538
22/06/09 12:25:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
22/06/09 12:25:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 55538, None)
22/06/09 12:25:54 INFO BlockManagerMasterEndpoint: Registering block manager MalcolmSurfacePro4.myfiosgateway.com:55538 with 366.3 MiB RAM, BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 55538, None)
22/06/09 12:25:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 55538, None)
22/06/09 12:25:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 55538, None)
22/06/09 12:25:55 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/C:/Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/spark-warehouse').
22/06/09 12:25:55 INFO SharedState: Warehouse path is 'file:/C:/Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/spark-warehouse'.
22/06/09 12:25:57 INFO InMemoryFileIndex: It took 49 ms to list leaf files for 1 paths.
22/06/09 12:25:57 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
22/06/09 12:26:03 INFO FileSourceStrategy: Pruning directories with:
22/06/09 12:26:03 INFO FileSourceStrategy: Pushed Filters:
22/06/09 12:26:03 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
22/06/09 12:26:03 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
22/06/09 12:26:05 INFO CodeGenerator: Code generated in 638.7878 ms
22/06/09 12:26:05 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 285.2 KiB, free 366.0 MiB)
22/06/09 12:26:05 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 366.0 MiB)
22/06/09 12:26:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:55538 (size: 23.9 KiB, free: 366.3 MiB)
22/06/09 12:26:05 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
22/06/09 12:26:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/09 12:26:06 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
22/06/09 12:26:06 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/09 12:26:06 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
22/06/09 12:26:06 INFO DAGScheduler: Parents of final stage: List()
22/06/09 12:26:06 INFO DAGScheduler: Missing parents: List()
22/06/09 12:26:06 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/09 12:26:06 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.7 KiB, free 366.0 MiB)
22/06/09 12:26:06 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 366.0 MiB)
22/06/09 12:26:06 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:55538 (size: 5.3 KiB, free: 366.3 MiB)
22/06/09 12:26:06 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1200
22/06/09 12:26:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/09 12:26:06 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
22/06/09 12:26:06 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7781 bytes)
22/06/09 12:26:06 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
22/06/09 12:26:07 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/09 12:26:07 INFO CodeGenerator: Code generated in 75.4301 ms
22/06/09 12:26:07 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1759 bytes result sent to driver
22/06/09 12:26:07 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1525 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/09 12:26:07 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
22/06/09 12:26:08 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 2.114 s
22/06/09 12:26:08 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
22/06/09 12:26:08 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
22/06/09 12:26:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
22/06/09 12:26:08 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 2.249633 s
22/06/09 12:26:08 INFO CodeGenerator: Code generated in 37.5068 ms
22/06/09 12:26:08 INFO FileSourceStrategy: Pruning directories with:
22/06/09 12:26:08 INFO FileSourceStrategy: Pushed Filters:
22/06/09 12:26:08 INFO FileSourceStrategy: Post-Scan Filters:
22/06/09 12:26:08 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
22/06/09 12:26:08 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 285.2 KiB, free 365.7 MiB)
22/06/09 12:26:08 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.7 MiB)
22/06/09 12:26:08 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:55538 (size: 23.9 KiB, free: 366.2 MiB)
22/06/09 12:26:08 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
22/06/09 12:26:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/09 12:26:09 INFO FileSourceStrategy: Pruning directories with:
22/06/09 12:26:09 INFO FileSourceStrategy: Pushed Filters:
22/06/09 12:26:09 INFO FileSourceStrategy: Post-Scan Filters:
22/06/09 12:26:09 INFO FileSourceStrategy: Output Data Schema: struct<Date: string>
22/06/09 12:26:10 INFO CodeGenerator: Code generated in 27.9005 ms
22/06/09 12:26:10 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 285.2 KiB, free 365.4 MiB)
22/06/09 12:26:10 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.4 MiB)
22/06/09 12:26:10 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:55538 (size: 23.9 KiB, free: 366.2 MiB)
22/06/09 12:26:10 INFO SparkContext: Created broadcast 3 from showString at NativeMethodAccessorImpl.java:0
22/06/09 12:26:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/09 12:26:10 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
22/06/09 12:26:10 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/09 12:26:10 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)
22/06/09 12:26:10 INFO DAGScheduler: Parents of final stage: List()
22/06/09 12:26:10 INFO DAGScheduler: Missing parents: List()
22/06/09 12:26:10 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/09 12:26:10 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 12.1 KiB, free 365.4 MiB)
22/06/09 12:26:10 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 365.4 MiB)
22/06/09 12:26:10 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:55538 (size: 6.2 KiB, free: 366.2 MiB)
22/06/09 12:26:10 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1200
22/06/09 12:26:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/09 12:26:10 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
22/06/09 12:26:10 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7781 bytes)
22/06/09 12:26:10 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
22/06/09 12:26:10 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/09 12:26:10 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1543 bytes result sent to driver
22/06/09 12:26:10 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 213 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/09 12:26:10 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
22/06/09 12:26:10 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.432 s
22/06/09 12:26:10 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
22/06/09 12:26:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
22/06/09 12:26:10 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.598348 s
22/06/09 12:26:10 INFO CodeGenerator: Code generated in 69.9071 ms
+---------+
|     Date|
+---------+
| 1/5/2019|
| 3/8/2019|
| 3/3/2019|
|1/27/2019|
| 2/8/2019|
|3/25/2019|
|2/25/2019|
|2/24/2019|
|1/10/2019|
|2/20/2019|
+---------+

22/06/09 12:26:11 INFO FileSourceStrategy: Pruning directories with:
22/06/09 12:26:11 INFO FileSourceStrategy: Pushed Filters:
22/06/09 12:26:11 INFO FileSourceStrategy: Post-Scan Filters:
22/06/09 12:26:11 INFO FileSourceStrategy: Output Data Schema: struct<Date: string>
22/06/09 12:26:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/06/09 12:26:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
22/06/09 12:26:11 INFO CodeGenerator: Code generated in 61.3732 ms
22/06/09 12:26:11 INFO CodeGenerator: Code generated in 51.4172 ms
22/06/09 12:26:11 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 285.2 KiB, free 365.1 MiB)
22/06/09 12:26:11 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.1 MiB)
22/06/09 12:26:11 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:55538 (size: 23.9 KiB, free: 366.2 MiB)
22/06/09 12:26:11 INFO SparkContext: Created broadcast 5 from csv at NativeMethodAccessorImpl.java:0
22/06/09 12:26:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/09 12:26:12 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
22/06/09 12:26:12 INFO DAGScheduler: Registering RDD 17 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0
22/06/09 12:26:12 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/09 12:26:12 INFO DAGScheduler: Final stage: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0)
22/06/09 12:26:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
22/06/09 12:26:12 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
22/06/09 12:26:12 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[17] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/09 12:26:12 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 14.8 KiB, free 365.0 MiB)
22/06/09 12:26:12 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 365.0 MiB)
22/06/09 12:26:12 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:55538 (size: 7.5 KiB, free: 366.2 MiB)
22/06/09 12:26:12 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1200
22/06/09 12:26:12 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[17] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/09 12:26:12 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
22/06/09 12:26:12 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7770 bytes)
22/06/09 12:26:12 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
22/06/09 12:26:12 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/09 12:26:12 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1842 bytes result sent to driver
22/06/09 12:26:12 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 253 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/09 12:26:12 INFO DAGScheduler: ShuffleMapStage 2 (csv at NativeMethodAccessorImpl.java:0) finished in 0.362 s
22/06/09 12:26:12 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
22/06/09 12:26:12 INFO DAGScheduler: looking for newly runnable stages
22/06/09 12:26:12 INFO DAGScheduler: running: Set()
22/06/09 12:26:12 INFO DAGScheduler: waiting: Set(ResultStage 3)
22/06/09 12:26:12 INFO DAGScheduler: failed: Set()
22/06/09 12:26:12 INFO DAGScheduler: Submitting ResultStage 3 (CoalescedRDD[21] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/09 12:26:12 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 151.5 KiB, free 364.9 MiB)
22/06/09 12:26:12 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 364.8 MiB)
22/06/09 12:26:12 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:55538 (size: 54.8 KiB, free: 366.1 MiB)
22/06/09 12:26:12 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1200
22/06/09 12:26:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (CoalescedRDD[21] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/09 12:26:13 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
22/06/09 12:26:13 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, NODE_LOCAL, 7601 bytes)
22/06/09 12:26:13 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
22/06/09 12:26:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/06/09 12:26:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
22/06/09 12:26:13 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3)
java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220609122612_0003_m_000000_3\part-00000-0d7acc36-c018-47ab-a328-4591f33081f7-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
22/06/09 12:26:13 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220609122612_0003_m_000000_3\part-00000-0d7acc36-c018-47ab-a328-4591f33081f7-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

22/06/09 12:26:13 ERROR TaskSetManager: Task 0 in stage 3.0 failed 1 times; aborting job
22/06/09 12:26:13 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
22/06/09 12:26:13 INFO TaskSchedulerImpl: Cancelling stage 3
22/06/09 12:26:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage cancelled
22/06/09 12:26:13 INFO DAGScheduler: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0) failed in 0.794 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220609122612_0003_m_000000_3\part-00000-0d7acc36-c018-47ab-a328-4591f33081f7-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
22/06/09 12:26:13 INFO DAGScheduler: Job 2 failed: csv at NativeMethodAccessorImpl.java:0, took 1.424061 s
22/06/09 12:26:13 ERROR FileFormatWriter: Aborting job 695d9c80-c06a-4a56-9370-4fb5b0ae4a54.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220609122612_0003_m_000000_3\part-00000-0d7acc36-c018-47ab-a328-4591f33081f7-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)
        at scala.Option.foreach(Option.scala:407)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)
        at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)
        at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)
        at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
        at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)
        at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220609122612_0003_m_000000_3\part-00000-0d7acc36-c018-47ab-a328-4591f33081f7-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        ... 1 more
Traceback (most recent call last):
  File "C:/Users/Malcolm Lewis/Desktop/Pipeline Pet Project/Spark Pipeline.py", line 36, in <module>
    output.coalesce(1).write.mode('overwrite').csv("filtered.csv")
  File "C:\Python37\lib\site-packages\pyspark\python\lib\pyspark.zip\pyspark\sql\readwriter.py", line 1027, in csv
  File "C:\Python37\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1305, in __call__
  File "C:\Python37\lib\site-packages\pyspark\python\lib\pyspark.zip\pyspark\sql\utils.py", line 131, in deco
  File "C:\Python37\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o31.csv.
: org.apache.spark.SparkException: Job aborted.
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)
        at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)
        at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)
        at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
        at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)
        at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220609122612_0003_m_000000_3\part-00000-0d7acc36-c018-47ab-a328-4591f33081f7-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)
        at scala.Option.foreach(Option.scala:407)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)
        ... 33 more
Caused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220609122612_0003_m_000000_3\part-00000-0d7acc36-c018-47ab-a328-4591f33081f7-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        ... 1 more

22/06/09 12:26:13 INFO SparkContext: Invoking stop() from shutdown hook
22/06/09 12:26:13 INFO SparkUI: Stopped Spark web UI at http://MalcolmSurfacePro4.myfiosgateway.com:4040
22/06/09 12:26:14 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
22/06/09 12:26:14 INFO MemoryStore: MemoryStore cleared
22/06/09 12:26:14 INFO BlockManager: BlockManager stopped
22/06/09 12:26:14 INFO BlockManagerMaster: BlockManagerMaster stopped
22/06/09 12:26:14 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
22/06/09 12:26:14 INFO SparkContext: Successfully stopped SparkContext
22/06/09 12:26:14 INFO ShutdownHookManager: Shutdown hook called
22/06/09 12:26:14 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-68285b86-762f-434e-8c52-90ff9596382a
22/06/09 12:26:14 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-3c996c36-60a8-4d76-b260-690ef2ce5b8f\pyspark-342c1911-8ce7-476d-b1d0-e780e25661e0
22/06/09 12:26:14 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-3c996c36-60a8-4d76-b260-690ef2ce5b8f

C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project>spark-submit "Spark Pipeline.py"
22/06/09 12:30:47 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable C:\hadoop-common-2.2.0-bin-master\bin\winutils\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2412)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2412)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
22/06/09 12:30:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
22/06/09 12:30:51 INFO SparkContext: Running Spark version 3.0.0
22/06/09 12:30:51 INFO ResourceUtils: ==============================================================
22/06/09 12:30:51 INFO ResourceUtils: Resources for spark.driver:

22/06/09 12:30:51 INFO ResourceUtils: ==============================================================
22/06/09 12:30:51 INFO SparkContext: Submitted application: Spark%20Pipeline.py
22/06/09 12:30:51 INFO SecurityManager: Changing view acls to: Malcolm Lewis
22/06/09 12:30:51 INFO SecurityManager: Changing modify acls to: Malcolm Lewis
22/06/09 12:30:51 INFO SecurityManager: Changing view acls groups to:
22/06/09 12:30:51 INFO SecurityManager: Changing modify acls groups to:
22/06/09 12:30:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Malcolm Lewis); groups with view permissions: Set(); users  with modify permissions: Set(Malcolm Lewis); groups with modify permissions: Set()
22/06/09 12:30:55 INFO Utils: Successfully started service 'sparkDriver' on port 55672.
22/06/09 12:30:55 INFO SparkEnv: Registering MapOutputTracker
22/06/09 12:30:55 INFO SparkEnv: Registering BlockManagerMaster
22/06/09 12:30:55 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
22/06/09 12:30:55 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
22/06/09 12:30:55 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
22/06/09 12:30:55 INFO DiskBlockManager: Created local directory at C:\Users\Malcolm Lewis\AppData\Local\Temp\blockmgr-09064404-1683-43e3-9483-f05d42f59c26
22/06/09 12:30:55 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
22/06/09 12:30:55 INFO SparkEnv: Registering OutputCommitCoordinator
22/06/09 12:30:56 INFO Utils: Successfully started service 'SparkUI' on port 4040.
22/06/09 12:30:56 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://MalcolmSurfacePro4.myfiosgateway.com:4040
22/06/09 12:30:56 INFO Executor: Starting executor ID driver on host MalcolmSurfacePro4.myfiosgateway.com
22/06/09 12:30:57 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55695.
22/06/09 12:30:57 INFO NettyBlockTransferService: Server created on MalcolmSurfacePro4.myfiosgateway.com:55695
22/06/09 12:30:57 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
22/06/09 12:30:57 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 55695, None)
22/06/09 12:30:57 INFO BlockManagerMasterEndpoint: Registering block manager MalcolmSurfacePro4.myfiosgateway.com:55695 with 366.3 MiB RAM, BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 55695, None)
22/06/09 12:30:57 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 55695, None)
22/06/09 12:30:57 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 55695, None)
22/06/09 12:30:58 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/C:/Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/spark-warehouse').
22/06/09 12:30:58 INFO SharedState: Warehouse path is 'file:/C:/Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/spark-warehouse'.
22/06/09 12:31:00 INFO InMemoryFileIndex: It took 51 ms to list leaf files for 1 paths.
22/06/09 12:31:01 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
22/06/09 12:31:04 INFO FileSourceStrategy: Pruning directories with:
22/06/09 12:31:04 INFO FileSourceStrategy: Pushed Filters:
22/06/09 12:31:04 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
22/06/09 12:31:04 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
22/06/09 12:31:06 INFO CodeGenerator: Code generated in 579.9449 ms
22/06/09 12:31:06 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 285.2 KiB, free 366.0 MiB)
22/06/09 12:31:06 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 366.0 MiB)
22/06/09 12:31:06 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:55695 (size: 23.9 KiB, free: 366.3 MiB)
22/06/09 12:31:06 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
22/06/09 12:31:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/09 12:31:06 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
22/06/09 12:31:06 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/09 12:31:06 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
22/06/09 12:31:06 INFO DAGScheduler: Parents of final stage: List()
22/06/09 12:31:06 INFO DAGScheduler: Missing parents: List()
22/06/09 12:31:06 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/09 12:31:07 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.7 KiB, free 366.0 MiB)
22/06/09 12:31:07 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 366.0 MiB)
22/06/09 12:31:07 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:55695 (size: 5.3 KiB, free: 366.3 MiB)
22/06/09 12:31:07 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1200
22/06/09 12:31:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/09 12:31:07 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
22/06/09 12:31:07 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7781 bytes)
22/06/09 12:31:07 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
22/06/09 12:31:08 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/09 12:31:08 INFO CodeGenerator: Code generated in 53.5391 ms
22/06/09 12:31:08 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1759 bytes result sent to driver
22/06/09 12:31:08 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1439 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/09 12:31:08 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
22/06/09 12:31:09 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 2.001 s
22/06/09 12:31:09 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
22/06/09 12:31:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
22/06/09 12:31:09 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 2.137352 s
22/06/09 12:31:09 INFO CodeGenerator: Code generated in 28.8577 ms
22/06/09 12:31:09 INFO FileSourceStrategy: Pruning directories with:
22/06/09 12:31:09 INFO FileSourceStrategy: Pushed Filters:
22/06/09 12:31:09 INFO FileSourceStrategy: Post-Scan Filters:
22/06/09 12:31:09 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
22/06/09 12:31:09 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 285.2 KiB, free 365.7 MiB)
22/06/09 12:31:09 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.7 MiB)
22/06/09 12:31:09 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:55695 (size: 23.9 KiB, free: 366.2 MiB)
22/06/09 12:31:09 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
22/06/09 12:31:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/09 12:31:09 INFO FileSourceStrategy: Pruning directories with:
22/06/09 12:31:09 INFO FileSourceStrategy: Pushed Filters:
22/06/09 12:31:09 INFO FileSourceStrategy: Post-Scan Filters:
22/06/09 12:31:09 INFO FileSourceStrategy: Output Data Schema: struct<Invoice ID: string, Branch: string, City: string, Customer type: string, Gender: string ... 15 more fields>
22/06/09 12:31:11 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 285.2 KiB, free 365.4 MiB)
22/06/09 12:31:11 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.4 MiB)
22/06/09 12:31:11 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:55695 (size: 23.9 KiB, free: 366.2 MiB)
22/06/09 12:31:11 INFO SparkContext: Created broadcast 3 from showString at NativeMethodAccessorImpl.java:0
22/06/09 12:31:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/09 12:31:11 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
22/06/09 12:31:11 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/09 12:31:11 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)
22/06/09 12:31:11 INFO DAGScheduler: Parents of final stage: List()
22/06/09 12:31:11 INFO DAGScheduler: Missing parents: List()
22/06/09 12:31:11 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[15] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/09 12:31:11 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.6 KiB, free 365.4 MiB)
22/06/09 12:31:11 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 365.3 MiB)
22/06/09 12:31:11 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:55695 (size: 10.7 KiB, free: 366.2 MiB)
22/06/09 12:31:11 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1200
22/06/09 12:31:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[15] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/09 12:31:11 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
22/06/09 12:31:11 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7781 bytes)
22/06/09 12:31:11 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
22/06/09 12:31:11 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/09 12:31:11 INFO CodeGenerator: Code generated in 195.9949 ms
22/06/09 12:31:12 INFO MemoryStore: Block rdd_12_0 stored as values in memory (estimated size 58.7 KiB, free 365.3 MiB)
22/06/09 12:31:12 INFO BlockManagerInfo: Added rdd_12_0 in memory on MalcolmSurfacePro4.myfiosgateway.com:55695 (size: 58.7 KiB, free: 366.2 MiB)
22/06/09 12:31:12 INFO CodeGenerator: Code generated in 9.2539 ms
22/06/09 12:31:12 INFO CodeGenerator: Code generated in 160.9987 ms
22/06/09 12:31:12 INFO Executor: 1 block locks were not released by TID = 1:
[rdd_12_0]
22/06/09 12:31:12 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1619 bytes result sent to driver
22/06/09 12:31:12 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1161 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/09 12:31:12 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
22/06/09 12:31:12 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 1.316 s
22/06/09 12:31:12 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
22/06/09 12:31:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
22/06/09 12:31:12 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 1.350518 s
22/06/09 12:31:12 INFO CodeGenerator: Code generated in 23.9699 ms
+---------+
|     Date|
+---------+
| 1/5/2019|
| 3/8/2019|
| 3/3/2019|
|1/27/2019|
| 2/8/2019|
|3/25/2019|
|2/25/2019|
|2/24/2019|
|1/10/2019|
|2/20/2019|
+---------+

22/06/09 12:31:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/06/09 12:31:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
22/06/09 12:31:13 INFO CodeGenerator: Code generated in 153.3468 ms
22/06/09 12:31:13 INFO BlockManagerInfo: Removed broadcast_4_piece0 on MalcolmSurfacePro4.myfiosgateway.com:55695 in memory (size: 10.7 KiB, free: 366.2 MiB)
22/06/09 12:31:13 INFO CodeGenerator: Code generated in 21.7105 ms
22/06/09 12:31:13 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
22/06/09 12:31:13 INFO DAGScheduler: Registering RDD 19 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0
22/06/09 12:31:13 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/09 12:31:13 INFO DAGScheduler: Final stage: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0)
22/06/09 12:31:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
22/06/09 12:31:13 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
22/06/09 12:31:13 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/09 12:31:13 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 29.7 KiB, free 365.3 MiB)
22/06/09 12:31:13 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 12.9 KiB, free 365.3 MiB)
22/06/09 12:31:13 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:55695 (size: 12.9 KiB, free: 366.2 MiB)
22/06/09 12:31:13 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1200
22/06/09 12:31:13 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/09 12:31:13 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
22/06/09 12:31:13 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7770 bytes)
22/06/09 12:31:13 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
22/06/09 12:31:13 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
22/06/09 12:31:13 INFO BlockManager: Found block rdd_12_0 locally
22/06/09 12:31:13 INFO Executor: 1 block locks were not released by TID = 2:
[rdd_12_0]
22/06/09 12:31:13 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1990 bytes result sent to driver
22/06/09 12:31:13 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 191 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/09 12:31:13 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
22/06/09 12:31:13 INFO DAGScheduler: ShuffleMapStage 2 (csv at NativeMethodAccessorImpl.java:0) finished in 0.293 s
22/06/09 12:31:13 INFO DAGScheduler: looking for newly runnable stages
22/06/09 12:31:13 INFO DAGScheduler: running: Set()
22/06/09 12:31:13 INFO DAGScheduler: waiting: Set(ResultStage 3)
22/06/09 12:31:13 INFO DAGScheduler: failed: Set()
22/06/09 12:31:13 INFO DAGScheduler: Submitting ResultStage 3 (CoalescedRDD[23] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/09 12:31:14 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 151.5 KiB, free 365.1 MiB)
22/06/09 12:31:14 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 365.1 MiB)
22/06/09 12:31:14 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:55695 (size: 54.8 KiB, free: 366.1 MiB)
22/06/09 12:31:14 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1200
22/06/09 12:31:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (CoalescedRDD[23] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/09 12:31:14 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
22/06/09 12:31:14 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, NODE_LOCAL, 7601 bytes)
22/06/09 12:31:14 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
22/06/09 12:31:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/06/09 12:31:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
22/06/09 12:31:14 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3)
java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220609123113_0003_m_000000_3\part-00000-90bf0ebd-e8bf-46b7-ad03-83153aa6126d-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
22/06/09 12:31:14 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220609123113_0003_m_000000_3\part-00000-90bf0ebd-e8bf-46b7-ad03-83153aa6126d-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

22/06/09 12:31:15 ERROR TaskSetManager: Task 0 in stage 3.0 failed 1 times; aborting job
22/06/09 12:31:15 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
22/06/09 12:31:15 INFO TaskSchedulerImpl: Cancelling stage 3
22/06/09 12:31:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage cancelled
22/06/09 12:31:15 INFO DAGScheduler: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0) failed in 1.052 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220609123113_0003_m_000000_3\part-00000-90bf0ebd-e8bf-46b7-ad03-83153aa6126d-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
22/06/09 12:31:15 INFO DAGScheduler: Job 2 failed: csv at NativeMethodAccessorImpl.java:0, took 1.593216 s
22/06/09 12:31:15 ERROR FileFormatWriter: Aborting job 6295341a-a75b-4a3d-8fa2-eb575d58ea82.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220609123113_0003_m_000000_3\part-00000-90bf0ebd-e8bf-46b7-ad03-83153aa6126d-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)
        at scala.Option.foreach(Option.scala:407)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)
        at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)
        at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)
        at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
        at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)
        at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220609123113_0003_m_000000_3\part-00000-90bf0ebd-e8bf-46b7-ad03-83153aa6126d-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        ... 1 more
Traceback (most recent call last):
  File "C:/Users/Malcolm Lewis/Desktop/Pipeline Pet Project/Spark Pipeline.py", line 36, in <module>
    output.coalesce(1).write.mode('overwrite').csv("filtered.csv")
  File "C:\Python37\lib\site-packages\pyspark\python\lib\pyspark.zip\pyspark\sql\readwriter.py", line 1027, in csv
  File "C:\Python37\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1305, in __call__
  File "C:\Python37\lib\site-packages\pyspark\python\lib\pyspark.zip\pyspark\sql\utils.py", line 131, in deco
  File "C:\Python37\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o33.csv.
: org.apache.spark.SparkException: Job aborted.
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)
        at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)
        at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)
        at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
        at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)
        at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220609123113_0003_m_000000_3\part-00000-90bf0ebd-e8bf-46b7-ad03-83153aa6126d-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)
        at scala.Option.foreach(Option.scala:407)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)
        ... 33 more
Caused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220609123113_0003_m_000000_3\part-00000-90bf0ebd-e8bf-46b7-ad03-83153aa6126d-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        ... 1 more

22/06/09 12:31:15 INFO SparkContext: Invoking stop() from shutdown hook
22/06/09 12:31:15 INFO SparkUI: Stopped Spark web UI at http://MalcolmSurfacePro4.myfiosgateway.com:4040
22/06/09 12:31:15 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
22/06/09 12:31:15 INFO MemoryStore: MemoryStore cleared
22/06/09 12:31:15 INFO BlockManager: BlockManager stopped
22/06/09 12:31:15 INFO BlockManagerMaster: BlockManagerMaster stopped
22/06/09 12:31:15 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
22/06/09 12:31:15 INFO SparkContext: Successfully stopped SparkContext
22/06/09 12:31:15 INFO ShutdownHookManager: Shutdown hook called
22/06/09 12:31:15 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-0a4b108f-75fb-47c6-97b3-7c1f81f24b77\pyspark-a738bd50-d155-4769-8fca-766cd94cc70d
22/06/09 12:31:15 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-0a4b108f-75fb-47c6-97b3-7c1f81f24b77
22/06/09 12:31:15 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-f6a7b655-b2ef-4cc3-ab33-0b19c94f2dfb

C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project>spark-submit "Spark Pipeline.py"
22/06/09 12:32:13 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable C:\hadoop-common-2.2.0-bin-master\bin\winutils\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2412)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2412)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
22/06/09 12:32:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
22/06/09 12:32:17 INFO SparkContext: Running Spark version 3.0.0
22/06/09 12:32:17 INFO ResourceUtils: ==============================================================
22/06/09 12:32:17 INFO ResourceUtils: Resources for spark.driver:

22/06/09 12:32:17 INFO ResourceUtils: ==============================================================
22/06/09 12:32:17 INFO SparkContext: Submitted application: Spark%20Pipeline.py
22/06/09 12:32:17 INFO SecurityManager: Changing view acls to: Malcolm Lewis
22/06/09 12:32:17 INFO SecurityManager: Changing modify acls to: Malcolm Lewis
22/06/09 12:32:17 INFO SecurityManager: Changing view acls groups to:
22/06/09 12:32:17 INFO SecurityManager: Changing modify acls groups to:
22/06/09 12:32:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Malcolm Lewis); groups with view permissions: Set(); users  with modify permissions: Set(Malcolm Lewis); groups with modify permissions: Set()
22/06/09 12:32:23 INFO Utils: Successfully started service 'sparkDriver' on port 55728.
22/06/09 12:32:23 INFO SparkEnv: Registering MapOutputTracker
22/06/09 12:32:23 INFO SparkEnv: Registering BlockManagerMaster
22/06/09 12:32:23 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
22/06/09 12:32:23 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
22/06/09 12:32:23 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
22/06/09 12:32:23 INFO DiskBlockManager: Created local directory at C:\Users\Malcolm Lewis\AppData\Local\Temp\blockmgr-1e41a460-51f0-4840-ac0c-0b91827e60d7
22/06/09 12:32:23 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
22/06/09 12:32:24 INFO SparkEnv: Registering OutputCommitCoordinator
22/06/09 12:32:25 INFO Utils: Successfully started service 'SparkUI' on port 4040.
22/06/09 12:32:25 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://MalcolmSurfacePro4.myfiosgateway.com:4040
22/06/09 12:32:27 INFO Executor: Starting executor ID driver on host MalcolmSurfacePro4.myfiosgateway.com
22/06/09 12:32:27 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55751.
22/06/09 12:32:27 INFO NettyBlockTransferService: Server created on MalcolmSurfacePro4.myfiosgateway.com:55751
22/06/09 12:32:27 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
22/06/09 12:32:27 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 55751, None)
22/06/09 12:32:28 INFO BlockManagerMasterEndpoint: Registering block manager MalcolmSurfacePro4.myfiosgateway.com:55751 with 366.3 MiB RAM, BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 55751, None)
22/06/09 12:32:28 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 55751, None)
22/06/09 12:32:28 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 55751, None)
22/06/09 12:32:31 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/C:/Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/spark-warehouse').
22/06/09 12:32:31 INFO SharedState: Warehouse path is 'file:/C:/Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/spark-warehouse'.
22/06/09 12:32:34 INFO InMemoryFileIndex: It took 76 ms to list leaf files for 1 paths.
22/06/09 12:32:34 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 1 paths.
22/06/09 12:32:41 INFO FileSourceStrategy: Pruning directories with:
22/06/09 12:32:41 INFO FileSourceStrategy: Pushed Filters:
22/06/09 12:32:41 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
22/06/09 12:32:41 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
22/06/09 12:32:43 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
22/06/09 12:32:44 INFO CodeGenerator: Code generated in 1077.0273 ms
22/06/09 12:32:44 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 285.2 KiB, free 366.0 MiB)
22/06/09 12:32:44 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 366.0 MiB)
22/06/09 12:32:44 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:55751 (size: 23.9 KiB, free: 366.3 MiB)
22/06/09 12:32:45 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
22/06/09 12:32:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/09 12:32:45 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
22/06/09 12:32:45 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/09 12:32:45 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
22/06/09 12:32:45 INFO DAGScheduler: Parents of final stage: List()
22/06/09 12:32:45 INFO DAGScheduler: Missing parents: List()
22/06/09 12:32:45 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/09 12:32:46 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.7 KiB, free 366.0 MiB)
22/06/09 12:32:46 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 366.0 MiB)
22/06/09 12:32:46 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:55751 (size: 5.3 KiB, free: 366.3 MiB)
22/06/09 12:32:46 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1200
22/06/09 12:32:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/09 12:32:46 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
22/06/09 12:32:46 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7781 bytes)
22/06/09 12:32:46 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
22/06/09 12:32:48 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/09 12:32:48 INFO CodeGenerator: Code generated in 68.9926 ms
22/06/09 12:32:49 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1759 bytes result sent to driver
22/06/09 12:32:49 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3051 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/09 12:32:49 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
22/06/09 12:32:49 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 3.836 s
22/06/09 12:32:49 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
22/06/09 12:32:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
22/06/09 12:32:49 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 4.252222 s
22/06/09 12:32:49 INFO CodeGenerator: Code generated in 84.272 ms
22/06/09 12:32:50 INFO FileSourceStrategy: Pruning directories with:
22/06/09 12:32:50 INFO FileSourceStrategy: Pushed Filters:
22/06/09 12:32:50 INFO FileSourceStrategy: Post-Scan Filters:
22/06/09 12:32:50 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
22/06/09 12:32:50 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 285.2 KiB, free 365.7 MiB)
22/06/09 12:32:50 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.7 MiB)
22/06/09 12:32:50 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:55751 (size: 23.9 KiB, free: 366.2 MiB)
22/06/09 12:32:50 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
22/06/09 12:32:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/09 12:32:51 INFO FileSourceStrategy: Pruning directories with:
22/06/09 12:32:51 INFO FileSourceStrategy: Pushed Filters:
22/06/09 12:32:51 INFO FileSourceStrategy: Post-Scan Filters:
22/06/09 12:32:51 INFO FileSourceStrategy: Output Data Schema: struct<Invoice ID: string, Branch: string, City: string, Customer type: string, Gender: string ... 15 more fields>
22/06/09 12:32:53 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 285.2 KiB, free 365.4 MiB)
22/06/09 12:32:53 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.4 MiB)
22/06/09 12:32:53 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:55751 (size: 23.9 KiB, free: 366.2 MiB)
22/06/09 12:32:53 INFO SparkContext: Created broadcast 3 from showString at NativeMethodAccessorImpl.java:0
22/06/09 12:32:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/09 12:32:53 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
22/06/09 12:32:53 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/09 12:32:53 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)
22/06/09 12:32:53 INFO DAGScheduler: Parents of final stage: List()
22/06/09 12:32:53 INFO DAGScheduler: Missing parents: List()
22/06/09 12:32:53 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[15] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/09 12:32:54 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.6 KiB, free 365.4 MiB)
22/06/09 12:32:54 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 365.3 MiB)
22/06/09 12:32:54 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:55751 (size: 10.7 KiB, free: 366.2 MiB)
22/06/09 12:32:54 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1200
22/06/09 12:32:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[15] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/09 12:32:54 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
22/06/09 12:32:54 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7781 bytes)
22/06/09 12:32:54 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
22/06/09 12:32:54 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/09 12:32:54 INFO CodeGenerator: Code generated in 207.2969 ms
22/06/09 12:32:55 INFO MemoryStore: Block rdd_12_0 stored as values in memory (estimated size 91.3 KiB, free 365.3 MiB)
22/06/09 12:32:55 INFO BlockManagerInfo: Added rdd_12_0 in memory on MalcolmSurfacePro4.myfiosgateway.com:55751 (size: 91.3 KiB, free: 366.1 MiB)
22/06/09 12:32:55 INFO CodeGenerator: Code generated in 14.6519 ms
22/06/09 12:32:55 INFO CodeGenerator: Code generated in 110.2257 ms
22/06/09 12:32:55 INFO Executor: 1 block locks were not released by TID = 1:
[rdd_12_0]
22/06/09 12:32:55 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1619 bytes result sent to driver
22/06/09 12:32:56 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1886 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/09 12:32:56 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
22/06/09 12:32:56 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 2.057 s
22/06/09 12:32:56 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
22/06/09 12:32:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
22/06/09 12:32:56 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 2.159341 s
22/06/09 12:32:56 INFO CodeGenerator: Code generated in 100.2787 ms
+---------+
|     Date|
+---------+
| 1/5/2019|
| 3/8/2019|
| 3/3/2019|
|1/27/2019|
| 2/8/2019|
|3/25/2019|
|2/25/2019|
|2/24/2019|
|1/10/2019|
|2/20/2019|
+---------+

22/06/09 12:32:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/06/09 12:32:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
22/06/09 12:32:57 INFO CodeGenerator: Code generated in 83.9021 ms
22/06/09 12:32:57 INFO CodeGenerator: Code generated in 62.1721 ms
22/06/09 12:32:57 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
22/06/09 12:32:57 INFO DAGScheduler: Registering RDD 19 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0
22/06/09 12:32:57 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/09 12:32:57 INFO DAGScheduler: Final stage: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0)
22/06/09 12:32:57 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
22/06/09 12:32:57 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
22/06/09 12:32:57 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/09 12:32:58 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 29.7 KiB, free 365.2 MiB)
22/06/09 12:32:58 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 12.9 KiB, free 365.2 MiB)
22/06/09 12:32:58 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:55751 (size: 12.9 KiB, free: 366.1 MiB)
22/06/09 12:32:58 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1200
22/06/09 12:32:58 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/09 12:32:58 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
22/06/09 12:32:58 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7770 bytes)
22/06/09 12:32:58 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
22/06/09 12:32:58 INFO BlockManager: Found block rdd_12_0 locally
22/06/09 12:32:58 INFO Executor: 1 block locks were not released by TID = 2:
[rdd_12_0]
22/06/09 12:32:58 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1990 bytes result sent to driver
22/06/09 12:32:58 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 586 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/09 12:32:58 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
22/06/09 12:32:58 INFO DAGScheduler: ShuffleMapStage 2 (csv at NativeMethodAccessorImpl.java:0) finished in 0.954 s
22/06/09 12:32:58 INFO BlockManagerInfo: Removed broadcast_4_piece0 on MalcolmSurfacePro4.myfiosgateway.com:55751 in memory (size: 10.7 KiB, free: 366.1 MiB)
22/06/09 12:32:58 INFO DAGScheduler: looking for newly runnable stages
22/06/09 12:32:58 INFO DAGScheduler: running: Set()
22/06/09 12:32:58 INFO DAGScheduler: waiting: Set(ResultStage 3)
22/06/09 12:32:58 INFO DAGScheduler: failed: Set()
22/06/09 12:32:59 INFO DAGScheduler: Submitting ResultStage 3 (CoalescedRDD[23] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/09 12:32:59 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 151.5 KiB, free 365.1 MiB)
22/06/09 12:32:59 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 365.0 MiB)
22/06/09 12:32:59 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:55751 (size: 54.8 KiB, free: 366.1 MiB)
22/06/09 12:32:59 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1200
22/06/09 12:32:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (CoalescedRDD[23] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/09 12:32:59 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
22/06/09 12:32:59 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, NODE_LOCAL, 7601 bytes)
22/06/09 12:32:59 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
22/06/09 12:32:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/06/09 12:32:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
22/06/09 12:32:59 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3)
java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220609123257_0003_m_000000_3\part-00000-3ee0ea57-4cd0-4ec9-9a4e-d5894116d7b0-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
22/06/09 12:32:59 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220609123257_0003_m_000000_3\part-00000-3ee0ea57-4cd0-4ec9-9a4e-d5894116d7b0-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

22/06/09 12:33:00 ERROR TaskSetManager: Task 0 in stage 3.0 failed 1 times; aborting job
22/06/09 12:33:00 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
22/06/09 12:33:00 INFO TaskSchedulerImpl: Cancelling stage 3
22/06/09 12:33:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage cancelled
22/06/09 12:33:00 INFO DAGScheduler: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0) failed in 1.512 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220609123257_0003_m_000000_3\part-00000-3ee0ea57-4cd0-4ec9-9a4e-d5894116d7b0-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
22/06/09 12:33:00 INFO DAGScheduler: Job 2 failed: csv at NativeMethodAccessorImpl.java:0, took 2.772821 s
22/06/09 12:33:00 ERROR FileFormatWriter: Aborting job 83d78a45-e184-4748-96f4-9539159b86a4.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220609123257_0003_m_000000_3\part-00000-3ee0ea57-4cd0-4ec9-9a4e-d5894116d7b0-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)
        at scala.Option.foreach(Option.scala:407)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)
        at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)
        at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)
        at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
        at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)
        at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220609123257_0003_m_000000_3\part-00000-3ee0ea57-4cd0-4ec9-9a4e-d5894116d7b0-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        ... 1 more
Traceback (most recent call last):
  File "C:/Users/Malcolm Lewis/Desktop/Pipeline Pet Project/Spark Pipeline.py", line 36, in <module>
    output.coalesce(1).write.mode('overwrite').csv("filtered.csv")
  File "C:\Python37\lib\site-packages\pyspark\python\lib\pyspark.zip\pyspark\sql\readwriter.py", line 1027, in csv
  File "C:\Python37\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1305, in __call__
  File "C:\Python37\lib\site-packages\pyspark\python\lib\pyspark.zip\pyspark\sql\utils.py", line 131, in deco
  File "C:\Python37\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o32.csv.
: org.apache.spark.SparkException: Job aborted.
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)
        at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)
        at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)
        at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
        at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)
        at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220609123257_0003_m_000000_3\part-00000-3ee0ea57-4cd0-4ec9-9a4e-d5894116d7b0-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)
        at scala.Option.foreach(Option.scala:407)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)
        ... 33 more
Caused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\M22/06/09 12:33:01 INFO SparkContext: Invoking stop() from shutdown hook
alcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220609123257_0003_m_000000_3\part-00000-3ee0ea57-4cd0-4ec9-9a4e-d5894116d7b0-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        ... 1 more

22/06/09 12:33:01 INFO SparkUI: Stopped Spark web UI at http://MalcolmSurfacePro4.myfiosgateway.com:4040
22/06/09 12:33:01 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
22/06/09 12:33:02 INFO MemoryStore: MemoryStore cleared
22/06/09 12:33:02 INFO BlockManager: BlockManager stopped
22/06/09 12:33:02 INFO BlockManagerMaster: BlockManagerMaster stopped
22/06/09 12:33:02 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
22/06/09 12:33:02 INFO SparkContext: Successfully stopped SparkContext
22/06/09 12:33:02 INFO ShutdownHookManager: Shutdown hook called
22/06/09 12:33:02 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-8e5834eb-bd2b-4780-81be-ee27f87194b4
22/06/09 12:33:02 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-a266f9f7-06e3-416d-a5f6-99706524271a
22/06/09 12:33:02 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-a266f9f7-06e3-416d-a5f6-99706524271a\pyspark-bfb87d56-aa58-4095-be4e-b0507c15b85e

C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project>spark-submit "Spark Pipeline.py"
22/06/09 12:33:20 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable C:\hadoop-common-2.2.0-bin-master\bin\winutils\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2412)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2412)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
22/06/09 12:33:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
22/06/09 12:33:22 INFO SparkContext: Running Spark version 3.0.0
22/06/09 12:33:22 INFO ResourceUtils: ==============================================================
22/06/09 12:33:22 INFO ResourceUtils: Resources for spark.driver:

22/06/09 12:33:22 INFO ResourceUtils: ==============================================================
22/06/09 12:33:22 INFO SparkContext: Submitted application: Spark%20Pipeline.py
22/06/09 12:33:22 INFO SecurityManager: Changing view acls to: Malcolm Lewis
22/06/09 12:33:22 INFO SecurityManager: Changing modify acls to: Malcolm Lewis
22/06/09 12:33:22 INFO SecurityManager: Changing view acls groups to:
22/06/09 12:33:22 INFO SecurityManager: Changing modify acls groups to:
22/06/09 12:33:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Malcolm Lewis); groups with view permissions: Set(); users  with modify permissions: Set(Malcolm Lewis); groups with modify permissions: Set()
22/06/09 12:33:25 INFO Utils: Successfully started service 'sparkDriver' on port 55793.
22/06/09 12:33:25 INFO SparkEnv: Registering MapOutputTracker
22/06/09 12:33:25 INFO SparkEnv: Registering BlockManagerMaster
22/06/09 12:33:25 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
22/06/09 12:33:25 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
22/06/09 12:33:25 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
22/06/09 12:33:25 INFO DiskBlockManager: Created local directory at C:\Users\Malcolm Lewis\AppData\Local\Temp\blockmgr-273fd2be-66f5-48f4-8821-aef0cadfe109
22/06/09 12:33:25 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
22/06/09 12:33:25 INFO SparkEnv: Registering OutputCommitCoordinator
22/06/09 12:33:25 INFO Utils: Successfully started service 'SparkUI' on port 4040.
22/06/09 12:33:26 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://MalcolmSurfacePro4.myfiosgateway.com:4040
22/06/09 12:33:26 INFO Executor: Starting executor ID driver on host MalcolmSurfacePro4.myfiosgateway.com
22/06/09 12:33:26 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55816.
22/06/09 12:33:26 INFO NettyBlockTransferService: Server created on MalcolmSurfacePro4.myfiosgateway.com:55816
22/06/09 12:33:26 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
22/06/09 12:33:26 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 55816, None)
22/06/09 12:33:26 INFO BlockManagerMasterEndpoint: Registering block manager MalcolmSurfacePro4.myfiosgateway.com:55816 with 366.3 MiB RAM, BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 55816, None)
22/06/09 12:33:26 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 55816, None)
22/06/09 12:33:26 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 55816, None)
22/06/09 12:33:28 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/C:/Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/spark-warehouse').
22/06/09 12:33:28 INFO SharedState: Warehouse path is 'file:/C:/Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/spark-warehouse'.
22/06/09 12:33:30 INFO InMemoryFileIndex: It took 60 ms to list leaf files for 1 paths.
22/06/09 12:33:30 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.
22/06/09 12:33:35 INFO FileSourceStrategy: Pruning directories with:
22/06/09 12:33:42 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
22/06/09 12:36:13 INFO FileSourceStrategy: Pushed Filters:
22/06/09 12:36:13 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
22/06/09 12:36:13 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
22/06/09 12:36:15 INFO CodeGenerator: Code generated in 613.6835 ms
22/06/09 12:36:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 285.2 KiB, free 366.0 MiB)
22/06/09 12:36:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 366.0 MiB)
22/06/09 12:36:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:55816 (size: 23.9 KiB, free: 366.3 MiB)
22/06/09 12:36:15 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
22/06/09 12:36:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/09 12:36:16 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
22/06/09 12:36:16 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/09 12:36:16 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
22/06/09 12:36:16 INFO DAGScheduler: Parents of final stage: List()
22/06/09 12:36:16 INFO DAGScheduler: Missing parents: List()
22/06/09 12:36:16 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/09 12:36:16 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.7 KiB, free 366.0 MiB)
22/06/09 12:36:16 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 366.0 MiB)
22/06/09 12:36:16 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:55816 (size: 5.3 KiB, free: 366.3 MiB)
22/06/09 12:36:16 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1200
22/06/09 12:36:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/09 12:36:16 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
22/06/09 12:36:16 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7781 bytes)
22/06/09 12:36:16 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
22/06/09 12:36:17 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/09 12:36:17 INFO CodeGenerator: Code generated in 35.779 ms
22/06/09 12:36:17 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1759 bytes result sent to driver
22/06/09 12:36:17 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1189 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/09 12:36:17 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
22/06/09 12:36:17 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 1.551 s
22/06/09 12:36:17 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
22/06/09 12:36:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
22/06/09 12:36:17 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 1.716308 s
22/06/09 12:36:17 INFO CodeGenerator: Code generated in 55.4518 ms
22/06/09 12:36:18 INFO FileSourceStrategy: Pruning directories with:
22/06/09 12:36:18 INFO FileSourceStrategy: Pushed Filters:
22/06/09 12:36:18 INFO FileSourceStrategy: Post-Scan Filters:
22/06/09 12:36:18 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
22/06/09 12:36:18 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 285.2 KiB, free 365.7 MiB)
22/06/09 12:36:18 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.7 MiB)
22/06/09 12:36:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:55816 (size: 23.9 KiB, free: 366.2 MiB)
22/06/09 12:36:18 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
22/06/09 12:36:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/09 12:36:18 INFO FileSourceStrategy: Pruning directories with:
22/06/09 12:36:19 INFO FileSourceStrategy: Pushed Filters:
22/06/09 12:36:19 INFO FileSourceStrategy: Post-Scan Filters:
22/06/09 12:36:19 INFO FileSourceStrategy: Output Data Schema: struct<Invoice ID: string, Branch: string, City: string, Customer type: string, Gender: string ... 15 more fields>
22/06/09 12:36:21 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 285.2 KiB, free 365.4 MiB)
22/06/09 12:36:21 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.4 MiB)
22/06/09 12:36:21 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:55816 (size: 23.9 KiB, free: 366.2 MiB)
22/06/09 12:36:21 INFO SparkContext: Created broadcast 3 from showString at NativeMethodAccessorImpl.java:0
22/06/09 12:36:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/09 12:36:21 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
22/06/09 12:36:21 INFO DAGScheduler: Registering RDD 16 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 0
22/06/09 12:36:21 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/09 12:36:21 INFO DAGScheduler: Final stage: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0)
22/06/09 12:36:21 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
22/06/09 12:36:21 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)
22/06/09 12:36:21 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[16] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/09 12:36:22 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 28.3 KiB, free 365.4 MiB)
22/06/09 12:36:22 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 365.3 MiB)
22/06/09 12:36:22 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:55816 (size: 12.3 KiB, free: 366.2 MiB)
22/06/09 12:36:22 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1200
22/06/09 12:36:22 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[16] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/09 12:36:22 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
22/06/09 12:36:22 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7770 bytes)
22/06/09 12:36:22 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
22/06/09 12:36:22 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/09 12:36:22 INFO CodeGenerator: Code generated in 257.1944 ms
22/06/09 12:36:23 INFO MemoryStore: Block rdd_12_0 stored as values in memory (estimated size 91.3 KiB, free 365.3 MiB)
22/06/09 12:36:23 INFO BlockManagerInfo: Added rdd_12_0 in memory on MalcolmSurfacePro4.myfiosgateway.com:55816 (size: 91.3 KiB, free: 366.1 MiB)
22/06/09 12:36:24 INFO CodeGenerator: Code generated in 16.2009 ms
22/06/09 12:36:24 INFO CodeGenerator: Code generated in 178.8971 ms
22/06/09 12:36:24 INFO Executor: 1 block locks were not released by TID = 1:
[rdd_12_0]
22/06/09 12:36:24 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2393 bytes result sent to driver
22/06/09 12:36:24 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2729 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/09 12:36:24 INFO DAGScheduler: ShuffleMapStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 3.137 s
22/06/09 12:36:24 INFO DAGScheduler: looking for newly runnable stages
22/06/09 12:36:25 INFO DAGScheduler: running: Set()
22/06/09 12:36:24 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
22/06/09 12:36:25 INFO DAGScheduler: waiting: Set(ResultStage 2)
22/06/09 12:36:25 INFO DAGScheduler: failed: Set()
22/06/09 12:36:25 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[22] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/09 12:36:25 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 31.1 KiB, free 365.2 MiB)
22/06/09 12:36:25 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 13.2 KiB, free 365.2 MiB)
22/06/09 12:36:25 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:55816 (size: 13.2 KiB, free: 366.1 MiB)
22/06/09 12:36:25 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1200
22/06/09 12:36:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[22] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/09 12:36:25 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
22/06/09 12:36:25 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
22/06/09 12:36:25 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
22/06/09 12:36:25 INFO ShuffleBlockFetcherIterator: Getting 1 (156.0 B) non-empty blocks including 1 (156.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
22/06/09 12:36:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 37 ms
22/06/09 12:36:25 INFO MemoryStore: Block rdd_19_0 stored as values in memory (estimated size 456.0 B, free 365.2 MiB)
22/06/09 12:36:25 INFO BlockManagerInfo: Added rdd_19_0 in memory on MalcolmSurfacePro4.myfiosgateway.com:55816 (size: 456.0 B, free: 366.1 MiB)
22/06/09 12:36:25 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2475 bytes result sent to driver
22/06/09 12:36:25 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 452 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/09 12:36:25 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
22/06/09 12:36:25 INFO DAGScheduler: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 0.634 s
22/06/09 12:36:25 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
22/06/09 12:36:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
22/06/09 12:36:25 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 4.210561 s
22/06/09 12:36:26 INFO CodeGenerator: Code generated in 126.3904 ms
+---------+
|     Date|
+---------+
| 1/5/2019|
| 3/8/2019|
| 3/3/2019|
|1/27/2019|
| 2/8/2019|
|3/25/2019|
|2/25/2019|
|2/24/2019|
|1/10/2019|
|2/20/2019|
+---------+

22/06/09 12:36:26 INFO BlockManagerInfo: Removed broadcast_5_piece0 on MalcolmSurfacePro4.myfiosgateway.com:55816 in memory (size: 13.2 KiB, free: 366.1 MiB)
22/06/09 12:36:26 INFO BlockManagerInfo: Removed broadcast_4_piece0 on MalcolmSurfacePro4.myfiosgateway.com:55816 in memory (size: 12.3 KiB, free: 366.1 MiB)
22/06/09 12:36:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/06/09 12:36:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
22/06/09 12:36:27 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
22/06/09 12:36:27 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/09 12:36:27 INFO DAGScheduler: Final stage: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0)
22/06/09 12:36:27 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
22/06/09 12:36:27 INFO DAGScheduler: Missing parents: List()
22/06/09 12:36:27 INFO DAGScheduler: Submitting ResultStage 4 (CoalescedRDD[25] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/09 12:36:27 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 171.7 KiB, free 365.1 MiB)
22/06/09 12:36:27 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 61.9 KiB, free 365.1 MiB)
22/06/09 12:36:27 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:55816 (size: 61.9 KiB, free: 366.1 MiB)
22/06/09 12:36:27 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1200
22/06/09 12:36:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (CoalescedRDD[25] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/09 12:36:27 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
22/06/09 12:36:28 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, NODE_LOCAL, 7601 bytes)
22/06/09 12:36:28 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)
22/06/09 12:36:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/06/09 12:36:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
22/06/09 12:36:28 ERROR Executor: Exception in task 0.0 in stage 4.0 (TID 3)
java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220609123626_0004_m_000000_3\part-00000-cbfc5424-cf8f-461e-a32b-5d4d0bed9cc2-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
22/06/09 12:36:28 WARN TaskSetManager: Lost task 0.0 in stage 4.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220609123626_0004_m_000000_3\part-00000-cbfc5424-cf8f-461e-a32b-5d4d0bed9cc2-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

22/06/09 12:36:28 ERROR TaskSetManager: Task 0 in stage 4.0 failed 1 times; aborting job
22/06/09 12:36:28 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
22/06/09 12:36:29 INFO TaskSchedulerImpl: Cancelling stage 4
22/06/09 12:36:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage cancelled
22/06/09 12:36:29 INFO DAGScheduler: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0) failed in 1.578 s due to Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220609123626_0004_m_000000_3\part-00000-cbfc5424-cf8f-461e-a32b-5d4d0bed9cc2-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
22/06/09 12:36:29 INFO DAGScheduler: Job 2 failed: csv at NativeMethodAccessorImpl.java:0, took 1.911135 s
22/06/09 12:36:29 ERROR FileFormatWriter: Aborting job fb798d30-9223-4441-812a-3008b34a3310.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220609123626_0004_m_000000_3\part-00000-cbfc5424-cf8f-461e-a32b-5d4d0bed9cc2-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)
        at scala.Option.foreach(Option.scala:407)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)
        at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)
        at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)
        at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
        at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)
        at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220609123626_0004_m_000000_3\part-00000-cbfc5424-cf8f-461e-a32b-5d4d0bed9cc2-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        ... 1 more
Traceback (most recent call last):
  File "C:/Users/Malcolm Lewis/Desktop/Pipeline Pet Project/Spark Pipeline.py", line 36, in <module>
    output.coalesce(1).write.mode('overwrite').csv("filtered.csv")
  File "C:\Python37\lib\site-packages\pyspark\python\lib\pyspark.zip\pyspark\sql\readwriter.py", line 1027, in csv
  File "C:\Python37\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1305, in __call__
  File "C:\Python37\lib\site-packages\pyspark\python\lib\pyspark.zip\pyspark\sql\utils.py", line 131, in deco
  File "C:\Python37\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o33.csv.
: org.apache.spark.SparkException: Job aborted.
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)
        at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)
        at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)
        at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
        at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)
        at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220609123626_0004_m_000000_3\part-00000-cbfc5424-cf8f-461e-a32b-5d4d0bed9cc2-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)
        at scala.Option.foreach(Option.scala:407)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)
        ... 33 more
Caused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220609123626_0004_m_000000_3\part-00000-cbfc5424-cf8f-461e-a32b-5d4d0bed9cc2-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        ... 1 more

22/06/09 12:36:30 INFO SparkContext: Invoking stop() from shutdown hook
22/06/09 12:36:30 INFO SparkUI: Stopped Spark web UI at http://MalcolmSurfacePro4.myfiosgateway.com:4040
22/06/09 12:36:30 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
22/06/09 12:36:30 INFO MemoryStore: MemoryStore cleared
22/06/09 12:36:30 INFO BlockManager: BlockManager stopped
22/06/09 12:36:30 INFO BlockManagerMaster: BlockManagerMaster stopped
22/06/09 12:36:30 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
22/06/09 12:36:30 INFO SparkContext: Successfully stopped SparkContext
22/06/09 12:36:30 INFO ShutdownHookManager: Shutdown hook called
22/06/09 12:36:30 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-a1b74b67-8fea-44e8-8b40-df8a32b1417a
22/06/09 12:36:30 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-a1b74b67-8fea-44e8-8b40-df8a32b1417a\pyspark-721988ba-976f-437f-b56b-e06a4095872f
22/06/09 12:36:30 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-29a7f19c-84d6-4eb9-b59c-284a5e089e4d

C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project>spark-submit "Spark Pipeline.py"
22/06/10 00:02:20 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable C:\hadoop-common-2.2.0-bin-master\bin\winutils\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2412)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2412)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
22/06/10 00:02:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
22/06/10 00:02:22 INFO SparkContext: Running Spark version 3.0.0
22/06/10 00:02:22 INFO ResourceUtils: ==============================================================
22/06/10 00:02:22 INFO ResourceUtils: Resources for spark.driver:

22/06/10 00:02:22 INFO ResourceUtils: ==============================================================
22/06/10 00:02:22 INFO SparkContext: Submitted application: Spark%20Pipeline.py
22/06/10 00:02:23 INFO SecurityManager: Changing view acls to: Malcolm Lewis
22/06/10 00:02:23 INFO SecurityManager: Changing modify acls to: Malcolm Lewis
22/06/10 00:02:23 INFO SecurityManager: Changing view acls groups to:
22/06/10 00:02:23 INFO SecurityManager: Changing modify acls groups to:
22/06/10 00:02:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Malcolm Lewis); groups with view permissions: Set(); users  with modify permissions: Set(Malcolm Lewis); groups with modify permissions: Set()
22/06/10 00:02:24 INFO Utils: Successfully started service 'sparkDriver' on port 50004.
22/06/10 00:02:24 INFO SparkEnv: Registering MapOutputTracker
22/06/10 00:02:24 INFO SparkEnv: Registering BlockManagerMaster
22/06/10 00:02:24 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
22/06/10 00:02:24 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
22/06/10 00:02:24 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
22/06/10 00:02:24 INFO DiskBlockManager: Created local directory at C:\Users\Malcolm Lewis\AppData\Local\Temp\blockmgr-101e88b1-5121-4d42-991f-42e47f278879
22/06/10 00:02:24 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
22/06/10 00:02:24 INFO SparkEnv: Registering OutputCommitCoordinator
22/06/10 00:02:25 INFO Utils: Successfully started service 'SparkUI' on port 4040.
22/06/10 00:02:25 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://MalcolmSurfacePro4.myfiosgateway.com:4040
22/06/10 00:02:25 INFO Executor: Starting executor ID driver on host MalcolmSurfacePro4.myfiosgateway.com
22/06/10 00:02:25 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50027.
22/06/10 00:02:25 INFO NettyBlockTransferService: Server created on MalcolmSurfacePro4.myfiosgateway.com:50027
22/06/10 00:02:25 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
22/06/10 00:02:25 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 50027, None)
22/06/10 00:02:25 INFO BlockManagerMasterEndpoint: Registering block manager MalcolmSurfacePro4.myfiosgateway.com:50027 with 366.3 MiB RAM, BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 50027, None)
22/06/10 00:02:25 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 50027, None)
22/06/10 00:02:25 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 50027, None)
22/06/10 00:02:26 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/C:/Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/spark-warehouse').
22/06/10 00:02:26 INFO SharedState: Warehouse path is 'file:/C:/Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/spark-warehouse'.
22/06/10 00:02:27 INFO InMemoryFileIndex: It took 49 ms to list leaf files for 1 paths.
22/06/10 00:02:27 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
22/06/10 00:02:30 INFO FileSourceStrategy: Pruning directories with:
22/06/10 00:02:30 INFO FileSourceStrategy: Pushed Filters:
22/06/10 00:02:30 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
22/06/10 00:02:30 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
22/06/10 00:02:31 INFO CodeGenerator: Code generated in 308.8724 ms
22/06/10 00:02:31 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 285.2 KiB, free 366.0 MiB)
22/06/10 00:02:31 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 366.0 MiB)
22/06/10 00:02:31 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50027 (size: 23.9 KiB, free: 366.3 MiB)
22/06/10 00:02:31 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
22/06/10 00:02:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 00:02:31 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
22/06/10 00:02:31 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/10 00:02:31 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
22/06/10 00:02:31 INFO DAGScheduler: Parents of final stage: List()
22/06/10 00:02:31 INFO DAGScheduler: Missing parents: List()
22/06/10 00:02:31 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 00:02:31 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.7 KiB, free 366.0 MiB)
22/06/10 00:02:31 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 366.0 MiB)
22/06/10 00:02:31 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50027 (size: 5.3 KiB, free: 366.3 MiB)
22/06/10 00:02:31 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1200
22/06/10 00:02:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 00:02:31 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
22/06/10 00:02:31 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7781 bytes)
22/06/10 00:02:31 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
22/06/10 00:02:32 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/10 00:02:32 INFO CodeGenerator: Code generated in 13.7625 ms
22/06/10 00:02:32 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1673 bytes result sent to driver
22/06/10 00:02:32 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 479 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/10 00:02:32 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
22/06/10 00:02:32 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 0.690 s
22/06/10 00:02:32 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
22/06/10 00:02:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
22/06/10 00:02:32 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 0.892268 s
22/06/10 00:02:32 INFO CodeGenerator: Code generated in 17.5457 ms
22/06/10 00:02:32 INFO FileSourceStrategy: Pruning directories with:
22/06/10 00:02:32 INFO FileSourceStrategy: Pushed Filters:
22/06/10 00:02:32 INFO FileSourceStrategy: Post-Scan Filters:
22/06/10 00:02:32 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
22/06/10 00:02:32 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 285.2 KiB, free 365.7 MiB)
22/06/10 00:02:32 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.7 MiB)
22/06/10 00:02:32 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50027 (size: 23.9 KiB, free: 366.2 MiB)
22/06/10 00:02:32 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
22/06/10 00:02:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 00:02:33 INFO FileSourceStrategy: Pruning directories with:
22/06/10 00:02:33 INFO FileSourceStrategy: Pushed Filters:
22/06/10 00:02:33 INFO FileSourceStrategy: Post-Scan Filters:
22/06/10 00:02:33 INFO FileSourceStrategy: Output Data Schema: struct<Date: string>
22/06/10 00:02:33 INFO CodeGenerator: Code generated in 11.427 ms
22/06/10 00:02:33 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 285.2 KiB, free 365.4 MiB)
22/06/10 00:02:33 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.4 MiB)
22/06/10 00:02:33 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50027 (size: 23.9 KiB, free: 366.2 MiB)
22/06/10 00:02:33 INFO SparkContext: Created broadcast 3 from showString at NativeMethodAccessorImpl.java:0
22/06/10 00:02:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 00:02:33 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
22/06/10 00:02:33 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/10 00:02:33 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)
22/06/10 00:02:33 INFO DAGScheduler: Parents of final stage: List()
22/06/10 00:02:33 INFO DAGScheduler: Missing parents: List()
22/06/10 00:02:33 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 00:02:33 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 12.1 KiB, free 365.4 MiB)
22/06/10 00:02:33 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 365.4 MiB)
22/06/10 00:02:33 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50027 (size: 6.2 KiB, free: 366.2 MiB)
22/06/10 00:02:33 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1200
22/06/10 00:02:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 00:02:33 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
22/06/10 00:02:33 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7781 bytes)
22/06/10 00:02:33 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
22/06/10 00:02:33 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/10 00:02:33 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1500 bytes result sent to driver
22/06/10 00:02:33 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 70 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/10 00:02:33 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
22/06/10 00:02:33 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.113 s
22/06/10 00:02:33 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
22/06/10 00:02:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
22/06/10 00:02:33 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.128661 s
22/06/10 00:02:33 INFO CodeGenerator: Code generated in 17.3317 ms
+---------+
|     Date|
+---------+
| 1/5/2019|
| 3/8/2019|
| 3/3/2019|
|1/27/2019|
| 2/8/2019|
|3/25/2019|
|2/25/2019|
|2/24/2019|
|1/10/2019|
|2/20/2019|
+---------+

22/06/10 00:02:33 INFO FileSourceStrategy: Pruning directories with:
22/06/10 00:02:33 INFO FileSourceStrategy: Pushed Filters:
22/06/10 00:02:33 INFO FileSourceStrategy: Post-Scan Filters:
22/06/10 00:02:33 INFO FileSourceStrategy: Output Data Schema: struct<Date: string>
Traceback (most recent call last):
  File "C:/Users/Malcolm Lewis/Desktop/Pipeline Pet Project/Spark Pipeline.py", line 36, in <module>
    output.write.csv("filtered.csv")
  File "C:\Python37\lib\site-packages\pyspark\python\lib\pyspark.zip\pyspark\sql\readwriter.py", line 1027, in csv
  File "C:\Python37\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1305, in __call__
  File "C:\Python37\lib\site-packages\pyspark\python\lib\pyspark.zip\pyspark\sql\utils.py", line 137, in deco
  File "<string>", line 3, in raise_from
pyspark.sql.utils.AnalysisException: path file:/C:/Users/Malcolm Lewis/Desktop/Pipeline Pet Project/filtered.csv already exists.;
22/06/10 00:02:33 INFO SparkContext: Invoking stop() from shutdown hook
22/06/10 00:02:33 INFO SparkUI: Stopped Spark web UI at http://MalcolmSurfacePro4.myfiosgateway.com:4040
22/06/10 00:02:33 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
22/06/10 00:02:33 INFO MemoryStore: MemoryStore cleared
22/06/10 00:02:33 INFO BlockManager: BlockManager stopped
22/06/10 00:02:33 INFO BlockManagerMaster: BlockManagerMaster stopped
22/06/10 00:02:33 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
22/06/10 00:02:33 INFO SparkContext: Successfully stopped SparkContext
22/06/10 00:02:33 INFO ShutdownHookManager: Shutdown hook called
22/06/10 00:02:33 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-907ce797-fbcb-4ed9-ae5c-aab1fabeb462
22/06/10 00:02:33 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-d5b4f15d-615e-44d6-9ee9-6d5ecf318115\pyspark-f8b42352-7efb-46e5-bd56-5cc8ae7139db
22/06/10 00:02:33 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-d5b4f15d-615e-44d6-9ee9-6d5ecf318115

C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project>spark-submit "Spark Pipeline.py"
22/06/10 00:03:17 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable C:\hadoop-common-2.2.0-bin-master\bin\winutils\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2412)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2412)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
22/06/10 00:03:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
22/06/10 00:03:18 INFO SparkContext: Running Spark version 3.0.0
22/06/10 00:03:18 INFO ResourceUtils: ==============================================================
22/06/10 00:03:18 INFO ResourceUtils: Resources for spark.driver:

22/06/10 00:03:18 INFO ResourceUtils: ==============================================================
22/06/10 00:03:18 INFO SparkContext: Submitted application: Spark%20Pipeline.py
22/06/10 00:03:18 INFO SecurityManager: Changing view acls to: Malcolm Lewis
22/06/10 00:03:18 INFO SecurityManager: Changing modify acls to: Malcolm Lewis
22/06/10 00:03:18 INFO SecurityManager: Changing view acls groups to:
22/06/10 00:03:18 INFO SecurityManager: Changing modify acls groups to:
22/06/10 00:03:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Malcolm Lewis); groups with view permissions: Set(); users  with modify permissions: Set(Malcolm Lewis); groups with modify permissions: Set()
22/06/10 00:03:19 INFO Utils: Successfully started service 'sparkDriver' on port 50056.
22/06/10 00:03:20 INFO SparkEnv: Registering MapOutputTracker
22/06/10 00:03:20 INFO SparkEnv: Registering BlockManagerMaster
22/06/10 00:03:20 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
22/06/10 00:03:20 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
22/06/10 00:03:20 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
22/06/10 00:03:20 INFO DiskBlockManager: Created local directory at C:\Users\Malcolm Lewis\AppData\Local\Temp\blockmgr-793ceb4e-6d3d-498f-8e80-58bcb22a1141
22/06/10 00:03:20 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
22/06/10 00:03:20 INFO SparkEnv: Registering OutputCommitCoordinator
22/06/10 00:03:20 INFO Utils: Successfully started service 'SparkUI' on port 4040.
22/06/10 00:03:20 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://MalcolmSurfacePro4.myfiosgateway.com:4040
22/06/10 00:03:20 INFO Executor: Starting executor ID driver on host MalcolmSurfacePro4.myfiosgateway.com
22/06/10 00:03:20 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50079.
22/06/10 00:03:20 INFO NettyBlockTransferService: Server created on MalcolmSurfacePro4.myfiosgateway.com:50079
22/06/10 00:03:20 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
22/06/10 00:03:20 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 50079, None)
22/06/10 00:03:20 INFO BlockManagerMasterEndpoint: Registering block manager MalcolmSurfacePro4.myfiosgateway.com:50079 with 366.3 MiB RAM, BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 50079, None)
22/06/10 00:03:20 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 50079, None)
22/06/10 00:03:20 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 50079, None)
22/06/10 00:03:21 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/C:/Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/spark-warehouse').
22/06/10 00:03:21 INFO SharedState: Warehouse path is 'file:/C:/Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/spark-warehouse'.
22/06/10 00:03:22 INFO InMemoryFileIndex: It took 34 ms to list leaf files for 1 paths.
22/06/10 00:03:22 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
22/06/10 00:03:25 INFO FileSourceStrategy: Pruning directories with:
22/06/10 00:03:25 INFO FileSourceStrategy: Pushed Filters:
22/06/10 00:03:25 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
22/06/10 00:03:25 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
22/06/10 00:03:25 INFO CodeGenerator: Code generated in 260.1823 ms
22/06/10 00:03:25 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 285.2 KiB, free 366.0 MiB)
22/06/10 00:03:26 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 366.0 MiB)
22/06/10 00:03:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50079 (size: 23.9 KiB, free: 366.3 MiB)
22/06/10 00:03:26 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
22/06/10 00:03:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 00:03:26 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
22/06/10 00:03:26 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/10 00:03:26 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
22/06/10 00:03:26 INFO DAGScheduler: Parents of final stage: List()
22/06/10 00:03:26 INFO DAGScheduler: Missing parents: List()
22/06/10 00:03:26 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 00:03:26 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.7 KiB, free 366.0 MiB)
22/06/10 00:03:26 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 366.0 MiB)
22/06/10 00:03:26 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50079 (size: 5.3 KiB, free: 366.3 MiB)
22/06/10 00:03:26 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1200
22/06/10 00:03:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 00:03:26 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
22/06/10 00:03:26 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7781 bytes)
22/06/10 00:03:26 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
22/06/10 00:03:26 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/10 00:03:26 INFO CodeGenerator: Code generated in 17.0069 ms
22/06/10 00:03:26 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1673 bytes result sent to driver
22/06/10 00:03:26 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 460 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/10 00:03:26 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
22/06/10 00:03:26 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 0.633 s
22/06/10 00:03:26 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
22/06/10 00:03:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
22/06/10 00:03:27 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 0.908604 s
22/06/10 00:03:27 INFO CodeGenerator: Code generated in 19.0657 ms
22/06/10 00:03:27 INFO FileSourceStrategy: Pruning directories with:
22/06/10 00:03:27 INFO FileSourceStrategy: Pushed Filters:
22/06/10 00:03:27 INFO FileSourceStrategy: Post-Scan Filters:
22/06/10 00:03:27 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
22/06/10 00:03:27 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 285.2 KiB, free 365.7 MiB)
22/06/10 00:03:27 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.7 MiB)
22/06/10 00:03:27 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50079 (size: 23.9 KiB, free: 366.2 MiB)
22/06/10 00:03:27 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
22/06/10 00:03:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 00:03:27 INFO FileSourceStrategy: Pruning directories with:
22/06/10 00:03:27 INFO FileSourceStrategy: Pushed Filters:
22/06/10 00:03:27 INFO FileSourceStrategy: Post-Scan Filters:
22/06/10 00:03:27 INFO FileSourceStrategy: Output Data Schema: struct<Date: string>
22/06/10 00:03:27 INFO CodeGenerator: Code generated in 11.83 ms
22/06/10 00:03:27 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 285.2 KiB, free 365.4 MiB)
22/06/10 00:03:27 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.4 MiB)
22/06/10 00:03:27 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50079 (size: 23.9 KiB, free: 366.2 MiB)
22/06/10 00:03:27 INFO SparkContext: Created broadcast 3 from showString at NativeMethodAccessorImpl.java:0
22/06/10 00:03:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 00:03:27 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
22/06/10 00:03:27 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/10 00:03:27 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)
22/06/10 00:03:27 INFO DAGScheduler: Parents of final stage: List()
22/06/10 00:03:27 INFO DAGScheduler: Missing parents: List()
22/06/10 00:03:27 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 00:03:27 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 12.1 KiB, free 365.4 MiB)
22/06/10 00:03:27 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 365.4 MiB)
22/06/10 00:03:27 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50079 (size: 6.2 KiB, free: 366.2 MiB)
22/06/10 00:03:27 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1200
22/06/10 00:03:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 00:03:27 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
22/06/10 00:03:27 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7781 bytes)
22/06/10 00:03:27 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
22/06/10 00:03:27 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/10 00:03:27 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1543 bytes result sent to driver
22/06/10 00:03:27 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 69 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/10 00:03:27 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
22/06/10 00:03:27 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.112 s
22/06/10 00:03:27 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
22/06/10 00:03:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
22/06/10 00:03:27 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.127069 s
22/06/10 00:03:28 INFO CodeGenerator: Code generated in 21.8373 ms
+---------+
|     Date|
+---------+
| 1/5/2019|
| 3/8/2019|
| 3/3/2019|
|1/27/2019|
| 2/8/2019|
|3/25/2019|
|2/25/2019|
|2/24/2019|
|1/10/2019|
|2/20/2019|
+---------+

22/06/10 00:03:28 INFO FileSourceStrategy: Pruning directories with:
22/06/10 00:03:28 INFO FileSourceStrategy: Pushed Filters:
22/06/10 00:03:28 INFO FileSourceStrategy: Post-Scan Filters:
22/06/10 00:03:28 INFO FileSourceStrategy: Output Data Schema: struct<Date: string>
22/06/10 00:03:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/06/10 00:03:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
22/06/10 00:03:28 INFO CodeGenerator: Code generated in 16.9985 ms
22/06/10 00:03:28 INFO CodeGenerator: Code generated in 19.3883 ms
22/06/10 00:03:28 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 285.2 KiB, free 365.1 MiB)
22/06/10 00:03:28 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.1 MiB)
22/06/10 00:03:28 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50079 (size: 23.9 KiB, free: 366.2 MiB)
22/06/10 00:03:28 INFO SparkContext: Created broadcast 5 from csv at NativeMethodAccessorImpl.java:0
22/06/10 00:03:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 00:03:28 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
22/06/10 00:03:28 INFO DAGScheduler: Registering RDD 17 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0
22/06/10 00:03:28 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/10 00:03:28 INFO DAGScheduler: Final stage: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0)
22/06/10 00:03:28 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
22/06/10 00:03:28 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
22/06/10 00:03:28 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[17] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 00:03:28 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 14.8 KiB, free 365.0 MiB)
22/06/10 00:03:28 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 365.0 MiB)
22/06/10 00:03:28 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50079 (size: 7.5 KiB, free: 366.2 MiB)
22/06/10 00:03:28 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1200
22/06/10 00:03:28 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[17] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 00:03:28 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
22/06/10 00:03:28 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7770 bytes)
22/06/10 00:03:28 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
22/06/10 00:03:28 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/10 00:03:28 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1842 bytes result sent to driver
22/06/10 00:03:28 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 107 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/10 00:03:28 INFO DAGScheduler: ShuffleMapStage 2 (csv at NativeMethodAccessorImpl.java:0) finished in 0.143 s
22/06/10 00:03:28 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
22/06/10 00:03:28 INFO DAGScheduler: looking for newly runnable stages
22/06/10 00:03:28 INFO DAGScheduler: running: Set()
22/06/10 00:03:28 INFO DAGScheduler: waiting: Set(ResultStage 3)
22/06/10 00:03:28 INFO DAGScheduler: failed: Set()
22/06/10 00:03:28 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 00:03:28 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 151.2 KiB, free 364.9 MiB)
22/06/10 00:03:28 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 54.7 KiB, free 364.8 MiB)
22/06/10 00:03:28 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50079 (size: 54.7 KiB, free: 366.1 MiB)
22/06/10 00:03:28 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1200
22/06/10 00:03:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 00:03:28 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
22/06/10 00:03:28 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
22/06/10 00:03:28 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
22/06/10 00:03:28 INFO ShuffleBlockFetcherIterator: Getting 1 (156.0 B) non-empty blocks including 1 (156.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
22/06/10 00:03:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
22/06/10 00:03:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/06/10 00:03:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
22/06/10 00:03:28 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3)
java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610000328_0003_m_000000_3\part-00000-fb00cb33-0d21-4491-8c1b-cbd9b7b3bf0f-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
22/06/10 00:03:28 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610000328_0003_m_000000_3\part-00000-fb00cb33-0d21-4491-8c1b-cbd9b7b3bf0f-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

22/06/10 00:03:28 ERROR TaskSetManager: Task 0 in stage 3.0 failed 1 times; aborting job
22/06/10 00:03:28 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
22/06/10 00:03:28 INFO TaskSchedulerImpl: Cancelling stage 3
22/06/10 00:03:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage cancelled
22/06/10 00:03:28 INFO DAGScheduler: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0) failed in 0.320 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610000328_0003_m_000000_3\part-00000-fb00cb33-0d21-4491-8c1b-cbd9b7b3bf0f-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
22/06/10 00:03:28 INFO DAGScheduler: Job 2 failed: csv at NativeMethodAccessorImpl.java:0, took 0.520748 s
22/06/10 00:03:28 ERROR FileFormatWriter: Aborting job e541bfd5-2944-48a5-b5f3-f031c4193636.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610000328_0003_m_000000_3\part-00000-fb00cb33-0d21-4491-8c1b-cbd9b7b3bf0f-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)
        at scala.Option.foreach(Option.scala:407)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)
        at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)
        at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)
        at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
        at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)
        at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610000328_0003_m_000000_3\part-00000-fb00cb33-0d21-4491-8c1b-cbd9b7b3bf0f-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        ... 1 more
Traceback (most recent call last):
  File "C:/Users/Malcolm Lewis/Desktop/Pipeline Pet Project/Spark Pipeline.py", line 36, in <module>
    output.write.csv("filtered.csv")
  File "C:\Python37\lib\site-packages\pyspark\python\lib\pyspark.zip\pyspark\sql\readwriter.py", line 1027, in csv
  File "C:\Python37\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1305, in __call__
  File "C:\Python37\lib\site-packages\pyspark\python\lib\pyspark.zip\pyspark\sql\utils.py", line 131, in deco
  File "C:\Python37\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o29.csv.
: org.apache.spark.SparkException: Job aborted.
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)
        at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)
        at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)
        at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
        at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)
        at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610000328_0003_m_000000_3\part-00000-fb00cb33-0d21-4491-8c1b-cbd9b7b3bf0f-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)
        at scala.Option.foreach(Option.scala:407)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)
        ... 33 more
Caused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610000328_0003_m_000000_3\part-00000-fb00cb33-0d21-4491-8c1b-cbd9b7b3bf0f-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        ... 1 more

22/06/10 00:03:29 INFO SparkContext: Invoking stop() from shutdown hook
22/06/10 00:03:29 INFO SparkUI: Stopped Spark web UI at http://MalcolmSurfacePro4.myfiosgateway.com:4040
22/06/10 00:03:29 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
22/06/10 00:03:29 INFO MemoryStore: MemoryStore cleared
22/06/10 00:03:29 INFO BlockManager: BlockManager stopped
22/06/10 00:03:29 INFO BlockManagerMaster: BlockManagerMaster stopped
22/06/10 00:03:29 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
22/06/10 00:03:29 INFO SparkContext: Successfully stopped SparkContext
22/06/10 00:03:29 INFO ShutdownHookManager: Shutdown hook called
22/06/10 00:03:29 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-da3bb165-af20-438b-ba35-69aefbcdae21
22/06/10 00:03:29 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-4cbbc7a5-695e-4f20-86ca-fa166877fc43
22/06/10 00:03:29 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-da3bb165-af20-438b-ba35-69aefbcdae21\pyspark-9bd30bde-2568-4c20-bb79-edfe8de217e4

C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project>spark-submit "Spark Pipeline.py"
22/06/10 00:04:27 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable C:\hadoop-common-2.2.0-bin-master\bin\winutils\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2412)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2412)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
22/06/10 00:04:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
22/06/10 00:04:28 INFO SparkContext: Running Spark version 3.0.0
22/06/10 00:04:29 INFO ResourceUtils: ==============================================================
22/06/10 00:04:29 INFO ResourceUtils: Resources for spark.driver:

22/06/10 00:04:29 INFO ResourceUtils: ==============================================================
22/06/10 00:04:29 INFO SparkContext: Submitted application: Spark%20Pipeline.py
22/06/10 00:04:29 INFO SecurityManager: Changing view acls to: Malcolm Lewis
22/06/10 00:04:29 INFO SecurityManager: Changing modify acls to: Malcolm Lewis
22/06/10 00:04:29 INFO SecurityManager: Changing view acls groups to:
22/06/10 00:04:29 INFO SecurityManager: Changing modify acls groups to:
22/06/10 00:04:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Malcolm Lewis); groups with view permissions: Set(); users  with modify permissions: Set(Malcolm Lewis); groups with modify permissions: Set()
22/06/10 00:04:30 INFO Utils: Successfully started service 'sparkDriver' on port 50116.
22/06/10 00:04:30 INFO SparkEnv: Registering MapOutputTracker
22/06/10 00:04:30 INFO SparkEnv: Registering BlockManagerMaster
22/06/10 00:04:30 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
22/06/10 00:04:30 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
22/06/10 00:04:30 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
22/06/10 00:04:30 INFO DiskBlockManager: Created local directory at C:\Users\Malcolm Lewis\AppData\Local\Temp\blockmgr-6b61198b-9758-4202-a5d6-94ec84a80f09
22/06/10 00:04:30 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
22/06/10 00:04:30 INFO SparkEnv: Registering OutputCommitCoordinator
22/06/10 00:04:31 INFO Utils: Successfully started service 'SparkUI' on port 4040.
22/06/10 00:04:31 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://MalcolmSurfacePro4.myfiosgateway.com:4040
22/06/10 00:04:31 INFO Executor: Starting executor ID driver on host MalcolmSurfacePro4.myfiosgateway.com
22/06/10 00:04:31 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50139.
22/06/10 00:04:31 INFO NettyBlockTransferService: Server created on MalcolmSurfacePro4.myfiosgateway.com:50139
22/06/10 00:04:31 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
22/06/10 00:04:31 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 50139, None)
22/06/10 00:04:31 INFO BlockManagerMasterEndpoint: Registering block manager MalcolmSurfacePro4.myfiosgateway.com:50139 with 366.3 MiB RAM, BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 50139, None)
22/06/10 00:04:31 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 50139, None)
22/06/10 00:04:31 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 50139, None)
22/06/10 00:04:32 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/C:/Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/spark-warehouse').
22/06/10 00:04:32 INFO SharedState: Warehouse path is 'file:/C:/Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/spark-warehouse'.
22/06/10 00:04:33 INFO InMemoryFileIndex: It took 34 ms to list leaf files for 1 paths.
22/06/10 00:04:33 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
22/06/10 00:04:36 INFO FileSourceStrategy: Pruning directories with:
22/06/10 00:04:36 INFO FileSourceStrategy: Pushed Filters:
22/06/10 00:04:36 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
22/06/10 00:04:36 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
22/06/10 00:04:37 INFO CodeGenerator: Code generated in 478.8537 ms
22/06/10 00:04:37 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 285.2 KiB, free 366.0 MiB)
22/06/10 00:04:37 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 366.0 MiB)
22/06/10 00:04:37 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50139 (size: 23.9 KiB, free: 366.3 MiB)
22/06/10 00:04:37 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
22/06/10 00:04:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 00:04:37 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
22/06/10 00:04:37 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/10 00:04:37 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
22/06/10 00:04:37 INFO DAGScheduler: Parents of final stage: List()
22/06/10 00:04:37 INFO DAGScheduler: Missing parents: List()
22/06/10 00:04:37 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 00:04:37 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.7 KiB, free 366.0 MiB)
22/06/10 00:04:37 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 366.0 MiB)
22/06/10 00:04:37 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50139 (size: 5.3 KiB, free: 366.3 MiB)
22/06/10 00:04:37 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1200
22/06/10 00:04:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 00:04:37 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
22/06/10 00:04:37 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7781 bytes)
22/06/10 00:04:37 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
22/06/10 00:04:38 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/10 00:04:38 INFO CodeGenerator: Code generated in 18.7742 ms
22/06/10 00:04:38 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1716 bytes result sent to driver
22/06/10 00:04:38 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 507 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/10 00:04:38 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
22/06/10 00:04:38 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 0.692 s
22/06/10 00:04:38 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
22/06/10 00:04:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
22/06/10 00:04:38 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 0.873202 s
22/06/10 00:04:38 INFO CodeGenerator: Code generated in 18.8612 ms
22/06/10 00:04:38 INFO FileSourceStrategy: Pruning directories with:
22/06/10 00:04:38 INFO FileSourceStrategy: Pushed Filters:
22/06/10 00:04:38 INFO FileSourceStrategy: Post-Scan Filters:
22/06/10 00:04:38 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
22/06/10 00:04:38 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 285.2 KiB, free 365.7 MiB)
22/06/10 00:04:38 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.7 MiB)
22/06/10 00:04:38 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50139 (size: 23.9 KiB, free: 366.2 MiB)
22/06/10 00:04:38 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
22/06/10 00:04:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 00:04:39 INFO FileSourceStrategy: Pruning directories with:
22/06/10 00:04:39 INFO FileSourceStrategy: Pushed Filters:
22/06/10 00:04:39 INFO FileSourceStrategy: Post-Scan Filters:
22/06/10 00:04:39 INFO FileSourceStrategy: Output Data Schema: struct<Date: string>
22/06/10 00:04:39 INFO CodeGenerator: Code generated in 11.4174 ms
22/06/10 00:04:39 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 285.2 KiB, free 365.4 MiB)
22/06/10 00:04:39 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.4 MiB)
22/06/10 00:04:39 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50139 (size: 23.9 KiB, free: 366.2 MiB)
22/06/10 00:04:39 INFO SparkContext: Created broadcast 3 from showString at NativeMethodAccessorImpl.java:0
22/06/10 00:04:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 00:04:39 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
22/06/10 00:04:39 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/10 00:04:39 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)
22/06/10 00:04:39 INFO DAGScheduler: Parents of final stage: List()
22/06/10 00:04:39 INFO DAGScheduler: Missing parents: List()
22/06/10 00:04:39 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 00:04:39 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 12.1 KiB, free 365.4 MiB)
22/06/10 00:04:39 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 365.4 MiB)
22/06/10 00:04:39 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50139 (size: 6.2 KiB, free: 366.2 MiB)
22/06/10 00:04:39 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1200
22/06/10 00:04:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 00:04:39 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
22/06/10 00:04:39 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7781 bytes)
22/06/10 00:04:39 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
22/06/10 00:04:39 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/10 00:04:39 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1543 bytes result sent to driver
22/06/10 00:04:39 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 56 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/10 00:04:39 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
22/06/10 00:04:39 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.100 s
22/06/10 00:04:39 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
22/06/10 00:04:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
22/06/10 00:04:39 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.108089 s
22/06/10 00:04:39 INFO CodeGenerator: Code generated in 22.1975 ms
+---------+
|     Date|
+---------+
| 1/5/2019|
| 3/8/2019|
| 3/3/2019|
|1/27/2019|
| 2/8/2019|
|3/25/2019|
|2/25/2019|
|2/24/2019|
|1/10/2019|
|2/20/2019|
+---------+

22/06/10 00:04:39 INFO FileSourceStrategy: Pruning directories with:
22/06/10 00:04:39 INFO FileSourceStrategy: Pushed Filters:
22/06/10 00:04:39 INFO FileSourceStrategy: Post-Scan Filters:
22/06/10 00:04:39 INFO FileSourceStrategy: Output Data Schema: struct<Date: string>
Traceback (most recent call last):
  File "C:/Users/Malcolm Lewis/Desktop/Pipeline Pet Project/Spark Pipeline.py", line 36, in <module>
    output.write.csv("filtered.csv")
  File "C:\Python37\lib\site-packages\pyspark\python\lib\pyspark.zip\pyspark\sql\readwriter.py", line 1027, in csv
  File "C:\Python37\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1305, in __call__
  File "C:\Python37\lib\site-packages\pyspark\python\lib\pyspark.zip\pyspark\sql\utils.py", line 137, in deco
  File "<string>", line 3, in raise_from
pyspark.sql.utils.AnalysisException: path file:/C:/Users/Malcolm Lewis/Desktop/Pipeline Pet Project/filtered.csv already exists.;
22/06/10 00:04:39 INFO SparkContext: Invoking stop() from shutdown hook
22/06/10 00:04:39 INFO SparkUI: Stopped Spark web UI at http://MalcolmSurfacePro4.myfiosgateway.com:4040
22/06/10 00:04:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
22/06/10 00:04:39 INFO MemoryStore: MemoryStore cleared
22/06/10 00:04:39 INFO BlockManager: BlockManager stopped
22/06/10 00:04:39 INFO BlockManagerMaster: BlockManagerMaster stopped
22/06/10 00:04:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
22/06/10 00:04:39 INFO SparkContext: Successfully stopped SparkContext
22/06/10 00:04:39 INFO ShutdownHookManager: Shutdown hook called
22/06/10 00:04:39 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-dbadb3e2-283e-4807-a651-81796b8a4539
22/06/10 00:04:39 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-e9c3ec90-b56d-484f-b162-4951d789dd58
22/06/10 00:04:39 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-e9c3ec90-b56d-484f-b162-4951d789dd58\pyspark-79201c19-8227-4045-bad0-90508374b78a

C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project>spark-submit "Spark Pipeline.py"
22/06/10 00:05:07 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable C:\hadoop-common-2.2.0-bin-master\bin\winutils\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2412)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2412)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
22/06/10 00:05:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
22/06/10 00:05:09 INFO SparkContext: Running Spark version 3.0.0
22/06/10 00:05:09 INFO ResourceUtils: ==============================================================
22/06/10 00:05:09 INFO ResourceUtils: Resources for spark.driver:

22/06/10 00:05:09 INFO ResourceUtils: ==============================================================
22/06/10 00:05:09 INFO SparkContext: Submitted application: Spark%20Pipeline.py
22/06/10 00:05:09 INFO SecurityManager: Changing view acls to: Malcolm Lewis
22/06/10 00:05:09 INFO SecurityManager: Changing modify acls to: Malcolm Lewis
22/06/10 00:05:09 INFO SecurityManager: Changing view acls groups to:
22/06/10 00:05:09 INFO SecurityManager: Changing modify acls groups to:
22/06/10 00:05:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Malcolm Lewis); groups with view permissions: Set(); users  with modify permissions: Set(Malcolm Lewis); groups with modify permissions: Set()
22/06/10 00:05:11 INFO Utils: Successfully started service 'sparkDriver' on port 50164.
22/06/10 00:05:11 INFO SparkEnv: Registering MapOutputTracker
22/06/10 00:05:11 INFO SparkEnv: Registering BlockManagerMaster
22/06/10 00:05:11 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
22/06/10 00:05:11 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
22/06/10 00:05:11 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
22/06/10 00:05:11 INFO DiskBlockManager: Created local directory at C:\Users\Malcolm Lewis\AppData\Local\Temp\blockmgr-e2cf02a7-593a-4d4b-8e13-88a0eb873f04
22/06/10 00:05:11 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
22/06/10 00:05:11 INFO SparkEnv: Registering OutputCommitCoordinator
22/06/10 00:05:11 INFO Utils: Successfully started service 'SparkUI' on port 4040.
22/06/10 00:05:11 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://MalcolmSurfacePro4.myfiosgateway.com:4040
22/06/10 00:05:11 INFO Executor: Starting executor ID driver on host MalcolmSurfacePro4.myfiosgateway.com
22/06/10 00:05:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50187.
22/06/10 00:05:11 INFO NettyBlockTransferService: Server created on MalcolmSurfacePro4.myfiosgateway.com:50187
22/06/10 00:05:11 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
22/06/10 00:05:12 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 50187, None)
22/06/10 00:05:12 INFO BlockManagerMasterEndpoint: Registering block manager MalcolmSurfacePro4.myfiosgateway.com:50187 with 366.3 MiB RAM, BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 50187, None)
22/06/10 00:05:12 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 50187, None)
22/06/10 00:05:12 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 50187, None)
22/06/10 00:05:12 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/C:/Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/spark-warehouse').
22/06/10 00:05:12 INFO SharedState: Warehouse path is 'file:/C:/Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/spark-warehouse'.
22/06/10 00:05:14 INFO InMemoryFileIndex: It took 35 ms to list leaf files for 1 paths.
22/06/10 00:05:14 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
22/06/10 00:05:16 INFO FileSourceStrategy: Pruning directories with:
22/06/10 00:05:16 INFO FileSourceStrategy: Pushed Filters:
22/06/10 00:05:16 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
22/06/10 00:05:16 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
22/06/10 00:05:17 INFO CodeGenerator: Code generated in 324.2813 ms
22/06/10 00:05:17 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 285.2 KiB, free 366.0 MiB)
22/06/10 00:05:17 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 366.0 MiB)
22/06/10 00:05:17 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50187 (size: 23.9 KiB, free: 366.3 MiB)
22/06/10 00:05:17 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
22/06/10 00:05:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 00:05:17 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
22/06/10 00:05:18 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/10 00:05:18 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
22/06/10 00:05:18 INFO DAGScheduler: Parents of final stage: List()
22/06/10 00:05:18 INFO DAGScheduler: Missing parents: List()
22/06/10 00:05:18 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 00:05:18 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.7 KiB, free 366.0 MiB)
22/06/10 00:05:18 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 366.0 MiB)
22/06/10 00:05:18 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50187 (size: 5.3 KiB, free: 366.3 MiB)
22/06/10 00:05:18 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1200
22/06/10 00:05:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 00:05:18 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
22/06/10 00:05:18 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7781 bytes)
22/06/10 00:05:18 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
22/06/10 00:05:18 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/10 00:05:18 INFO CodeGenerator: Code generated in 20.8922 ms
22/06/10 00:05:18 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1716 bytes result sent to driver
22/06/10 00:05:18 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 523 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/10 00:05:18 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
22/06/10 00:05:18 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 0.675 s
22/06/10 00:05:18 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
22/06/10 00:05:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
22/06/10 00:05:18 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 0.857823 s
22/06/10 00:05:18 INFO CodeGenerator: Code generated in 12.4464 ms
22/06/10 00:05:19 INFO FileSourceStrategy: Pruning directories with:
22/06/10 00:05:19 INFO FileSourceStrategy: Pushed Filters:
22/06/10 00:05:19 INFO FileSourceStrategy: Post-Scan Filters:
22/06/10 00:05:19 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
22/06/10 00:05:19 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 285.2 KiB, free 365.7 MiB)
22/06/10 00:05:19 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.7 MiB)
22/06/10 00:05:19 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50187 (size: 23.9 KiB, free: 366.2 MiB)
22/06/10 00:05:19 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
22/06/10 00:05:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 00:05:19 INFO FileSourceStrategy: Pruning directories with:
22/06/10 00:05:19 INFO FileSourceStrategy: Pushed Filters:
22/06/10 00:05:19 INFO FileSourceStrategy: Post-Scan Filters:
22/06/10 00:05:19 INFO FileSourceStrategy: Output Data Schema: struct<Date: string>
22/06/10 00:05:19 INFO CodeGenerator: Code generated in 18.2034 ms
22/06/10 00:05:19 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 285.2 KiB, free 365.4 MiB)
22/06/10 00:05:19 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.4 MiB)
22/06/10 00:05:19 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50187 (size: 23.9 KiB, free: 366.2 MiB)
22/06/10 00:05:19 INFO SparkContext: Created broadcast 3 from showString at NativeMethodAccessorImpl.java:0
22/06/10 00:05:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 00:05:19 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
22/06/10 00:05:19 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/10 00:05:19 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)
22/06/10 00:05:19 INFO DAGScheduler: Parents of final stage: List()
22/06/10 00:05:19 INFO DAGScheduler: Missing parents: List()
22/06/10 00:05:19 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 00:05:19 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 12.1 KiB, free 365.4 MiB)
22/06/10 00:05:19 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 365.4 MiB)
22/06/10 00:05:19 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50187 (size: 6.2 KiB, free: 366.2 MiB)
22/06/10 00:05:19 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1200
22/06/10 00:05:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 00:05:19 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
22/06/10 00:05:19 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7781 bytes)
22/06/10 00:05:19 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
22/06/10 00:05:19 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/10 00:05:20 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1543 bytes result sent to driver
22/06/10 00:05:20 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 290 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/10 00:05:20 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.340 s
22/06/10 00:05:20 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
22/06/10 00:05:20 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
22/06/10 00:05:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
22/06/10 00:05:20 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.353472 s
22/06/10 00:05:20 INFO CodeGenerator: Code generated in 23.6643 ms
+---------+
|     Date|
+---------+
| 1/5/2019|
| 3/8/2019|
| 3/3/2019|
|1/27/2019|
| 2/8/2019|
|3/25/2019|
|2/25/2019|
|2/24/2019|
|1/10/2019|
|2/20/2019|
+---------+

22/06/10 00:05:20 INFO FileSourceStrategy: Pruning directories with:
22/06/10 00:05:20 INFO FileSourceStrategy: Pushed Filters:
22/06/10 00:05:20 INFO FileSourceStrategy: Post-Scan Filters:
22/06/10 00:05:20 INFO FileSourceStrategy: Output Data Schema: struct<Date: string>
22/06/10 00:05:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/06/10 00:05:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
22/06/10 00:05:20 INFO CodeGenerator: Code generated in 14.2415 ms
22/06/10 00:05:20 INFO CodeGenerator: Code generated in 11.9926 ms
22/06/10 00:05:20 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 285.2 KiB, free 365.1 MiB)
22/06/10 00:05:20 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.1 MiB)
22/06/10 00:05:20 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50187 (size: 23.9 KiB, free: 366.2 MiB)
22/06/10 00:05:20 INFO SparkContext: Created broadcast 5 from csv at NativeMethodAccessorImpl.java:0
22/06/10 00:05:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 00:05:20 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
22/06/10 00:05:20 INFO DAGScheduler: Registering RDD 17 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0
22/06/10 00:05:20 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/10 00:05:20 INFO DAGScheduler: Final stage: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0)
22/06/10 00:05:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
22/06/10 00:05:20 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
22/06/10 00:05:20 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[17] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 00:05:20 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 14.8 KiB, free 365.0 MiB)
22/06/10 00:05:20 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 365.0 MiB)
22/06/10 00:05:20 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50187 (size: 7.5 KiB, free: 366.2 MiB)
22/06/10 00:05:20 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1200
22/06/10 00:05:20 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[17] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 00:05:20 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
22/06/10 00:05:20 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7770 bytes)
22/06/10 00:05:20 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
22/06/10 00:05:20 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/10 00:05:20 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1842 bytes result sent to driver
22/06/10 00:05:20 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 118 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/10 00:05:20 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
22/06/10 00:05:20 INFO DAGScheduler: ShuffleMapStage 2 (csv at NativeMethodAccessorImpl.java:0) finished in 0.153 s
22/06/10 00:05:20 INFO DAGScheduler: looking for newly runnable stages
22/06/10 00:05:20 INFO DAGScheduler: running: Set()
22/06/10 00:05:20 INFO DAGScheduler: waiting: Set(ResultStage 3)
22/06/10 00:05:20 INFO DAGScheduler: failed: Set()
22/06/10 00:05:20 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 00:05:20 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 151.2 KiB, free 364.9 MiB)
22/06/10 00:05:20 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 54.7 KiB, free 364.8 MiB)
22/06/10 00:05:20 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50187 (size: 54.7 KiB, free: 366.1 MiB)
22/06/10 00:05:20 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1200
22/06/10 00:05:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 00:05:20 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
22/06/10 00:05:20 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
22/06/10 00:05:20 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
22/06/10 00:05:20 INFO ShuffleBlockFetcherIterator: Getting 1 (156.0 B) non-empty blocks including 1 (156.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
22/06/10 00:05:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
22/06/10 00:05:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/06/10 00:05:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
22/06/10 00:05:21 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3)
java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610000520_0003_m_000000_3\part-00000-a870e4dc-0ad4-4449-b210-0c410191dbdb-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
22/06/10 00:05:21 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610000520_0003_m_000000_3\part-00000-a870e4dc-0ad4-4449-b210-0c410191dbdb-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

22/06/10 00:05:21 ERROR TaskSetManager: Task 0 in stage 3.0 failed 1 times; aborting job
22/06/10 00:05:21 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
22/06/10 00:05:21 INFO TaskSchedulerImpl: Cancelling stage 3
22/06/10 00:05:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage cancelled
22/06/10 00:05:21 INFO DAGScheduler: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0) failed in 0.390 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610000520_0003_m_000000_3\part-00000-a870e4dc-0ad4-4449-b210-0c410191dbdb-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
22/06/10 00:05:21 INFO DAGScheduler: Job 2 failed: csv at NativeMethodAccessorImpl.java:0, took 0.587496 s
22/06/10 00:05:21 ERROR FileFormatWriter: Aborting job 22264860-13fe-4077-b60b-f01d7f2bf76e.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610000520_0003_m_000000_3\part-00000-a870e4dc-0ad4-4449-b210-0c410191dbdb-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)
        at scala.Option.foreach(Option.scala:407)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)
        at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)
        at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)
        at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
        at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)
        at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610000520_0003_m_000000_3\part-00000-a870e4dc-0ad4-4449-b210-0c410191dbdb-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        ... 1 more
Traceback (most recent call last):
  File "C:/Users/Malcolm Lewis/Desktop/Pipeline Pet Project/Spark Pipeline.py", line 36, in <module>
    output.write.csv("filtered.csv")
  File "C:\Python37\lib\site-packages\pyspark\python\lib\pyspark.zip\pyspark\sql\readwriter.py", line 1027, in csv
  File "C:\Python37\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1305, in __call__
  File "C:\Python37\lib\site-packages\pyspark\python\lib\pyspark.zip\pyspark\sql\utils.py", line 131, in deco
  File "C:\Python37\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o29.csv.
: org.apache.spark.SparkException: Job aborted.
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)
        at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)
        at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)
        at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
        at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)
        at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610000520_0003_m_000000_3\part-00000-a870e4dc-0ad4-4449-b210-0c410191dbdb-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)
        at scala.Option.foreach(Option.scala:407)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)
        ... 33 more
Caused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610000520_0003_m_000000_3\part-00000-a870e4dc-0ad4-4449-b210-0c410191dbdb-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        ... 1 more

22/06/10 00:05:21 INFO SparkContext: Invoking stop() from shutdown hook
22/06/10 00:05:21 INFO SparkUI: Stopped Spark web UI at http://MalcolmSurfacePro4.myfiosgateway.com:4040
22/06/10 00:05:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
22/06/10 00:05:21 INFO MemoryStore: MemoryStore cleared
22/06/10 00:05:21 INFO BlockManager: BlockManager stopped
22/06/10 00:05:21 INFO BlockManagerMaster: BlockManagerMaster stopped
22/06/10 00:05:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
22/06/10 00:05:21 INFO SparkContext: Successfully stopped SparkContext
22/06/10 00:05:21 INFO ShutdownHookManager: Shutdown hook called
22/06/10 00:05:21 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-b1234424-ddfc-4d81-81c4-e274e9f6666f
22/06/10 00:05:21 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-7ed94024-442e-4605-af9b-02b5882c140a
22/06/10 00:05:21 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-b1234424-ddfc-4d81-81c4-e274e9f6666f\pyspark-17ba7e72-3cdd-4e75-9de9-6bc68cae74f1

C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project>spark-submit "Spark Pipeline.py"
22/06/10 00:15:35 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable C:\hadoop-common-2.2.0-bin-master\bin\winutils\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2412)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2412)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
22/06/10 00:15:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
22/06/10 00:15:36 INFO SparkContext: Running Spark version 3.0.0
22/06/10 00:15:36 INFO ResourceUtils: ==============================================================
22/06/10 00:15:36 INFO ResourceUtils: Resources for spark.driver:

22/06/10 00:15:36 INFO ResourceUtils: ==============================================================
22/06/10 00:15:36 INFO SparkContext: Submitted application: Spark%20Pipeline.py
22/06/10 00:15:36 INFO SecurityManager: Changing view acls to: Malcolm Lewis
22/06/10 00:15:36 INFO SecurityManager: Changing modify acls to: Malcolm Lewis
22/06/10 00:15:36 INFO SecurityManager: Changing view acls groups to:
22/06/10 00:15:36 INFO SecurityManager: Changing modify acls groups to:
22/06/10 00:15:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Malcolm Lewis); groups with view permissions: Set(); users  with modify permissions: Set(Malcolm Lewis); groups with modify permissions: Set()
22/06/10 00:15:37 INFO Utils: Successfully started service 'sparkDriver' on port 50330.
22/06/10 00:15:37 INFO SparkEnv: Registering MapOutputTracker
22/06/10 00:15:38 INFO SparkEnv: Registering BlockManagerMaster
22/06/10 00:15:38 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
22/06/10 00:15:38 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
22/06/10 00:15:38 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
22/06/10 00:15:38 INFO DiskBlockManager: Created local directory at C:\Users\Malcolm Lewis\AppData\Local\Temp\blockmgr-7b1a587b-f2e4-4d6b-b646-9455283013b4
22/06/10 00:15:38 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
22/06/10 00:15:38 INFO SparkEnv: Registering OutputCommitCoordinator
22/06/10 00:15:38 INFO Utils: Successfully started service 'SparkUI' on port 4040.
22/06/10 00:15:38 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://MalcolmSurfacePro4.myfiosgateway.com:4040
22/06/10 00:15:38 INFO Executor: Starting executor ID driver on host MalcolmSurfacePro4.myfiosgateway.com
22/06/10 00:15:38 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50353.
22/06/10 00:15:38 INFO NettyBlockTransferService: Server created on MalcolmSurfacePro4.myfiosgateway.com:50353
22/06/10 00:15:38 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
22/06/10 00:15:38 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 50353, None)
22/06/10 00:15:38 INFO BlockManagerMasterEndpoint: Registering block manager MalcolmSurfacePro4.myfiosgateway.com:50353 with 366.3 MiB RAM, BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 50353, None)
22/06/10 00:15:38 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 50353, None)
22/06/10 00:15:38 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 50353, None)
22/06/10 00:15:39 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/C:/Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/spark-warehouse').
22/06/10 00:15:39 INFO SharedState: Warehouse path is 'file:/C:/Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/spark-warehouse'.
22/06/10 00:15:41 INFO InMemoryFileIndex: It took 63 ms to list leaf files for 1 paths.
22/06/10 00:15:41 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
22/06/10 00:15:44 INFO FileSourceStrategy: Pruning directories with:
22/06/10 00:15:44 INFO FileSourceStrategy: Pushed Filters:
22/06/10 00:15:44 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
22/06/10 00:15:44 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
22/06/10 00:15:45 INFO CodeGenerator: Code generated in 367.8647 ms
22/06/10 00:15:45 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 285.2 KiB, free 366.0 MiB)
22/06/10 00:15:45 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 366.0 MiB)
22/06/10 00:15:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50353 (size: 23.9 KiB, free: 366.3 MiB)
22/06/10 00:15:45 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
22/06/10 00:15:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 00:15:45 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
22/06/10 00:15:45 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/10 00:15:45 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
22/06/10 00:15:45 INFO DAGScheduler: Parents of final stage: List()
22/06/10 00:15:45 INFO DAGScheduler: Missing parents: List()
22/06/10 00:15:45 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 00:15:45 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.7 KiB, free 366.0 MiB)
22/06/10 00:15:45 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 366.0 MiB)
22/06/10 00:15:45 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50353 (size: 5.3 KiB, free: 366.3 MiB)
22/06/10 00:15:45 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1200
22/06/10 00:15:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 00:15:45 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
22/06/10 00:15:45 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7781 bytes)
22/06/10 00:15:45 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
22/06/10 00:15:45 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/10 00:15:45 INFO CodeGenerator: Code generated in 15.2106 ms
22/06/10 00:15:46 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1673 bytes result sent to driver
22/06/10 00:15:46 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 459 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/10 00:15:46 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
22/06/10 00:15:46 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 0.610 s
22/06/10 00:15:46 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
22/06/10 00:15:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
22/06/10 00:15:46 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 0.809404 s
22/06/10 00:15:46 INFO CodeGenerator: Code generated in 14.7824 ms
22/06/10 00:15:46 INFO FileSourceStrategy: Pruning directories with:
22/06/10 00:15:46 INFO FileSourceStrategy: Pushed Filters:
22/06/10 00:15:46 INFO FileSourceStrategy: Post-Scan Filters:
22/06/10 00:15:46 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
22/06/10 00:15:46 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 285.2 KiB, free 365.7 MiB)
22/06/10 00:15:46 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.7 MiB)
22/06/10 00:15:46 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50353 (size: 23.9 KiB, free: 366.2 MiB)
22/06/10 00:15:46 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
22/06/10 00:15:46 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 00:15:47 INFO FileSourceStrategy: Pruning directories with:
22/06/10 00:15:47 INFO FileSourceStrategy: Pushed Filters:
22/06/10 00:15:47 INFO FileSourceStrategy: Post-Scan Filters:
22/06/10 00:15:47 INFO FileSourceStrategy: Output Data Schema: struct<Date: string>
22/06/10 00:15:47 INFO CodeGenerator: Code generated in 16.159 ms
22/06/10 00:15:47 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 285.2 KiB, free 365.4 MiB)
22/06/10 00:15:47 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.4 MiB)
22/06/10 00:15:47 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50353 (size: 23.9 KiB, free: 366.2 MiB)
22/06/10 00:15:47 INFO SparkContext: Created broadcast 3 from showString at NativeMethodAccessorImpl.java:0
22/06/10 00:15:47 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 00:15:47 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
22/06/10 00:15:47 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/10 00:15:47 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)
22/06/10 00:15:47 INFO DAGScheduler: Parents of final stage: List()
22/06/10 00:15:47 INFO DAGScheduler: Missing parents: List()
22/06/10 00:15:47 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 00:15:47 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 12.1 KiB, free 365.4 MiB)
22/06/10 00:15:47 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 365.4 MiB)
22/06/10 00:15:47 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50353 (size: 6.2 KiB, free: 366.2 MiB)
22/06/10 00:15:47 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1200
22/06/10 00:15:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 00:15:47 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
22/06/10 00:15:47 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7781 bytes)
22/06/10 00:15:47 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
22/06/10 00:15:47 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/10 00:15:47 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1543 bytes result sent to driver
22/06/10 00:15:47 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 72 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/10 00:15:47 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
22/06/10 00:15:47 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.107 s
22/06/10 00:15:47 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
22/06/10 00:15:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
22/06/10 00:15:47 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.119534 s
22/06/10 00:15:47 INFO CodeGenerator: Code generated in 15.9836 ms
+---------+
|     Date|
+---------+
| 1/5/2019|
| 3/8/2019|
| 3/3/2019|
|1/27/2019|
| 2/8/2019|
|3/25/2019|
|2/25/2019|
|2/24/2019|
|1/10/2019|
|2/20/2019|
+---------+

22/06/10 00:15:47 INFO FileSourceStrategy: Pruning directories with:
22/06/10 00:15:47 INFO FileSourceStrategy: Pushed Filters:
22/06/10 00:15:47 INFO FileSourceStrategy: Post-Scan Filters:
22/06/10 00:15:47 INFO FileSourceStrategy: Output Data Schema: struct<Date: string>
Traceback (most recent call last):
  File "C:/Users/Malcolm Lewis/Desktop/Pipeline Pet Project/Spark Pipeline.py", line 36, in <module>
    output.write.csv("filtered.csv")
  File "C:\Python37\lib\site-packages\pyspark\python\lib\pyspark.zip\pyspark\sql\readwriter.py", line 1027, in csv
  File "C:\Python37\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1305, in __call__
  File "C:\Python37\lib\site-packages\pyspark\python\lib\pyspark.zip\pyspark\sql\utils.py", line 137, in deco
  File "<string>", line 3, in raise_from
pyspark.sql.utils.AnalysisException: path file:/C:/Users/Malcolm Lewis/Desktop/Pipeline Pet Project/filtered.csv already exists.;
22/06/10 00:15:47 INFO SparkContext: Invoking stop() from shutdown hook
22/06/10 00:15:47 INFO SparkUI: Stopped Spark web UI at http://MalcolmSurfacePro4.myfiosgateway.com:4040
22/06/10 00:15:47 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
22/06/10 00:15:47 INFO MemoryStore: MemoryStore cleared
22/06/10 00:15:47 INFO BlockManager: BlockManager stopped
22/06/10 00:15:47 INFO BlockManagerMaster: BlockManagerMaster stopped
22/06/10 00:15:47 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
22/06/10 00:15:47 INFO SparkContext: Successfully stopped SparkContext
22/06/10 00:15:47 INFO ShutdownHookManager: Shutdown hook called
22/06/10 00:15:47 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-e1f36eed-2109-4f55-936d-28f886cf0089
22/06/10 00:15:47 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-e1f36eed-2109-4f55-936d-28f886cf0089\pyspark-eeaa36f4-92a6-4881-b5ae-9419f7b4153e
22/06/10 00:15:47 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-93edc7ff-2142-4767-a0ee-aafeb568ff5e

C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project>spark-submit "Spark Pipeline.py"
22/06/10 00:15:58 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable C:\hadoop-common-2.2.0-bin-master\bin\winutils\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2412)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2412)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
22/06/10 00:15:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
22/06/10 00:15:59 INFO SparkContext: Running Spark version 3.0.0
22/06/10 00:15:59 INFO ResourceUtils: ==============================================================
22/06/10 00:15:59 INFO ResourceUtils: Resources for spark.driver:

22/06/10 00:15:59 INFO ResourceUtils: ==============================================================
22/06/10 00:15:59 INFO SparkContext: Submitted application: Spark%20Pipeline.py
22/06/10 00:15:59 INFO SecurityManager: Changing view acls to: Malcolm Lewis
22/06/10 00:15:59 INFO SecurityManager: Changing modify acls to: Malcolm Lewis
22/06/10 00:15:59 INFO SecurityManager: Changing view acls groups to:
22/06/10 00:15:59 INFO SecurityManager: Changing modify acls groups to:
22/06/10 00:15:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Malcolm Lewis); groups with view permissions: Set(); users  with modify permissions: Set(Malcolm Lewis); groups with modify permissions: Set()
22/06/10 00:16:01 INFO Utils: Successfully started service 'sparkDriver' on port 50375.
22/06/10 00:16:01 INFO SparkEnv: Registering MapOutputTracker
22/06/10 00:16:01 INFO SparkEnv: Registering BlockManagerMaster
22/06/10 00:16:01 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
22/06/10 00:16:01 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
22/06/10 00:16:01 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
22/06/10 00:16:01 INFO DiskBlockManager: Created local directory at C:\Users\Malcolm Lewis\AppData\Local\Temp\blockmgr-8b11dfa1-992d-4152-8eaf-87f17d7eb439
22/06/10 00:16:01 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
22/06/10 00:16:01 INFO SparkEnv: Registering OutputCommitCoordinator
22/06/10 00:16:01 INFO Utils: Successfully started service 'SparkUI' on port 4040.
22/06/10 00:16:01 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://MalcolmSurfacePro4.myfiosgateway.com:4040
22/06/10 00:16:02 INFO Executor: Starting executor ID driver on host MalcolmSurfacePro4.myfiosgateway.com
22/06/10 00:16:02 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50398.
22/06/10 00:16:02 INFO NettyBlockTransferService: Server created on MalcolmSurfacePro4.myfiosgateway.com:50398
22/06/10 00:16:02 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
22/06/10 00:16:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 50398, None)
22/06/10 00:16:02 INFO BlockManagerMasterEndpoint: Registering block manager MalcolmSurfacePro4.myfiosgateway.com:50398 with 366.3 MiB RAM, BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 50398, None)
22/06/10 00:16:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 50398, None)
22/06/10 00:16:02 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 50398, None)
22/06/10 00:16:03 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/C:/Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/spark-warehouse').
22/06/10 00:16:03 INFO SharedState: Warehouse path is 'file:/C:/Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/spark-warehouse'.
22/06/10 00:16:03 INFO InMemoryFileIndex: It took 36 ms to list leaf files for 1 paths.
22/06/10 00:16:04 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
22/06/10 00:16:06 INFO FileSourceStrategy: Pruning directories with:
22/06/10 00:16:06 INFO FileSourceStrategy: Pushed Filters:
22/06/10 00:16:06 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
22/06/10 00:16:06 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
22/06/10 00:16:07 INFO CodeGenerator: Code generated in 273.1932 ms
22/06/10 00:16:07 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 285.2 KiB, free 366.0 MiB)
22/06/10 00:16:07 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 366.0 MiB)
22/06/10 00:16:07 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50398 (size: 23.9 KiB, free: 366.3 MiB)
22/06/10 00:16:07 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
22/06/10 00:16:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 00:16:07 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
22/06/10 00:16:07 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/10 00:16:07 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
22/06/10 00:16:07 INFO DAGScheduler: Parents of final stage: List()
22/06/10 00:16:07 INFO DAGScheduler: Missing parents: List()
22/06/10 00:16:07 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 00:16:08 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.7 KiB, free 366.0 MiB)
22/06/10 00:16:08 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 366.0 MiB)
22/06/10 00:16:08 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50398 (size: 5.3 KiB, free: 366.3 MiB)
22/06/10 00:16:08 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1200
22/06/10 00:16:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 00:16:08 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
22/06/10 00:16:08 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7781 bytes)
22/06/10 00:16:08 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
22/06/10 00:16:08 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/10 00:16:08 INFO CodeGenerator: Code generated in 16.1456 ms
22/06/10 00:16:08 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1673 bytes result sent to driver
22/06/10 00:16:08 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 549 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/10 00:16:08 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
22/06/10 00:16:08 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 0.787 s
22/06/10 00:16:08 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
22/06/10 00:16:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
22/06/10 00:16:08 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 1.102593 s
22/06/10 00:16:09 INFO CodeGenerator: Code generated in 27.8846 ms
22/06/10 00:16:09 INFO FileSourceStrategy: Pruning directories with:
22/06/10 00:16:09 INFO FileSourceStrategy: Pushed Filters:
22/06/10 00:16:09 INFO FileSourceStrategy: Post-Scan Filters:
22/06/10 00:16:09 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
22/06/10 00:16:09 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 285.2 KiB, free 365.7 MiB)
22/06/10 00:16:09 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.7 MiB)
22/06/10 00:16:09 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50398 (size: 23.9 KiB, free: 366.2 MiB)
22/06/10 00:16:09 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
22/06/10 00:16:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 00:16:09 INFO FileSourceStrategy: Pruning directories with:
22/06/10 00:16:09 INFO FileSourceStrategy: Pushed Filters:
22/06/10 00:16:09 INFO FileSourceStrategy: Post-Scan Filters:
22/06/10 00:16:09 INFO FileSourceStrategy: Output Data Schema: struct<Date: string>
22/06/10 00:16:09 INFO CodeGenerator: Code generated in 11.2413 ms
22/06/10 00:16:09 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 285.2 KiB, free 365.4 MiB)
22/06/10 00:16:09 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.4 MiB)
22/06/10 00:16:09 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50398 (size: 23.9 KiB, free: 366.2 MiB)
22/06/10 00:16:09 INFO SparkContext: Created broadcast 3 from showString at NativeMethodAccessorImpl.java:0
22/06/10 00:16:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 00:16:09 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
22/06/10 00:16:09 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/10 00:16:09 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)
22/06/10 00:16:09 INFO DAGScheduler: Parents of final stage: List()
22/06/10 00:16:09 INFO DAGScheduler: Missing parents: List()
22/06/10 00:16:09 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 00:16:09 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 12.1 KiB, free 365.4 MiB)
22/06/10 00:16:09 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 365.4 MiB)
22/06/10 00:16:09 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50398 (size: 6.2 KiB, free: 366.2 MiB)
22/06/10 00:16:09 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1200
22/06/10 00:16:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 00:16:09 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
22/06/10 00:16:09 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7781 bytes)
22/06/10 00:16:09 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
22/06/10 00:16:09 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/10 00:16:09 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1543 bytes result sent to driver
22/06/10 00:16:09 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 68 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/10 00:16:09 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.113 s
22/06/10 00:16:09 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
22/06/10 00:16:09 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
22/06/10 00:16:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
22/06/10 00:16:09 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.129939 s
22/06/10 00:16:09 INFO CodeGenerator: Code generated in 15.5416 ms
+---------+
|     Date|
+---------+
| 1/5/2019|
| 3/8/2019|
| 3/3/2019|
|1/27/2019|
| 2/8/2019|
|3/25/2019|
|2/25/2019|
|2/24/2019|
|1/10/2019|
|2/20/2019|
+---------+

22/06/10 00:16:10 INFO FileSourceStrategy: Pruning directories with:
22/06/10 00:16:10 INFO FileSourceStrategy: Pushed Filters:
22/06/10 00:16:10 INFO FileSourceStrategy: Post-Scan Filters:
22/06/10 00:16:10 INFO FileSourceStrategy: Output Data Schema: struct<Date: string>
22/06/10 00:16:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/06/10 00:16:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
22/06/10 00:16:10 INFO CodeGenerator: Code generated in 34.873 ms
22/06/10 00:16:10 INFO CodeGenerator: Code generated in 28.2626 ms
22/06/10 00:16:10 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 285.2 KiB, free 365.1 MiB)
22/06/10 00:16:10 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.1 MiB)
22/06/10 00:16:10 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50398 (size: 23.9 KiB, free: 366.2 MiB)
22/06/10 00:16:10 INFO SparkContext: Created broadcast 5 from csv at NativeMethodAccessorImpl.java:0
22/06/10 00:16:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 00:16:10 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
22/06/10 00:16:10 INFO DAGScheduler: Registering RDD 17 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0
22/06/10 00:16:10 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/10 00:16:10 INFO DAGScheduler: Final stage: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0)
22/06/10 00:16:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
22/06/10 00:16:10 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
22/06/10 00:16:10 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[17] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 00:16:10 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 14.8 KiB, free 365.0 MiB)
22/06/10 00:16:10 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 365.0 MiB)
22/06/10 00:16:10 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50398 (size: 7.5 KiB, free: 366.2 MiB)
22/06/10 00:16:10 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1200
22/06/10 00:16:10 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[17] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 00:16:10 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
22/06/10 00:16:10 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7770 bytes)
22/06/10 00:16:10 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
22/06/10 00:16:10 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/10 00:16:10 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1842 bytes result sent to driver
22/06/10 00:16:10 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 116 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/10 00:16:10 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
22/06/10 00:16:10 INFO DAGScheduler: ShuffleMapStage 2 (csv at NativeMethodAccessorImpl.java:0) finished in 0.150 s
22/06/10 00:16:10 INFO DAGScheduler: looking for newly runnable stages
22/06/10 00:16:10 INFO DAGScheduler: running: Set()
22/06/10 00:16:10 INFO DAGScheduler: waiting: Set(ResultStage 3)
22/06/10 00:16:10 INFO DAGScheduler: failed: Set()
22/06/10 00:16:10 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 00:16:10 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 151.2 KiB, free 364.9 MiB)
22/06/10 00:16:10 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 54.7 KiB, free 364.8 MiB)
22/06/10 00:16:10 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50398 (size: 54.7 KiB, free: 366.1 MiB)
22/06/10 00:16:10 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1200
22/06/10 00:16:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 00:16:10 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
22/06/10 00:16:10 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
22/06/10 00:16:10 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
22/06/10 00:16:10 INFO ShuffleBlockFetcherIterator: Getting 1 (156.0 B) non-empty blocks including 1 (156.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
22/06/10 00:16:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
22/06/10 00:16:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/06/10 00:16:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
22/06/10 00:16:10 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3)
java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610001610_0003_m_000000_3\part-00000-fcbec2b9-2857-44a6-ab13-f35fe1997ec4-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
22/06/10 00:16:10 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610001610_0003_m_000000_3\part-00000-fcbec2b9-2857-44a6-ab13-f35fe1997ec4-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

22/06/10 00:16:10 ERROR TaskSetManager: Task 0 in stage 3.0 failed 1 times; aborting job
22/06/10 00:16:10 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
22/06/10 00:16:10 INFO TaskSchedulerImpl: Cancelling stage 3
22/06/10 00:16:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage cancelled
22/06/10 00:16:10 INFO DAGScheduler: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0) failed in 0.275 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610001610_0003_m_000000_3\part-00000-fcbec2b9-2857-44a6-ab13-f35fe1997ec4-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
22/06/10 00:16:10 INFO DAGScheduler: Job 2 failed: csv at NativeMethodAccessorImpl.java:0, took 0.488616 s
22/06/10 00:16:10 ERROR FileFormatWriter: Aborting job 1f7f8801-b007-4d6a-8ce1-4276072cafe9.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610001610_0003_m_000000_3\part-00000-fcbec2b9-2857-44a6-ab13-f35fe1997ec4-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)
        at scala.Option.foreach(Option.scala:407)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)
        at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)
        at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)
        at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
        at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)
        at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610001610_0003_m_000000_3\part-00000-fcbec2b9-2857-44a6-ab13-f35fe1997ec4-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        ... 1 more
Traceback (most recent call last):
  File "C:/Users/Malcolm Lewis/Desktop/Pipeline Pet Project/Spark Pipeline.py", line 36, in <module>
    output.write.csv("filtered.csv")
  File "C:\Python37\lib\site-packages\pyspark\python\lib\pyspark.zip\pyspark\sql\readwriter.py", line 1027, in csv
  File "C:\Python37\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1305, in __call__
  File "C:\Python37\lib\site-packages\pyspark\python\lib\pyspark.zip\pyspark\sql\utils.py", line 131, in deco
  File "C:\Python37\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o29.csv.
: org.apache.spark.SparkException: Job aborted.
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)
        at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)
        at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)
        at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
        at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)
        at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610001610_0003_m_000000_3\part-00000-fcbec2b9-2857-44a6-ab13-f35fe1997ec4-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)
        at scala.Option.foreach(Option.scala:407)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)
        ... 33 more
Caused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610001610_0003_m_000000_3\part-00000-fcbec2b9-2857-44a6-ab13-f35fe1997ec4-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        ... 1 more

22/06/10 00:16:10 INFO SparkContext: Invoking stop() from shutdown hook
22/06/10 00:16:11 INFO SparkUI: Stopped Spark web UI at http://MalcolmSurfacePro4.myfiosgateway.com:4040
22/06/10 00:16:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
22/06/10 00:16:11 INFO MemoryStore: MemoryStore cleared
22/06/10 00:16:11 INFO BlockManager: BlockManager stopped
22/06/10 00:16:11 INFO BlockManagerMaster: BlockManagerMaster stopped
22/06/10 00:16:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
22/06/10 00:16:11 INFO SparkContext: Successfully stopped SparkContext
22/06/10 00:16:11 INFO ShutdownHookManager: Shutdown hook called
22/06/10 00:16:11 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-4d31c1ab-29fd-427a-9073-bacbe45f31ef
22/06/10 00:16:11 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-4d31c1ab-29fd-427a-9073-bacbe45f31ef\pyspark-be40fc74-ab2b-4e80-8b6a-87d59c6e347e
22/06/10 00:16:11 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-314ad6f0-02bc-496f-87e5-d12446d5c178

C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project>spark-submit "Spark Pipeline.py"
22/06/10 00:29:30 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2412)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2412)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
22/06/10 00:29:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
22/06/10 00:29:31 INFO SparkContext: Running Spark version 3.0.0
22/06/10 00:29:31 INFO ResourceUtils: ==============================================================
22/06/10 00:29:31 INFO ResourceUtils: Resources for spark.driver:

22/06/10 00:29:31 INFO ResourceUtils: ==============================================================
22/06/10 00:29:31 INFO SparkContext: Submitted application: Spark%20Pipeline.py
22/06/10 00:29:31 INFO SecurityManager: Changing view acls to: Malcolm Lewis
22/06/10 00:29:31 INFO SecurityManager: Changing modify acls to: Malcolm Lewis
22/06/10 00:29:31 INFO SecurityManager: Changing view acls groups to:
22/06/10 00:29:31 INFO SecurityManager: Changing modify acls groups to:
22/06/10 00:29:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Malcolm Lewis); groups with view permissions: Set(); users  with modify permissions: Set(Malcolm Lewis); groups with modify permissions: Set()
22/06/10 00:29:32 INFO Utils: Successfully started service 'sparkDriver' on port 50494.
22/06/10 00:29:32 INFO SparkEnv: Registering MapOutputTracker
22/06/10 00:29:32 INFO SparkEnv: Registering BlockManagerMaster
22/06/10 00:29:32 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
22/06/10 00:29:32 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
22/06/10 00:29:32 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
22/06/10 00:29:32 INFO DiskBlockManager: Created local directory at C:\Users\Malcolm Lewis\AppData\Local\Temp\blockmgr-7c2fd339-f238-4d7a-b905-9bcd59e6c431
22/06/10 00:29:32 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
22/06/10 00:29:33 INFO SparkEnv: Registering OutputCommitCoordinator
22/06/10 00:29:33 INFO Utils: Successfully started service 'SparkUI' on port 4040.
22/06/10 00:29:33 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://MalcolmSurfacePro4.myfiosgateway.com:4040
22/06/10 00:29:33 INFO Executor: Starting executor ID driver on host MalcolmSurfacePro4.myfiosgateway.com
22/06/10 00:29:33 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50517.
22/06/10 00:29:33 INFO NettyBlockTransferService: Server created on MalcolmSurfacePro4.myfiosgateway.com:50517
22/06/10 00:29:33 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
22/06/10 00:29:33 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 50517, None)
22/06/10 00:29:33 INFO BlockManagerMasterEndpoint: Registering block manager MalcolmSurfacePro4.myfiosgateway.com:50517 with 366.3 MiB RAM, BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 50517, None)
22/06/10 00:29:33 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 50517, None)
22/06/10 00:29:33 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 50517, None)
22/06/10 00:29:34 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/C:/Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/spark-warehouse').
22/06/10 00:29:34 INFO SharedState: Warehouse path is 'file:/C:/Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/spark-warehouse'.
22/06/10 00:29:35 INFO InMemoryFileIndex: It took 30 ms to list leaf files for 1 paths.
22/06/10 00:29:35 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
22/06/10 00:29:37 INFO FileSourceStrategy: Pruning directories with:
22/06/10 00:29:37 INFO FileSourceStrategy: Pushed Filters:
22/06/10 00:29:37 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
22/06/10 00:29:37 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
22/06/10 00:29:38 INFO CodeGenerator: Code generated in 306.2228 ms
22/06/10 00:29:38 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 285.2 KiB, free 366.0 MiB)
22/06/10 00:29:38 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 366.0 MiB)
22/06/10 00:29:38 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50517 (size: 23.9 KiB, free: 366.3 MiB)
22/06/10 00:29:38 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
22/06/10 00:29:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 00:29:39 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
22/06/10 00:29:39 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/10 00:29:39 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
22/06/10 00:29:39 INFO DAGScheduler: Parents of final stage: List()
22/06/10 00:29:39 INFO DAGScheduler: Missing parents: List()
22/06/10 00:29:39 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 00:29:39 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.7 KiB, free 366.0 MiB)
22/06/10 00:29:39 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 366.0 MiB)
22/06/10 00:29:39 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50517 (size: 5.3 KiB, free: 366.3 MiB)
22/06/10 00:29:39 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1200
22/06/10 00:29:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 00:29:39 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
22/06/10 00:29:39 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7781 bytes)
22/06/10 00:29:39 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
22/06/10 00:29:39 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/10 00:29:39 INFO CodeGenerator: Code generated in 13.4402 ms
22/06/10 00:29:39 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1673 bytes result sent to driver
22/06/10 00:29:39 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 418 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/10 00:29:39 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
22/06/10 00:29:39 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 0.578 s
22/06/10 00:29:39 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
22/06/10 00:29:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
22/06/10 00:29:39 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 0.807976 s
22/06/10 00:29:39 INFO CodeGenerator: Code generated in 13.9857 ms
22/06/10 00:29:39 INFO FileSourceStrategy: Pruning directories with:
22/06/10 00:29:39 INFO FileSourceStrategy: Pushed Filters:
22/06/10 00:29:39 INFO FileSourceStrategy: Post-Scan Filters:
22/06/10 00:29:39 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
22/06/10 00:29:39 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 285.2 KiB, free 365.7 MiB)
22/06/10 00:29:39 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.7 MiB)
22/06/10 00:29:39 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50517 (size: 23.9 KiB, free: 366.2 MiB)
22/06/10 00:29:39 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
22/06/10 00:29:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 00:29:40 INFO FileSourceStrategy: Pruning directories with:
22/06/10 00:29:40 INFO FileSourceStrategy: Pushed Filters:
22/06/10 00:29:40 INFO FileSourceStrategy: Post-Scan Filters:
22/06/10 00:29:40 INFO FileSourceStrategy: Output Data Schema: struct<Date: string>
22/06/10 00:29:40 INFO CodeGenerator: Code generated in 10.9648 ms
22/06/10 00:29:40 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 285.2 KiB, free 365.4 MiB)
22/06/10 00:29:40 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.4 MiB)
22/06/10 00:29:40 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50517 (size: 23.9 KiB, free: 366.2 MiB)
22/06/10 00:29:40 INFO SparkContext: Created broadcast 3 from showString at NativeMethodAccessorImpl.java:0
22/06/10 00:29:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 00:29:40 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
22/06/10 00:29:40 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/10 00:29:40 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)
22/06/10 00:29:40 INFO DAGScheduler: Parents of final stage: List()
22/06/10 00:29:40 INFO DAGScheduler: Missing parents: List()
22/06/10 00:29:40 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 00:29:40 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 12.1 KiB, free 365.4 MiB)
22/06/10 00:29:40 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 365.4 MiB)
22/06/10 00:29:40 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50517 (size: 6.2 KiB, free: 366.2 MiB)
22/06/10 00:29:40 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1200
22/06/10 00:29:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 00:29:40 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
22/06/10 00:29:40 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7781 bytes)
22/06/10 00:29:40 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
22/06/10 00:29:40 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/10 00:29:40 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1543 bytes result sent to driver
22/06/10 00:29:40 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 51 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/10 00:29:40 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.085 s
22/06/10 00:29:40 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
22/06/10 00:29:40 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
22/06/10 00:29:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
22/06/10 00:29:40 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.098354 s
22/06/10 00:29:40 INFO CodeGenerator: Code generated in 14.0099 ms
+---------+
|     Date|
+---------+
| 1/5/2019|
| 3/8/2019|
| 3/3/2019|
|1/27/2019|
| 2/8/2019|
|3/25/2019|
|2/25/2019|
|2/24/2019|
|1/10/2019|
|2/20/2019|
+---------+

22/06/10 00:29:40 INFO FileSourceStrategy: Pruning directories with:
22/06/10 00:29:40 INFO FileSourceStrategy: Pushed Filters:
22/06/10 00:29:40 INFO FileSourceStrategy: Post-Scan Filters:
22/06/10 00:29:40 INFO FileSourceStrategy: Output Data Schema: struct<Date: string>
22/06/10 00:29:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/06/10 00:29:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
22/06/10 00:29:40 INFO CodeGenerator: Code generated in 9.6992 ms
22/06/10 00:29:40 INFO CodeGenerator: Code generated in 12.3811 ms
22/06/10 00:29:40 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 285.2 KiB, free 365.1 MiB)
22/06/10 00:29:40 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.1 MiB)
22/06/10 00:29:40 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50517 (size: 23.9 KiB, free: 366.2 MiB)
22/06/10 00:29:40 INFO SparkContext: Created broadcast 5 from csv at NativeMethodAccessorImpl.java:0
22/06/10 00:29:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 00:29:40 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
22/06/10 00:29:40 INFO DAGScheduler: Registering RDD 17 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0
22/06/10 00:29:40 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/10 00:29:40 INFO DAGScheduler: Final stage: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0)
22/06/10 00:29:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
22/06/10 00:29:40 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
22/06/10 00:29:40 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[17] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 00:29:40 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 14.8 KiB, free 365.0 MiB)
22/06/10 00:29:40 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 365.0 MiB)
22/06/10 00:29:40 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50517 (size: 7.5 KiB, free: 366.2 MiB)
22/06/10 00:29:40 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1200
22/06/10 00:29:40 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[17] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 00:29:40 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
22/06/10 00:29:40 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7770 bytes)
22/06/10 00:29:40 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
22/06/10 00:29:40 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/10 00:29:41 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1842 bytes result sent to driver
22/06/10 00:29:41 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 78 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/10 00:29:41 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
22/06/10 00:29:41 INFO DAGScheduler: ShuffleMapStage 2 (csv at NativeMethodAccessorImpl.java:0) finished in 0.107 s
22/06/10 00:29:41 INFO DAGScheduler: looking for newly runnable stages
22/06/10 00:29:41 INFO DAGScheduler: running: Set()
22/06/10 00:29:41 INFO DAGScheduler: waiting: Set(ResultStage 3)
22/06/10 00:29:41 INFO DAGScheduler: failed: Set()
22/06/10 00:29:41 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 00:29:41 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 151.2 KiB, free 364.9 MiB)
22/06/10 00:29:41 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 54.7 KiB, free 364.8 MiB)
22/06/10 00:29:41 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50517 (size: 54.7 KiB, free: 366.1 MiB)
22/06/10 00:29:41 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1200
22/06/10 00:29:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 00:29:41 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
22/06/10 00:29:41 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
22/06/10 00:29:41 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
22/06/10 00:29:41 INFO ShuffleBlockFetcherIterator: Getting 1 (156.0 B) non-empty blocks including 1 (156.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
22/06/10 00:29:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
22/06/10 00:29:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/06/10 00:29:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
22/06/10 00:29:41 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3)
java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610002940_0003_m_000000_3\part-00000-c1a3bc45-2764-415a-a739-a74bb0b3135c-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
22/06/10 00:29:41 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610002940_0003_m_000000_3\part-00000-c1a3bc45-2764-415a-a739-a74bb0b3135c-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

22/06/10 00:29:41 ERROR TaskSetManager: Task 0 in stage 3.0 failed 1 times; aborting job
22/06/10 00:29:41 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
22/06/10 00:29:41 INFO TaskSchedulerImpl: Cancelling stage 3
22/06/10 00:29:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage cancelled
22/06/10 00:29:41 INFO DAGScheduler: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0) failed in 0.203 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610002940_0003_m_000000_3\part-00000-c1a3bc45-2764-415a-a739-a74bb0b3135c-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
22/06/10 00:29:41 INFO DAGScheduler: Job 2 failed: csv at NativeMethodAccessorImpl.java:0, took 0.352366 s
22/06/10 00:29:41 ERROR FileFormatWriter: Aborting job 8cf30ef7-69fd-4922-8007-33547387c46d.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610002940_0003_m_000000_3\part-00000-c1a3bc45-2764-415a-a739-a74bb0b3135c-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)
        at scala.Option.foreach(Option.scala:407)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)
        at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)
        at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)
        at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
        at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)
        at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610002940_0003_m_000000_3\part-00000-c1a3bc45-2764-415a-a739-a74bb0b3135c-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        ... 1 more
Traceback (most recent call last):
  File "C:/Users/Malcolm Lewis/Desktop/Pipeline Pet Project/Spark Pipeline.py", line 36, in <module>
    output.write.csv("filtered.csv")
  File "C:\Python37\lib\site-packages\pyspark\python\lib\pyspark.zip\pyspark\sql\readwriter.py", line 1027, in csv
  File "C:\Python37\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1305, in __call__
  File "C:\Python37\lib\site-packages\pyspark\python\lib\pyspark.zip\pyspark\sql\utils.py", line 131, in deco
  File "C:\Python37\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o29.csv.
: org.apache.spark.SparkException: Job aborted.
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)
        at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)
        at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)
        at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
        at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)
        at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610002940_0003_m_000000_3\part-00000-c1a3bc45-2764-415a-a739-a74bb0b3135c-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)
        at scala.Option.foreach(Option.scala:407)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)
        ... 33 more
Caused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610002940_0003_m_000000_3\part-00000-c1a3bc45-2764-415a-a739-a74bb0b3135c-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        ... 1 more

22/06/10 00:29:41 INFO SparkContext: Invoking stop() from shutdown hook
22/06/10 00:29:41 INFO SparkUI: Stopped Spark web UI at http://MalcolmSurfacePro4.myfiosgateway.com:4040
22/06/10 00:29:41 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
22/06/10 00:29:41 INFO MemoryStore: MemoryStore cleared
22/06/10 00:29:41 INFO BlockManager: BlockManager stopped
22/06/10 00:29:41 INFO BlockManagerMaster: BlockManagerMaster stopped
22/06/10 00:29:41 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
22/06/10 00:29:41 INFO SparkContext: Successfully stopped SparkContext
22/06/10 00:29:41 INFO ShutdownHookManager: Shutdown hook called
22/06/10 00:29:41 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-f91d3a51-26ea-4c61-953a-30f76686ecf1\pyspark-eaa4bbb3-71b7-42b4-a3d9-20e4563ce07d
22/06/10 00:29:41 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-6d354caa-8f67-415f-8cd2-511f9233df59
22/06/10 00:29:41 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-f91d3a51-26ea-4c61-953a-30f76686ecf1

C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project>spark-submit "Spark Pipeline.py"
22/06/10 00:32:02 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2412)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2412)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
22/06/10 00:32:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
22/06/10 00:32:03 INFO SparkContext: Running Spark version 3.0.0
22/06/10 00:32:04 INFO ResourceUtils: ==============================================================
22/06/10 00:32:04 INFO ResourceUtils: Resources for spark.driver:

22/06/10 00:32:04 INFO ResourceUtils: ==============================================================
22/06/10 00:32:04 INFO SparkContext: Submitted application: Spark%20Pipeline.py
22/06/10 00:32:04 INFO SecurityManager: Changing view acls to: Malcolm Lewis
22/06/10 00:32:04 INFO SecurityManager: Changing modify acls to: Malcolm Lewis
22/06/10 00:32:04 INFO SecurityManager: Changing view acls groups to:
22/06/10 00:32:04 INFO SecurityManager: Changing modify acls groups to:
22/06/10 00:32:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Malcolm Lewis); groups with view permissions: Set(); users  with modify permissions: Set(Malcolm Lewis); groups with modify permissions: Set()
22/06/10 00:32:05 INFO Utils: Successfully started service 'sparkDriver' on port 50566.
22/06/10 00:32:05 INFO SparkEnv: Registering MapOutputTracker
22/06/10 00:32:05 INFO SparkEnv: Registering BlockManagerMaster
22/06/10 00:32:05 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
22/06/10 00:32:05 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
22/06/10 00:32:05 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
22/06/10 00:32:05 INFO DiskBlockManager: Created local directory at C:\Users\Malcolm Lewis\AppData\Local\Temp\blockmgr-885e1a53-378e-42cd-b37f-c2c5ce3dd149
22/06/10 00:32:05 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
22/06/10 00:32:05 INFO SparkEnv: Registering OutputCommitCoordinator
22/06/10 00:32:06 INFO Utils: Successfully started service 'SparkUI' on port 4040.
22/06/10 00:32:06 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://MalcolmSurfacePro4.myfiosgateway.com:4040
22/06/10 00:32:06 INFO Executor: Starting executor ID driver on host MalcolmSurfacePro4.myfiosgateway.com
22/06/10 00:32:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50589.
22/06/10 00:32:06 INFO NettyBlockTransferService: Server created on MalcolmSurfacePro4.myfiosgateway.com:50589
22/06/10 00:32:06 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
22/06/10 00:32:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 50589, None)
22/06/10 00:32:06 INFO BlockManagerMasterEndpoint: Registering block manager MalcolmSurfacePro4.myfiosgateway.com:50589 with 366.3 MiB RAM, BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 50589, None)
22/06/10 00:32:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 50589, None)
22/06/10 00:32:06 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 50589, None)
22/06/10 00:32:07 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/C:/Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/spark-warehouse').
22/06/10 00:32:07 INFO SharedState: Warehouse path is 'file:/C:/Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/spark-warehouse'.
22/06/10 00:32:08 INFO InMemoryFileIndex: It took 33 ms to list leaf files for 1 paths.
22/06/10 00:32:08 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
22/06/10 00:32:10 INFO FileSourceStrategy: Pruning directories with:
22/06/10 00:32:10 INFO FileSourceStrategy: Pushed Filters:
22/06/10 00:32:10 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
22/06/10 00:32:10 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
22/06/10 00:32:11 INFO CodeGenerator: Code generated in 249.7748 ms
22/06/10 00:32:11 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 285.2 KiB, free 366.0 MiB)
22/06/10 00:32:11 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 366.0 MiB)
22/06/10 00:32:11 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50589 (size: 23.9 KiB, free: 366.3 MiB)
22/06/10 00:32:11 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
22/06/10 00:32:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 00:32:11 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
22/06/10 00:32:11 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/10 00:32:11 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
22/06/10 00:32:11 INFO DAGScheduler: Parents of final stage: List()
22/06/10 00:32:11 INFO DAGScheduler: Missing parents: List()
22/06/10 00:32:11 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 00:32:11 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.7 KiB, free 366.0 MiB)
22/06/10 00:32:11 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 366.0 MiB)
22/06/10 00:32:11 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50589 (size: 5.3 KiB, free: 366.3 MiB)
22/06/10 00:32:11 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1200
22/06/10 00:32:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 00:32:11 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
22/06/10 00:32:11 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7781 bytes)
22/06/10 00:32:11 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
22/06/10 00:32:12 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/10 00:32:12 INFO CodeGenerator: Code generated in 32.5082 ms
22/06/10 00:32:12 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1673 bytes result sent to driver
22/06/10 00:32:12 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 539 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/10 00:32:12 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
22/06/10 00:32:12 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 0.679 s
22/06/10 00:32:12 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
22/06/10 00:32:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
22/06/10 00:32:12 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 0.933672 s
22/06/10 00:32:12 INFO CodeGenerator: Code generated in 9.8039 ms
22/06/10 00:32:12 INFO FileSourceStrategy: Pruning directories with:
22/06/10 00:32:12 INFO FileSourceStrategy: Pushed Filters:
22/06/10 00:32:12 INFO FileSourceStrategy: Post-Scan Filters:
22/06/10 00:32:12 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
22/06/10 00:32:12 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 285.2 KiB, free 365.7 MiB)
22/06/10 00:32:12 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.7 MiB)
22/06/10 00:32:12 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50589 (size: 23.9 KiB, free: 366.2 MiB)
22/06/10 00:32:12 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
22/06/10 00:32:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 00:32:13 INFO FileSourceStrategy: Pruning directories with:
22/06/10 00:32:13 INFO FileSourceStrategy: Pushed Filters:
22/06/10 00:32:13 INFO FileSourceStrategy: Post-Scan Filters:
22/06/10 00:32:13 INFO FileSourceStrategy: Output Data Schema: struct<Date: string>
22/06/10 00:32:13 INFO CodeGenerator: Code generated in 9.9479 ms
22/06/10 00:32:13 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 285.2 KiB, free 365.4 MiB)
22/06/10 00:32:13 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.4 MiB)
22/06/10 00:32:13 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50589 (size: 23.9 KiB, free: 366.2 MiB)
22/06/10 00:32:13 INFO SparkContext: Created broadcast 3 from showString at NativeMethodAccessorImpl.java:0
22/06/10 00:32:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 00:32:13 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
22/06/10 00:32:13 INFO DAGScheduler: Registering RDD 14 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 0
22/06/10 00:32:13 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/10 00:32:13 INFO DAGScheduler: Final stage: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0)
22/06/10 00:32:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
22/06/10 00:32:13 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)
22/06/10 00:32:13 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[14] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 00:32:13 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 17.3 KiB, free 365.4 MiB)
22/06/10 00:32:13 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 8.7 KiB, free 365.4 MiB)
22/06/10 00:32:13 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50589 (size: 8.7 KiB, free: 366.2 MiB)
22/06/10 00:32:13 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1200
22/06/10 00:32:13 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[14] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 00:32:13 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
22/06/10 00:32:13 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7770 bytes)
22/06/10 00:32:13 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
22/06/10 00:32:13 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/10 00:32:13 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2634 bytes result sent to driver
22/06/10 00:32:13 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 111 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/10 00:32:13 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
22/06/10 00:32:13 INFO DAGScheduler: ShuffleMapStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.178 s
22/06/10 00:32:13 INFO DAGScheduler: looking for newly runnable stages
22/06/10 00:32:13 INFO DAGScheduler: running: Set()
22/06/10 00:32:13 INFO DAGScheduler: waiting: Set(ResultStage 2)
22/06/10 00:32:13 INFO DAGScheduler: failed: Set()
22/06/10 00:32:13 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[20] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 00:32:13 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 19.5 KiB, free 365.3 MiB)
22/06/10 00:32:13 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 365.3 MiB)
22/06/10 00:32:13 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50589 (size: 9.7 KiB, free: 366.2 MiB)
22/06/10 00:32:13 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1200
22/06/10 00:32:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[20] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 00:32:13 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
22/06/10 00:32:13 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
22/06/10 00:32:13 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
22/06/10 00:32:13 INFO ShuffleBlockFetcherIterator: Getting 1 (156.0 B) non-empty blocks including 1 (156.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
22/06/10 00:32:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
22/06/10 00:32:13 INFO MemoryStore: Block rdd_17_0 stored as values in memory (estimated size 616.0 B, free 365.3 MiB)
22/06/10 00:32:13 INFO BlockManagerInfo: Added rdd_17_0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50589 (size: 616.0 B, free: 366.2 MiB)
22/06/10 00:32:13 INFO CodeGenerator: Code generated in 5.1826 ms
22/06/10 00:32:13 INFO CodeGenerator: Code generated in 33.9274 ms
22/06/10 00:32:13 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2716 bytes result sent to driver
22/06/10 00:32:13 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 204 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/10 00:32:13 INFO DAGScheduler: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 0.242 s
22/06/10 00:32:13 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
22/06/10 00:32:13 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
22/06/10 00:32:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
22/06/10 00:32:13 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.467607 s
22/06/10 00:32:14 INFO CodeGenerator: Code generated in 12.8516 ms
+---------+
|     Date|
+---------+
| 1/5/2019|
| 3/8/2019|
| 3/3/2019|
|1/27/2019|
| 2/8/2019|
|3/25/2019|
|2/25/2019|
|2/24/2019|
|1/10/2019|
|2/20/2019|
+---------+

22/06/10 00:32:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/06/10 00:32:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
22/06/10 00:32:14 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
22/06/10 00:32:14 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/10 00:32:14 INFO DAGScheduler: Final stage: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0)
22/06/10 00:32:14 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
22/06/10 00:32:14 INFO DAGScheduler: Missing parents: List()
22/06/10 00:32:14 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[22] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 00:32:14 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 159.8 KiB, free 365.2 MiB)
22/06/10 00:32:14 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 58.3 KiB, free 365.1 MiB)
22/06/10 00:32:14 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50589 (size: 58.3 KiB, free: 366.1 MiB)
22/06/10 00:32:14 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1200
22/06/10 00:32:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[22] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 00:32:14 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
22/06/10 00:32:14 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7325 bytes)
22/06/10 00:32:14 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)
22/06/10 00:32:14 INFO BlockManager: Found block rdd_17_0 locally
22/06/10 00:32:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/06/10 00:32:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
22/06/10 00:32:14 ERROR Executor: Exception in task 0.0 in stage 4.0 (TID 3)
java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610003214_0004_m_000000_3\part-00000-49f8dfb4-3c01-4d93-a65a-aa81c0645eab-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
22/06/10 00:32:14 WARN TaskSetManager: Lost task 0.0 in stage 4.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610003214_0004_m_000000_3\part-00000-49f8dfb4-3c01-4d93-a65a-aa81c0645eab-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

22/06/10 00:32:14 ERROR TaskSetManager: Task 0 in stage 4.0 failed 1 times; aborting job
22/06/10 00:32:14 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
22/06/10 00:32:14 INFO TaskSchedulerImpl: Cancelling stage 4
22/06/10 00:32:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage cancelled
22/06/10 00:32:14 INFO DAGScheduler: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0) failed in 0.178 s due to Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610003214_0004_m_000000_3\part-00000-49f8dfb4-3c01-4d93-a65a-aa81c0645eab-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
22/06/10 00:32:14 INFO DAGScheduler: Job 2 failed: csv at NativeMethodAccessorImpl.java:0, took 0.189916 s
22/06/10 00:32:14 ERROR FileFormatWriter: Aborting job 4a15c6ce-d460-478c-bb59-a2a9d5d61ac5.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610003214_0004_m_000000_3\part-00000-49f8dfb4-3c01-4d93-a65a-aa81c0645eab-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)
        at scala.Option.foreach(Option.scala:407)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)
        at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)
        at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)
        at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
        at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)
        at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610003214_0004_m_000000_3\part-00000-49f8dfb4-3c01-4d93-a65a-aa81c0645eab-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        ... 1 more
Traceback (most recent call last):
  File "C:/Users/Malcolm Lewis/Desktop/Pipeline Pet Project/Spark Pipeline.py", line 36, in <module>
    output.write.csv("filtered.csv")
  File "C:\Python37\lib\site-packages\pyspark\python\lib\pyspark.zip\pyspark\sql\readwriter.py", line 1027, in csv
  File "C:\Python37\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1305, in __call__
  File "C:\Python37\lib\site-packages\pyspark\python\lib\pyspark.zip\pyspark\sql\utils.py", line 131, in deco
  File "C:\Python37\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o31.csv.
: org.apache.spark.SparkException: Job aborted.
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)
        at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)
        at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)
        at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
        at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)
        at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610003214_0004_m_000000_3\part-00000-49f8dfb4-3c01-4d93-a65a-aa81c0645eab-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)
        at scala.Option.foreach(Option.scala:407)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)
        ... 33 more
Caused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610003214_0004_m_000000_3\part-00000-49f8dfb4-3c01-4d93-a65a-aa81c0645eab-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        ... 1 more

22/06/10 00:32:14 INFO SparkContext: Invoking stop() from shutdown hook
22/06/10 00:32:14 INFO SparkUI: Stopped Spark web UI at http://MalcolmSurfacePro4.myfiosgateway.com:4040
22/06/10 00:32:14 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
22/06/10 00:32:14 INFO MemoryStore: MemoryStore cleared
22/06/10 00:32:14 INFO BlockManager: BlockManager stopped
22/06/10 00:32:14 INFO BlockManagerMaster: BlockManagerMaster stopped
22/06/10 00:32:14 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
22/06/10 00:32:14 INFO SparkContext: Successfully stopped SparkContext
22/06/10 00:32:14 INFO ShutdownHookManager: Shutdown hook called
22/06/10 00:32:14 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-4225803a-4404-477e-bf96-ef14866166eb
22/06/10 00:32:14 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-facf35f9-0fb6-4d39-93c1-193a703f1e12
22/06/10 00:32:14 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-4225803a-4404-477e-bf96-ef14866166eb\pyspark-2046a677-1846-4f81-9573-016642148787

C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project>spark-submit "Spark Pipeline.py"
22/06/10 00:32:52 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2412)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2412)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
22/06/10 00:32:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
22/06/10 00:32:53 INFO SparkContext: Running Spark version 3.0.0
22/06/10 00:32:53 INFO ResourceUtils: ==============================================================
22/06/10 00:32:53 INFO ResourceUtils: Resources for spark.driver:

22/06/10 00:32:53 INFO ResourceUtils: ==============================================================
22/06/10 00:32:53 INFO SparkContext: Submitted application: Spark%20Pipeline.py
22/06/10 00:32:53 INFO SecurityManager: Changing view acls to: Malcolm Lewis
22/06/10 00:32:53 INFO SecurityManager: Changing modify acls to: Malcolm Lewis
22/06/10 00:32:53 INFO SecurityManager: Changing view acls groups to:
22/06/10 00:32:53 INFO SecurityManager: Changing modify acls groups to:
22/06/10 00:32:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Malcolm Lewis); groups with view permissions: Set(); users  with modify permissions: Set(Malcolm Lewis); groups with modify permissions: Set()
22/06/10 00:32:54 INFO Utils: Successfully started service 'sparkDriver' on port 50612.
22/06/10 00:32:54 INFO SparkEnv: Registering MapOutputTracker
22/06/10 00:32:54 INFO SparkEnv: Registering BlockManagerMaster
22/06/10 00:32:54 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
22/06/10 00:32:54 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
22/06/10 00:32:54 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
22/06/10 00:32:54 INFO DiskBlockManager: Created local directory at C:\Users\Malcolm Lewis\AppData\Local\Temp\blockmgr-441300f8-222a-476e-be26-e1da7b7dcae7
22/06/10 00:32:54 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
22/06/10 00:32:54 INFO SparkEnv: Registering OutputCommitCoordinator
22/06/10 00:32:55 INFO Utils: Successfully started service 'SparkUI' on port 4040.
22/06/10 00:32:55 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://MalcolmSurfacePro4.myfiosgateway.com:4040
22/06/10 00:32:55 INFO Executor: Starting executor ID driver on host MalcolmSurfacePro4.myfiosgateway.com
22/06/10 00:32:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50635.
22/06/10 00:32:55 INFO NettyBlockTransferService: Server created on MalcolmSurfacePro4.myfiosgateway.com:50635
22/06/10 00:32:55 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
22/06/10 00:32:55 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 50635, None)
22/06/10 00:32:55 INFO BlockManagerMasterEndpoint: Registering block manager MalcolmSurfacePro4.myfiosgateway.com:50635 with 366.3 MiB RAM, BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 50635, None)
22/06/10 00:32:55 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 50635, None)
22/06/10 00:32:55 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 50635, None)
22/06/10 00:32:56 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/C:/Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/spark-warehouse').
22/06/10 00:32:56 INFO SharedState: Warehouse path is 'file:/C:/Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/spark-warehouse'.
22/06/10 00:32:57 INFO InMemoryFileIndex: It took 44 ms to list leaf files for 1 paths.
22/06/10 00:32:57 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
22/06/10 00:32:59 INFO FileSourceStrategy: Pruning directories with:
22/06/10 00:32:59 INFO FileSourceStrategy: Pushed Filters:
22/06/10 00:32:59 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
22/06/10 00:32:59 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
22/06/10 00:33:00 INFO CodeGenerator: Code generated in 339.6878 ms
22/06/10 00:33:00 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 285.2 KiB, free 366.0 MiB)
22/06/10 00:33:00 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 366.0 MiB)
22/06/10 00:33:00 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50635 (size: 23.9 KiB, free: 366.3 MiB)
22/06/10 00:33:00 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
22/06/10 00:33:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 00:33:01 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
22/06/10 00:33:01 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/10 00:33:01 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
22/06/10 00:33:01 INFO DAGScheduler: Parents of final stage: List()
22/06/10 00:33:01 INFO DAGScheduler: Missing parents: List()
22/06/10 00:33:01 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 00:33:01 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.7 KiB, free 366.0 MiB)
22/06/10 00:33:01 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 366.0 MiB)
22/06/10 00:33:01 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50635 (size: 5.3 KiB, free: 366.3 MiB)
22/06/10 00:33:01 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1200
22/06/10 00:33:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 00:33:01 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
22/06/10 00:33:01 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7781 bytes)
22/06/10 00:33:01 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
22/06/10 00:33:01 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/10 00:33:01 INFO CodeGenerator: Code generated in 15.2442 ms
22/06/10 00:33:01 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1673 bytes result sent to driver
22/06/10 00:33:01 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 453 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/10 00:33:01 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
22/06/10 00:33:01 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 0.662 s
22/06/10 00:33:01 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
22/06/10 00:33:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
22/06/10 00:33:01 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 0.927161 s
22/06/10 00:33:01 INFO CodeGenerator: Code generated in 14.0481 ms
22/06/10 00:33:02 INFO FileSourceStrategy: Pruning directories with:
22/06/10 00:33:02 INFO FileSourceStrategy: Pushed Filters:
22/06/10 00:33:02 INFO FileSourceStrategy: Post-Scan Filters:
22/06/10 00:33:02 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
22/06/10 00:33:02 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 285.2 KiB, free 365.7 MiB)
22/06/10 00:33:02 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.7 MiB)
22/06/10 00:33:02 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50635 (size: 23.9 KiB, free: 366.2 MiB)
22/06/10 00:33:02 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
22/06/10 00:33:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 00:33:02 INFO FileSourceStrategy: Pruning directories with:
22/06/10 00:33:02 INFO FileSourceStrategy: Pushed Filters:
22/06/10 00:33:02 INFO FileSourceStrategy: Post-Scan Filters:
22/06/10 00:33:02 INFO FileSourceStrategy: Output Data Schema: struct<Invoice ID: string, Branch: string, City: string, Customer type: string, Gender: string ... 15 more fields>
22/06/10 00:33:02 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 285.2 KiB, free 365.4 MiB)
22/06/10 00:33:02 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.4 MiB)
22/06/10 00:33:02 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50635 (size: 23.9 KiB, free: 366.2 MiB)
22/06/10 00:33:02 INFO SparkContext: Created broadcast 3 from showString at NativeMethodAccessorImpl.java:0
22/06/10 00:33:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 00:33:02 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
22/06/10 00:33:02 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/10 00:33:02 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)
22/06/10 00:33:02 INFO DAGScheduler: Parents of final stage: List()
22/06/10 00:33:02 INFO DAGScheduler: Missing parents: List()
22/06/10 00:33:02 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[15] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 00:33:02 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.6 KiB, free 365.4 MiB)
22/06/10 00:33:02 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 365.3 MiB)
22/06/10 00:33:02 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50635 (size: 10.7 KiB, free: 366.2 MiB)
22/06/10 00:33:02 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1200
22/06/10 00:33:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[15] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 00:33:02 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
22/06/10 00:33:02 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7781 bytes)
22/06/10 00:33:02 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
22/06/10 00:33:02 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/10 00:33:02 INFO CodeGenerator: Code generated in 31.3716 ms
22/06/10 00:33:03 INFO MemoryStore: Block rdd_12_0 stored as values in memory (estimated size 58.7 KiB, free 365.3 MiB)
22/06/10 00:33:03 INFO BlockManagerInfo: Added rdd_12_0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50635 (size: 58.7 KiB, free: 366.2 MiB)
22/06/10 00:33:03 INFO CodeGenerator: Code generated in 7.9357 ms
22/06/10 00:33:03 INFO CodeGenerator: Code generated in 44.3729 ms
22/06/10 00:33:03 INFO Executor: 1 block locks were not released by TID = 1:
[rdd_12_0]
22/06/10 00:33:03 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1619 bytes result sent to driver
22/06/10 00:33:03 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 492 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/10 00:33:03 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.551 s
22/06/10 00:33:03 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
22/06/10 00:33:03 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
22/06/10 00:33:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
22/06/10 00:33:03 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.571717 s
22/06/10 00:33:03 INFO CodeGenerator: Code generated in 13.2038 ms
+---------+
|     Date|
+---------+
| 1/5/2019|
| 3/8/2019|
| 3/3/2019|
|1/27/2019|
| 2/8/2019|
|3/25/2019|
|2/25/2019|
|2/24/2019|
|1/10/2019|
|2/20/2019|
+---------+

22/06/10 00:33:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/06/10 00:33:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
22/06/10 00:33:03 INFO CodeGenerator: Code generated in 13.5673 ms
22/06/10 00:33:03 INFO CodeGenerator: Code generated in 9.7871 ms
22/06/10 00:33:03 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
22/06/10 00:33:03 INFO DAGScheduler: Registering RDD 19 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0
22/06/10 00:33:03 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/10 00:33:03 INFO DAGScheduler: Final stage: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0)
22/06/10 00:33:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
22/06/10 00:33:03 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
22/06/10 00:33:03 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 00:33:03 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 29.7 KiB, free 365.3 MiB)
22/06/10 00:33:03 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 12.9 KiB, free 365.2 MiB)
22/06/10 00:33:03 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50635 (size: 12.9 KiB, free: 366.1 MiB)
22/06/10 00:33:03 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1200
22/06/10 00:33:03 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 00:33:03 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
22/06/10 00:33:03 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7770 bytes)
22/06/10 00:33:03 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
22/06/10 00:33:03 INFO BlockManager: Found block rdd_12_0 locally
22/06/10 00:33:03 INFO Executor: 1 block locks were not released by TID = 2:
[rdd_12_0]
22/06/10 00:33:03 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1990 bytes result sent to driver
22/06/10 00:33:03 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 80 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/10 00:33:03 INFO DAGScheduler: ShuffleMapStage 2 (csv at NativeMethodAccessorImpl.java:0) finished in 0.122 s
22/06/10 00:33:03 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
22/06/10 00:33:03 INFO DAGScheduler: looking for newly runnable stages
22/06/10 00:33:03 INFO DAGScheduler: running: Set()
22/06/10 00:33:03 INFO DAGScheduler: waiting: Set(ResultStage 3)
22/06/10 00:33:03 INFO DAGScheduler: failed: Set()
22/06/10 00:33:03 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[21] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 00:33:03 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 151.2 KiB, free 365.1 MiB)
22/06/10 00:33:03 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 54.7 KiB, free 365.0 MiB)
22/06/10 00:33:03 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:50635 (size: 54.7 KiB, free: 366.1 MiB)
22/06/10 00:33:03 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1200
22/06/10 00:33:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[21] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 00:33:03 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
22/06/10 00:33:03 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
22/06/10 00:33:03 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
22/06/10 00:33:03 INFO ShuffleBlockFetcherIterator: Getting 1 (156.0 B) non-empty blocks including 1 (156.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
22/06/10 00:33:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
22/06/10 00:33:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/06/10 00:33:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
22/06/10 00:33:03 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3)
java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610003303_0003_m_000000_3\part-00000-f885d612-2018-4e78-adb0-7aaffe632f4c-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
22/06/10 00:33:03 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610003303_0003_m_000000_3\part-00000-f885d612-2018-4e78-adb0-7aaffe632f4c-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

22/06/10 00:33:03 ERROR TaskSetManager: Task 0 in stage 3.0 failed 1 times; aborting job
22/06/10 00:33:03 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
22/06/10 00:33:03 INFO TaskSchedulerImpl: Cancelling stage 3
22/06/10 00:33:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage cancelled
22/06/10 00:33:03 INFO DAGScheduler: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0) failed in 0.240 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610003303_0003_m_000000_3\part-00000-f885d612-2018-4e78-adb0-7aaffe632f4c-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
22/06/10 00:33:03 INFO DAGScheduler: Job 2 failed: csv at NativeMethodAccessorImpl.java:0, took 0.406539 s
22/06/10 00:33:03 ERROR FileFormatWriter: Aborting job fe20d21c-311b-4715-9e33-2e507eac9ad0.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610003303_0003_m_000000_3\part-00000-f885d612-2018-4e78-adb0-7aaffe632f4c-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)
        at scala.Option.foreach(Option.scala:407)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)
        at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)
        at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)
        at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
        at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)
        at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610003303_0003_m_000000_3\part-00000-f885d612-2018-4e78-adb0-7aaffe632f4c-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        ... 1 more
Traceback (most recent call last):
  File "C:/Users/Malcolm Lewis/Desktop/Pipeline Pet Project/Spark Pipeline.py", line 36, in <module>
    output.write.csv("filtered.csv")
  File "C:\Python37\lib\site-packages\pyspark\python\lib\pyspark.zip\pyspark\sql\readwriter.py", line 1027, in csv
  File "C:\Python37\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1305, in __call__
  File "C:\Python37\lib\site-packages\pyspark\python\lib\pyspark.zip\pyspark\sql\utils.py", line 131, in deco
  File "C:\Python37\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o31.csv.
: org.apache.spark.SparkException: Job aborted.
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)
        at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)
        at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)
        at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
        at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)
        at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610003303_0003_m_000000_3\part-00000-f885d612-2018-4e78-adb0-7aaffe632f4c-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)
        at scala.Option.foreach(Option.scala:407)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)
        ... 33 more
Caused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610003303_0003_m_000000_3\part-00000-f885d612-2018-4e78-adb0-7aaffe632f4c-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        ... 1 more

22/06/10 00:33:03 INFO SparkContext: Invoking stop() from shutdown hook
22/06/10 00:33:03 INFO SparkUI: Stopped Spark web UI at http://MalcolmSurfacePro4.myfiosgateway.com:4040
22/06/10 00:33:03 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
22/06/10 00:33:04 INFO MemoryStore: MemoryStore cleared
22/06/10 00:33:04 INFO BlockManager: BlockManager stopped
22/06/10 00:33:04 INFO BlockManagerMaster: BlockManagerMaster stopped
22/06/10 00:33:04 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
22/06/10 00:33:04 INFO SparkContext: Successfully stopped SparkContext
22/06/10 00:33:04 INFO ShutdownHookManager: Shutdown hook called
22/06/10 00:33:04 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-9382bd7d-853b-4ec2-a060-0dbc0a69cf85
22/06/10 00:33:04 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-5e7bbe5b-364f-49bb-af27-bfe02cae2f71\pyspark-6bc5f760-3126-43dc-a616-9d49cbb1c2a0
22/06/10 00:33:04 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-5e7bbe5b-364f-49bb-af27-bfe02cae2f71

C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project>spark-submit "Spark Pipeline.py"
22/06/10 01:22:35 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2412)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2412)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
22/06/10 01:22:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
22/06/10 01:22:36 INFO SparkContext: Running Spark version 3.0.0
22/06/10 01:22:36 INFO ResourceUtils: ==============================================================
22/06/10 01:22:36 INFO ResourceUtils: Resources for spark.driver:

22/06/10 01:22:36 INFO ResourceUtils: ==============================================================
22/06/10 01:22:36 INFO SparkContext: Submitted application: Spark%20Pipeline.py
22/06/10 01:22:37 INFO SecurityManager: Changing view acls to: Malcolm Lewis
22/06/10 01:22:37 INFO SecurityManager: Changing modify acls to: Malcolm Lewis
22/06/10 01:22:37 INFO SecurityManager: Changing view acls groups to:
22/06/10 01:22:37 INFO SecurityManager: Changing modify acls groups to:
22/06/10 01:22:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Malcolm Lewis); groups with view permissions: Set(); users  with modify permissions: Set(Malcolm Lewis); groups with modify permissions: Set()
22/06/10 01:22:38 INFO Utils: Successfully started service 'sparkDriver' on port 51110.
22/06/10 01:22:38 INFO SparkEnv: Registering MapOutputTracker
22/06/10 01:22:38 INFO SparkEnv: Registering BlockManagerMaster
22/06/10 01:22:38 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
22/06/10 01:22:38 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
22/06/10 01:22:38 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
22/06/10 01:22:38 INFO DiskBlockManager: Created local directory at C:\Users\Malcolm Lewis\AppData\Local\Temp\blockmgr-e94438fe-1fa4-4b44-914a-e851b2d38dbb
22/06/10 01:22:38 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
22/06/10 01:22:38 INFO SparkEnv: Registering OutputCommitCoordinator
22/06/10 01:22:39 INFO Utils: Successfully started service 'SparkUI' on port 4040.
22/06/10 01:22:39 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://MalcolmSurfacePro4.myfiosgateway.com:4040
22/06/10 01:22:39 INFO Executor: Starting executor ID driver on host MalcolmSurfacePro4.myfiosgateway.com
22/06/10 01:22:39 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51133.
22/06/10 01:22:39 INFO NettyBlockTransferService: Server created on MalcolmSurfacePro4.myfiosgateway.com:51133
22/06/10 01:22:39 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
22/06/10 01:22:39 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 51133, None)
22/06/10 01:22:39 INFO BlockManagerMasterEndpoint: Registering block manager MalcolmSurfacePro4.myfiosgateway.com:51133 with 366.3 MiB RAM, BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 51133, None)
22/06/10 01:22:39 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 51133, None)
22/06/10 01:22:39 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 51133, None)
22/06/10 01:22:40 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/C:/Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/spark-warehouse').
22/06/10 01:22:40 INFO SharedState: Warehouse path is 'file:/C:/Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/spark-warehouse'.
22/06/10 01:22:41 INFO InMemoryFileIndex: It took 47 ms to list leaf files for 1 paths.
22/06/10 01:22:41 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
22/06/10 01:22:44 INFO FileSourceStrategy: Pruning directories with:
22/06/10 01:22:44 INFO FileSourceStrategy: Pushed Filters:
22/06/10 01:22:44 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
22/06/10 01:22:44 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
22/06/10 01:22:45 INFO CodeGenerator: Code generated in 314.2976 ms
22/06/10 01:22:45 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 285.2 KiB, free 366.0 MiB)
22/06/10 01:22:45 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 366.0 MiB)
22/06/10 01:22:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:51133 (size: 23.9 KiB, free: 366.3 MiB)
22/06/10 01:22:45 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
22/06/10 01:22:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 01:22:46 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
22/06/10 01:22:46 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/10 01:22:46 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
22/06/10 01:22:46 INFO DAGScheduler: Parents of final stage: List()
22/06/10 01:22:46 INFO DAGScheduler: Missing parents: List()
22/06/10 01:22:46 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 01:22:46 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.7 KiB, free 366.0 MiB)
22/06/10 01:22:46 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 366.0 MiB)
22/06/10 01:22:46 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:51133 (size: 5.3 KiB, free: 366.3 MiB)
22/06/10 01:22:46 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1200
22/06/10 01:22:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 01:22:46 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
22/06/10 01:22:46 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7781 bytes)
22/06/10 01:22:46 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
22/06/10 01:22:46 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/10 01:22:46 INFO CodeGenerator: Code generated in 13.8239 ms
22/06/10 01:22:46 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1716 bytes result sent to driver
22/06/10 01:22:46 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 450 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/10 01:22:46 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
22/06/10 01:22:46 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 0.593 s
22/06/10 01:22:46 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
22/06/10 01:22:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
22/06/10 01:22:46 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 0.759162 s
22/06/10 01:22:46 INFO CodeGenerator: Code generated in 14.3 ms
22/06/10 01:22:46 INFO FileSourceStrategy: Pruning directories with:
22/06/10 01:22:46 INFO FileSourceStrategy: Pushed Filters:
22/06/10 01:22:46 INFO FileSourceStrategy: Post-Scan Filters:
22/06/10 01:22:46 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
22/06/10 01:22:46 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 285.2 KiB, free 365.7 MiB)
22/06/10 01:22:46 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.7 MiB)
22/06/10 01:22:46 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:51133 (size: 23.9 KiB, free: 366.2 MiB)
22/06/10 01:22:46 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
22/06/10 01:22:46 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 01:22:47 INFO FileSourceStrategy: Pruning directories with:
22/06/10 01:22:47 INFO FileSourceStrategy: Pushed Filters:
22/06/10 01:22:47 INFO FileSourceStrategy: Post-Scan Filters:
22/06/10 01:22:47 INFO FileSourceStrategy: Output Data Schema: struct<Date: string>
22/06/10 01:22:47 INFO CodeGenerator: Code generated in 11.0361 ms
22/06/10 01:22:47 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 285.2 KiB, free 365.4 MiB)
22/06/10 01:22:47 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.4 MiB)
22/06/10 01:22:47 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:51133 (size: 23.9 KiB, free: 366.2 MiB)
22/06/10 01:22:47 INFO SparkContext: Created broadcast 3 from showString at NativeMethodAccessorImpl.java:0
22/06/10 01:22:47 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 01:22:47 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
22/06/10 01:22:47 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/10 01:22:47 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)
22/06/10 01:22:47 INFO DAGScheduler: Parents of final stage: List()
22/06/10 01:22:47 INFO DAGScheduler: Missing parents: List()
22/06/10 01:22:47 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 01:22:47 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 12.1 KiB, free 365.4 MiB)
22/06/10 01:22:47 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 365.4 MiB)
22/06/10 01:22:47 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:51133 (size: 6.2 KiB, free: 366.2 MiB)
22/06/10 01:22:47 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1200
22/06/10 01:22:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 01:22:47 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
22/06/10 01:22:47 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7781 bytes)
22/06/10 01:22:47 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
22/06/10 01:22:47 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/10 01:22:47 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1543 bytes result sent to driver
22/06/10 01:22:47 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 53 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/10 01:22:47 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.089 s
22/06/10 01:22:47 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
22/06/10 01:22:47 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
22/06/10 01:22:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
22/06/10 01:22:47 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.102635 s
22/06/10 01:22:47 INFO CodeGenerator: Code generated in 15.6514 ms
+---------+
|     Date|
+---------+
| 1/5/2019|
| 3/8/2019|
| 3/3/2019|
|1/27/2019|
| 2/8/2019|
|3/25/2019|
|2/25/2019|
|2/24/2019|
|1/10/2019|
|2/20/2019|
+---------+

22/06/10 01:22:47 INFO FileSourceStrategy: Pruning directories with:
22/06/10 01:22:47 INFO FileSourceStrategy: Pushed Filters:
22/06/10 01:22:47 INFO FileSourceStrategy: Post-Scan Filters:
22/06/10 01:22:47 INFO FileSourceStrategy: Output Data Schema: struct<Date: string>
22/06/10 01:22:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/06/10 01:22:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
22/06/10 01:22:47 INFO CodeGenerator: Code generated in 11.969 ms
22/06/10 01:22:47 INFO CodeGenerator: Code generated in 12.862 ms
22/06/10 01:22:47 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 285.2 KiB, free 365.1 MiB)
22/06/10 01:22:47 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.1 MiB)
22/06/10 01:22:47 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:51133 (size: 23.9 KiB, free: 366.2 MiB)
22/06/10 01:22:47 INFO SparkContext: Created broadcast 5 from csv at NativeMethodAccessorImpl.java:0
22/06/10 01:22:47 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 01:22:47 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
22/06/10 01:22:47 INFO DAGScheduler: Registering RDD 17 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0
22/06/10 01:22:47 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/10 01:22:47 INFO DAGScheduler: Final stage: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0)
22/06/10 01:22:47 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
22/06/10 01:22:47 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
22/06/10 01:22:47 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[17] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 01:22:47 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 14.8 KiB, free 365.0 MiB)
22/06/10 01:22:47 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 365.0 MiB)
22/06/10 01:22:47 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:51133 (size: 7.5 KiB, free: 366.2 MiB)
22/06/10 01:22:47 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1200
22/06/10 01:22:47 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[17] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 01:22:47 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
22/06/10 01:22:47 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7770 bytes)
22/06/10 01:22:47 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
22/06/10 01:22:47 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/10 01:22:47 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1799 bytes result sent to driver
22/06/10 01:22:47 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 92 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/10 01:22:47 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
22/06/10 01:22:47 INFO DAGScheduler: ShuffleMapStage 2 (csv at NativeMethodAccessorImpl.java:0) finished in 0.124 s
22/06/10 01:22:47 INFO DAGScheduler: looking for newly runnable stages
22/06/10 01:22:47 INFO DAGScheduler: running: Set()
22/06/10 01:22:47 INFO DAGScheduler: waiting: Set(ResultStage 3)
22/06/10 01:22:47 INFO DAGScheduler: failed: Set()
22/06/10 01:22:47 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 01:22:48 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 151.2 KiB, free 364.9 MiB)
22/06/10 01:22:48 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 364.8 MiB)
22/06/10 01:22:48 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:51133 (size: 54.8 KiB, free: 366.1 MiB)
22/06/10 01:22:48 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1200
22/06/10 01:22:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 01:22:48 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
22/06/10 01:22:48 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
22/06/10 01:22:48 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
22/06/10 01:22:48 INFO ShuffleBlockFetcherIterator: Getting 1 (156.0 B) non-empty blocks including 1 (156.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
22/06/10 01:22:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
22/06/10 01:22:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/06/10 01:22:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
22/06/10 01:22:48 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3)
java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610012247_0003_m_000000_3\part-00000-5100a40f-99e5-4122-82ce-e8ee2936b096-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
22/06/10 01:22:48 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610012247_0003_m_000000_3\part-00000-5100a40f-99e5-4122-82ce-e8ee2936b096-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

22/06/10 01:22:48 ERROR TaskSetManager: Task 0 in stage 3.0 failed 1 times; aborting job
22/06/10 01:22:48 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
22/06/10 01:22:48 INFO TaskSchedulerImpl: Cancelling stage 3
22/06/10 01:22:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage cancelled
22/06/10 01:22:48 INFO DAGScheduler: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0) failed in 0.254 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610012247_0003_m_000000_3\part-00000-5100a40f-99e5-4122-82ce-e8ee2936b096-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
22/06/10 01:22:48 INFO DAGScheduler: Job 2 failed: csv at NativeMethodAccessorImpl.java:0, took 0.422637 s
22/06/10 01:22:48 ERROR FileFormatWriter: Aborting job 3f8e6ecc-fcfe-4c37-8bb1-488aff64944d.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610012247_0003_m_000000_3\part-00000-5100a40f-99e5-4122-82ce-e8ee2936b096-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)
        at scala.Option.foreach(Option.scala:407)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)
        at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)
        at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)
        at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
        at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)
        at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610012247_0003_m_000000_3\part-00000-5100a40f-99e5-4122-82ce-e8ee2936b096-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        ... 1 more
Traceback (most recent call last):
  File "C:/Users/Malcolm Lewis/Desktop/Pipeline Pet Project/Spark Pipeline.py", line 36, in <module>
    output.write.csv("filtered.csv")
  File "C:\Python37\lib\site-packages\pyspark\python\lib\pyspark.zip\pyspark\sql\readwriter.py", line 1027, in csv
  File "C:\Python37\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1305, in __call__
  File "C:\Python37\lib\site-packages\pyspark\python\lib\pyspark.zip\pyspark\sql\utils.py", line 131, in deco
  File "C:\Python37\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o29.csv.
: org.apache.spark.SparkException: Job aborted.
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)
        at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)
        at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)
        at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
        at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)
        at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610012247_0003_m_000000_3\part-00000-5100a40f-99e5-4122-82ce-e8ee2936b096-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)
        at scala.Option.foreach(Option.scala:407)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)
        ... 33 more
Caused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610012247_0003_m_000000_3\part-00000-5100a40f-99e5-4122-82ce-e8ee2936b096-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        ... 1 more

22/06/10 01:22:48 INFO BlockManagerInfo: Removed broadcast_3_piece0 on MalcolmSurfacePro4.myfiosgateway.com:51133 in memory (size: 23.9 KiB, free: 366.2 MiB)
22/06/10 01:22:48 INFO BlockManagerInfo: Removed broadcast_4_piece0 on MalcolmSurfacePro4.myfiosgateway.com:51133 in memory (size: 6.2 KiB, free: 366.2 MiB)
22/06/10 01:22:48 INFO SparkContext: Invoking stop() from shutdown hook
22/06/10 01:22:48 INFO BlockManagerInfo: Removed broadcast_7_piece0 on MalcolmSurfacePro4.myfiosgateway.com:51133 in memory (size: 54.8 KiB, free: 366.2 MiB)
22/06/10 01:22:48 INFO BlockManagerInfo: Removed broadcast_6_piece0 on MalcolmSurfacePro4.myfiosgateway.com:51133 in memory (size: 7.5 KiB, free: 366.2 MiB)
22/06/10 01:22:48 INFO SparkUI: Stopped Spark web UI at http://MalcolmSurfacePro4.myfiosgateway.com:4040
22/06/10 01:22:48 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
22/06/10 01:22:48 INFO MemoryStore: MemoryStore cleared
22/06/10 01:22:48 INFO BlockManager: BlockManager stopped
22/06/10 01:22:48 INFO BlockManagerMaster: BlockManagerMaster stopped
22/06/10 01:22:48 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
22/06/10 01:22:48 INFO SparkContext: Successfully stopped SparkContext
22/06/10 01:22:48 INFO ShutdownHookManager: Shutdown hook called
22/06/10 01:22:48 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-9654373d-157b-4ee0-a2e3-f4226d9167bc\pyspark-c914dd9d-0079-4f6c-a54f-79a83bfadc24
22/06/10 01:22:48 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-60525ecc-a430-4965-a8c7-da6026dbba83
22/06/10 01:22:48 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-9654373d-157b-4ee0-a2e3-f4226d9167bc

C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project>spark-submit "Spark Pipeline.py"
22/06/10 01:25:22 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2412)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2412)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
22/06/10 01:25:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
22/06/10 01:25:23 INFO SparkContext: Running Spark version 3.0.0
22/06/10 01:25:23 INFO ResourceUtils: ==============================================================
22/06/10 01:25:23 INFO ResourceUtils: Resources for spark.driver:

22/06/10 01:25:23 INFO ResourceUtils: ==============================================================
22/06/10 01:25:23 INFO SparkContext: Submitted application: Spark%20Pipeline.py
22/06/10 01:25:23 INFO SecurityManager: Changing view acls to: Malcolm Lewis
22/06/10 01:25:23 INFO SecurityManager: Changing modify acls to: Malcolm Lewis
22/06/10 01:25:23 INFO SecurityManager: Changing view acls groups to:
22/06/10 01:25:23 INFO SecurityManager: Changing modify acls groups to:
22/06/10 01:25:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Malcolm Lewis); groups with view permissions: Set(); users  with modify permissions: Set(Malcolm Lewis); groups with modify permissions: Set()
22/06/10 01:25:24 INFO Utils: Successfully started service 'sparkDriver' on port 51181.
22/06/10 01:25:24 INFO SparkEnv: Registering MapOutputTracker
22/06/10 01:25:24 INFO SparkEnv: Registering BlockManagerMaster
22/06/10 01:25:24 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
22/06/10 01:25:24 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
22/06/10 01:25:24 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
22/06/10 01:25:24 INFO DiskBlockManager: Created local directory at C:\Users\Malcolm Lewis\AppData\Local\Temp\blockmgr-f455c4f9-05e7-443a-b350-ec378552da55
22/06/10 01:25:24 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
22/06/10 01:25:24 INFO SparkEnv: Registering OutputCommitCoordinator
22/06/10 01:25:25 INFO Utils: Successfully started service 'SparkUI' on port 4040.
22/06/10 01:25:25 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://MalcolmSurfacePro4.myfiosgateway.com:4040
22/06/10 01:25:25 INFO Executor: Starting executor ID driver on host MalcolmSurfacePro4.myfiosgateway.com
22/06/10 01:25:25 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51204.
22/06/10 01:25:25 INFO NettyBlockTransferService: Server created on MalcolmSurfacePro4.myfiosgateway.com:51204
22/06/10 01:25:25 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
22/06/10 01:25:25 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 51204, None)
22/06/10 01:25:25 INFO BlockManagerMasterEndpoint: Registering block manager MalcolmSurfacePro4.myfiosgateway.com:51204 with 366.3 MiB RAM, BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 51204, None)
22/06/10 01:25:25 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 51204, None)
22/06/10 01:25:25 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 51204, None)
22/06/10 01:25:26 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/C:/Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/spark-warehouse').
22/06/10 01:25:26 INFO SharedState: Warehouse path is 'file:/C:/Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/spark-warehouse'.
22/06/10 01:25:26 INFO InMemoryFileIndex: It took 31 ms to list leaf files for 1 paths.
22/06/10 01:25:26 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
22/06/10 01:25:29 INFO FileSourceStrategy: Pruning directories with:
22/06/10 01:25:29 INFO FileSourceStrategy: Pushed Filters:
22/06/10 01:25:29 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
22/06/10 01:25:29 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
22/06/10 01:25:30 INFO CodeGenerator: Code generated in 248.7568 ms
22/06/10 01:25:30 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 285.2 KiB, free 366.0 MiB)
22/06/10 01:25:30 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 366.0 MiB)
22/06/10 01:25:30 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:51204 (size: 23.9 KiB, free: 366.3 MiB)
22/06/10 01:25:30 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
22/06/10 01:25:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 01:25:30 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
22/06/10 01:25:30 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/10 01:25:30 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
22/06/10 01:25:30 INFO DAGScheduler: Parents of final stage: List()
22/06/10 01:25:30 INFO DAGScheduler: Missing parents: List()
22/06/10 01:25:30 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 01:25:30 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.7 KiB, free 366.0 MiB)
22/06/10 01:25:30 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 366.0 MiB)
22/06/10 01:25:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:51204 (size: 5.3 KiB, free: 366.3 MiB)
22/06/10 01:25:30 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1200
22/06/10 01:25:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 01:25:30 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
22/06/10 01:25:30 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7781 bytes)
22/06/10 01:25:30 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
22/06/10 01:25:31 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/10 01:25:31 INFO CodeGenerator: Code generated in 14.1237 ms
22/06/10 01:25:31 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1673 bytes result sent to driver
22/06/10 01:25:31 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 462 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/10 01:25:31 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
22/06/10 01:25:31 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 0.653 s
22/06/10 01:25:31 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
22/06/10 01:25:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
22/06/10 01:25:31 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 0.823633 s
22/06/10 01:25:31 INFO CodeGenerator: Code generated in 16.7203 ms
22/06/10 01:25:31 INFO FileSourceStrategy: Pruning directories with:
22/06/10 01:25:31 INFO FileSourceStrategy: Pushed Filters:
22/06/10 01:25:31 INFO FileSourceStrategy: Post-Scan Filters:
22/06/10 01:25:31 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
22/06/10 01:25:31 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 285.2 KiB, free 365.7 MiB)
22/06/10 01:25:31 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.7 MiB)
22/06/10 01:25:31 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:51204 (size: 23.9 KiB, free: 366.2 MiB)
22/06/10 01:25:31 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
22/06/10 01:25:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 01:25:31 INFO FileSourceStrategy: Pruning directories with:
22/06/10 01:25:31 INFO FileSourceStrategy: Pushed Filters:
22/06/10 01:25:31 INFO FileSourceStrategy: Post-Scan Filters:
22/06/10 01:25:31 INFO FileSourceStrategy: Output Data Schema: struct<Date: string>
22/06/10 01:25:31 INFO CodeGenerator: Code generated in 9.3511 ms
22/06/10 01:25:31 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 285.2 KiB, free 365.4 MiB)
22/06/10 01:25:31 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.4 MiB)
22/06/10 01:25:31 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:51204 (size: 23.9 KiB, free: 366.2 MiB)
22/06/10 01:25:31 INFO SparkContext: Created broadcast 3 from showString at NativeMethodAccessorImpl.java:0
22/06/10 01:25:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 01:25:31 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
22/06/10 01:25:31 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/10 01:25:31 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)
22/06/10 01:25:31 INFO DAGScheduler: Parents of final stage: List()
22/06/10 01:25:31 INFO DAGScheduler: Missing parents: List()
22/06/10 01:25:31 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 01:25:31 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 12.1 KiB, free 365.4 MiB)
22/06/10 01:25:31 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 365.4 MiB)
22/06/10 01:25:31 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:51204 (size: 6.2 KiB, free: 366.2 MiB)
22/06/10 01:25:31 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1200
22/06/10 01:25:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 01:25:31 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
22/06/10 01:25:31 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7781 bytes)
22/06/10 01:25:31 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
22/06/10 01:25:31 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/10 01:25:32 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1500 bytes result sent to driver
22/06/10 01:25:32 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 55 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/10 01:25:32 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.095 s
22/06/10 01:25:32 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
22/06/10 01:25:32 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
22/06/10 01:25:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
22/06/10 01:25:32 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.109769 s
22/06/10 01:25:32 INFO CodeGenerator: Code generated in 14.7549 ms
+---------+
|     Date|
+---------+
| 1/5/2019|
| 3/8/2019|
| 3/3/2019|
|1/27/2019|
| 2/8/2019|
|3/25/2019|
|2/25/2019|
|2/24/2019|
|1/10/2019|
|2/20/2019|
+---------+

22/06/10 01:25:32 INFO FileSourceStrategy: Pruning directories with:
22/06/10 01:25:32 INFO FileSourceStrategy: Pushed Filters:
22/06/10 01:25:32 INFO FileSourceStrategy: Post-Scan Filters:
22/06/10 01:25:32 INFO FileSourceStrategy: Output Data Schema: struct<Date: string>
22/06/10 01:25:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/06/10 01:25:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
22/06/10 01:25:32 INFO CodeGenerator: Code generated in 12.2746 ms
22/06/10 01:25:32 INFO CodeGenerator: Code generated in 10.4944 ms
22/06/10 01:25:32 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 285.2 KiB, free 365.1 MiB)
22/06/10 01:25:32 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.1 MiB)
22/06/10 01:25:32 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:51204 (size: 23.9 KiB, free: 366.2 MiB)
22/06/10 01:25:32 INFO SparkContext: Created broadcast 5 from csv at NativeMethodAccessorImpl.java:0
22/06/10 01:25:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 01:25:32 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
22/06/10 01:25:32 INFO DAGScheduler: Registering RDD 17 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0
22/06/10 01:25:32 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/10 01:25:32 INFO DAGScheduler: Final stage: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0)
22/06/10 01:25:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
22/06/10 01:25:32 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
22/06/10 01:25:32 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[17] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 01:25:32 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 14.8 KiB, free 365.0 MiB)
22/06/10 01:25:32 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 365.0 MiB)
22/06/10 01:25:32 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:51204 (size: 7.5 KiB, free: 366.2 MiB)
22/06/10 01:25:32 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1200
22/06/10 01:25:32 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[17] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 01:25:32 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
22/06/10 01:25:32 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7770 bytes)
22/06/10 01:25:32 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
22/06/10 01:25:32 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/10 01:25:32 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1842 bytes result sent to driver
22/06/10 01:25:32 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 93 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/10 01:25:32 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
22/06/10 01:25:32 INFO DAGScheduler: ShuffleMapStage 2 (csv at NativeMethodAccessorImpl.java:0) finished in 0.135 s
22/06/10 01:25:32 INFO DAGScheduler: looking for newly runnable stages
22/06/10 01:25:32 INFO DAGScheduler: running: Set()
22/06/10 01:25:32 INFO DAGScheduler: waiting: Set(ResultStage 3)
22/06/10 01:25:32 INFO DAGScheduler: failed: Set()
22/06/10 01:25:32 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 01:25:32 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 151.2 KiB, free 364.9 MiB)
22/06/10 01:25:32 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 364.8 MiB)
22/06/10 01:25:32 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:51204 (size: 54.8 KiB, free: 366.1 MiB)
22/06/10 01:25:32 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1200
22/06/10 01:25:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 01:25:32 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
22/06/10 01:25:32 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
22/06/10 01:25:32 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
22/06/10 01:25:32 INFO ShuffleBlockFetcherIterator: Getting 1 (156.0 B) non-empty blocks including 1 (156.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
22/06/10 01:25:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
22/06/10 01:25:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/06/10 01:25:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
22/06/10 01:25:32 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3)
java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610012532_0003_m_000000_3\part-00000-87ddc348-066b-4fa7-9904-8cd6bbe4fb25-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
22/06/10 01:25:32 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610012532_0003_m_000000_3\part-00000-87ddc348-066b-4fa7-9904-8cd6bbe4fb25-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

22/06/10 01:25:32 ERROR TaskSetManager: Task 0 in stage 3.0 failed 1 times; aborting job
22/06/10 01:25:32 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
22/06/10 01:25:32 INFO TaskSchedulerImpl: Cancelling stage 3
22/06/10 01:25:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage cancelled
22/06/10 01:25:32 INFO DAGScheduler: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0) failed in 0.219 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610012532_0003_m_000000_3\part-00000-87ddc348-066b-4fa7-9904-8cd6bbe4fb25-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
22/06/10 01:25:32 INFO DAGScheduler: Job 2 failed: csv at NativeMethodAccessorImpl.java:0, took 0.414454 s
22/06/10 01:25:32 ERROR FileFormatWriter: Aborting job 59695492-f60b-4e5e-8e51-8c6e4b32f0d7.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610012532_0003_m_000000_3\part-00000-87ddc348-066b-4fa7-9904-8cd6bbe4fb25-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)
        at scala.Option.foreach(Option.scala:407)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)
        at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)
        at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)
        at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
        at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)
        at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610012532_0003_m_000000_3\part-00000-87ddc348-066b-4fa7-9904-8cd6bbe4fb25-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        ... 1 more
Traceback (most recent call last):
  File "C:/Users/Malcolm Lewis/Desktop/Pipeline Pet Project/Spark Pipeline.py", line 36, in <module>
    output.write.csv("filtered.csv")
  File "C:\Python37\lib\site-packages\pyspark\python\lib\pyspark.zip\pyspark\sql\readwriter.py", line 1027, in csv
  File "C:\Python37\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1305, in __call__
  File "C:\Python37\lib\site-packages\pyspark\python\lib\pyspark.zip\pyspark\sql\utils.py", line 131, in deco
  File "C:\Python37\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o29.csv.
: org.apache.spark.SparkException: Job aborted.
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)
        at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)
        at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)
        at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
        at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)
        at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610012532_0003_m_000000_3\part-00000-87ddc348-066b-4fa7-9904-8cd6bbe4fb25-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)
        at scala.Option.foreach(Option.scala:407)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)
        ... 33 more
Caused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610012532_0003_m_000000_3\part-00000-87ddc348-066b-4fa7-9904-8cd6bbe4fb25-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        ... 1 more

22/06/10 01:25:32 INFO SparkContext: Invoking stop() from shutdown hook
22/06/10 01:25:32 INFO SparkUI: Stopped Spark web UI at http://MalcolmSurfacePro4.myfiosgateway.com:4040
22/06/10 01:25:32 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
22/06/10 01:25:32 INFO MemoryStore: MemoryStore cleared
22/06/10 01:25:32 INFO BlockManager: BlockManager stopped
22/06/10 01:25:32 INFO BlockManagerMaster: BlockManagerMaster stopped
22/06/10 01:25:32 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
22/06/10 01:25:32 INFO SparkContext: Successfully stopped SparkContext
22/06/10 01:25:32 INFO ShutdownHookManager: Shutdown hook called
22/06/10 01:25:32 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-cf9e24cb-858e-4358-8893-78e7b4ed04ee
22/06/10 01:25:32 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-4e312a03-571e-4945-9986-e693a43c15be
22/06/10 01:25:32 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-4e312a03-571e-4945-9986-e693a43c15be\pyspark-bf8d4a97-b351-414a-82a3-617312d2ab51

C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project>spark-submit "Spark Pipeline.py"
22/06/10 01:34:01 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2412)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2412)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
22/06/10 01:34:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
22/06/10 01:34:02 INFO SparkContext: Running Spark version 3.0.0
22/06/10 01:34:02 INFO ResourceUtils: ==============================================================
22/06/10 01:34:02 INFO ResourceUtils: Resources for spark.driver:

22/06/10 01:34:02 INFO ResourceUtils: ==============================================================
22/06/10 01:34:02 INFO SparkContext: Submitted application: Spark%20Pipeline.py
22/06/10 01:34:03 INFO SecurityManager: Changing view acls to: Malcolm Lewis
22/06/10 01:34:03 INFO SecurityManager: Changing modify acls to: Malcolm Lewis
22/06/10 01:34:03 INFO SecurityManager: Changing view acls groups to:
22/06/10 01:34:03 INFO SecurityManager: Changing modify acls groups to:
22/06/10 01:34:03 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Malcolm Lewis); groups with view permissions: Set(); users  with modify permissions: Set(Malcolm Lewis); groups with modify permissions: Set()
22/06/10 01:34:04 INFO Utils: Successfully started service 'sparkDriver' on port 51446.
22/06/10 01:34:04 INFO SparkEnv: Registering MapOutputTracker
22/06/10 01:34:04 INFO SparkEnv: Registering BlockManagerMaster
22/06/10 01:34:04 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
22/06/10 01:34:04 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
22/06/10 01:34:04 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
22/06/10 01:34:04 INFO DiskBlockManager: Created local directory at C:\Users\Malcolm Lewis\AppData\Local\Temp\blockmgr-68da8698-efb2-4d71-b061-769afd016a4a
22/06/10 01:34:04 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
22/06/10 01:34:04 INFO SparkEnv: Registering OutputCommitCoordinator
22/06/10 01:34:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
22/06/10 01:34:04 INFO Utils: Successfully started service 'SparkUI' on port 4041.
22/06/10 01:34:04 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://MalcolmSurfacePro4.myfiosgateway.com:4041
22/06/10 01:34:05 INFO Executor: Starting executor ID driver on host MalcolmSurfacePro4.myfiosgateway.com
22/06/10 01:34:05 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51469.
22/06/10 01:34:05 INFO NettyBlockTransferService: Server created on MalcolmSurfacePro4.myfiosgateway.com:51469
22/06/10 01:34:05 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
22/06/10 01:34:05 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 51469, None)
22/06/10 01:34:05 INFO BlockManagerMasterEndpoint: Registering block manager MalcolmSurfacePro4.myfiosgateway.com:51469 with 366.3 MiB RAM, BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 51469, None)
22/06/10 01:34:05 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 51469, None)
22/06/10 01:34:05 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 51469, None)
22/06/10 01:34:05 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/C:/Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/spark-warehouse').
22/06/10 01:34:05 INFO SharedState: Warehouse path is 'file:/C:/Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/spark-warehouse'.
22/06/10 01:34:06 INFO InMemoryFileIndex: It took 35 ms to list leaf files for 1 paths.
22/06/10 01:34:06 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
22/06/10 01:34:09 INFO FileSourceStrategy: Pruning directories with:
22/06/10 01:34:09 INFO FileSourceStrategy: Pushed Filters:
22/06/10 01:34:09 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
22/06/10 01:34:09 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
22/06/10 01:34:09 INFO CodeGenerator: Code generated in 232.5362 ms
22/06/10 01:34:09 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 285.2 KiB, free 366.0 MiB)
22/06/10 01:34:10 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 366.0 MiB)
22/06/10 01:34:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:51469 (size: 23.9 KiB, free: 366.3 MiB)
22/06/10 01:34:10 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
22/06/10 01:34:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 01:34:10 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
22/06/10 01:34:10 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/10 01:34:10 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
22/06/10 01:34:10 INFO DAGScheduler: Parents of final stage: List()
22/06/10 01:34:10 INFO DAGScheduler: Missing parents: List()
22/06/10 01:34:10 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 01:34:10 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.7 KiB, free 366.0 MiB)
22/06/10 01:34:10 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 366.0 MiB)
22/06/10 01:34:10 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:51469 (size: 5.3 KiB, free: 366.3 MiB)
22/06/10 01:34:10 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1200
22/06/10 01:34:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 01:34:10 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
22/06/10 01:34:10 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7781 bytes)
22/06/10 01:34:10 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
22/06/10 01:34:10 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/10 01:34:10 INFO CodeGenerator: Code generated in 14.1602 ms
22/06/10 01:34:10 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1673 bytes result sent to driver
22/06/10 01:34:10 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 434 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/10 01:34:10 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
22/06/10 01:34:11 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 0.610 s
22/06/10 01:34:11 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
22/06/10 01:34:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
22/06/10 01:34:11 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 0.847846 s
22/06/10 01:34:11 INFO CodeGenerator: Code generated in 13.9127 ms
22/06/10 01:34:11 INFO FileSourceStrategy: Pruning directories with:
22/06/10 01:34:11 INFO FileSourceStrategy: Pushed Filters:
22/06/10 01:34:11 INFO FileSourceStrategy: Post-Scan Filters:
22/06/10 01:34:11 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
22/06/10 01:34:11 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 285.2 KiB, free 365.7 MiB)
22/06/10 01:34:11 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.7 MiB)
22/06/10 01:34:11 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:51469 (size: 23.9 KiB, free: 366.2 MiB)
22/06/10 01:34:11 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
22/06/10 01:34:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 01:34:11 INFO FileSourceStrategy: Pruning directories with:
22/06/10 01:34:11 INFO FileSourceStrategy: Pushed Filters:
22/06/10 01:34:11 INFO FileSourceStrategy: Post-Scan Filters:
22/06/10 01:34:11 INFO FileSourceStrategy: Output Data Schema: struct<Date: string>
22/06/10 01:34:11 INFO CodeGenerator: Code generated in 11.4321 ms
22/06/10 01:34:11 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 285.2 KiB, free 365.4 MiB)
22/06/10 01:34:11 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.4 MiB)
22/06/10 01:34:11 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:51469 (size: 23.9 KiB, free: 366.2 MiB)
22/06/10 01:34:11 INFO SparkContext: Created broadcast 3 from showString at NativeMethodAccessorImpl.java:0
22/06/10 01:34:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 01:34:11 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
22/06/10 01:34:11 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/10 01:34:11 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)
22/06/10 01:34:11 INFO DAGScheduler: Parents of final stage: List()
22/06/10 01:34:11 INFO DAGScheduler: Missing parents: List()
22/06/10 01:34:11 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 01:34:11 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 12.1 KiB, free 365.4 MiB)
22/06/10 01:34:11 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 365.4 MiB)
22/06/10 01:34:11 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:51469 (size: 6.2 KiB, free: 366.2 MiB)
22/06/10 01:34:11 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1200
22/06/10 01:34:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 01:34:11 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
22/06/10 01:34:11 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7781 bytes)
22/06/10 01:34:11 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
22/06/10 01:34:11 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/10 01:34:11 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1543 bytes result sent to driver
22/06/10 01:34:11 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 53 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/10 01:34:11 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
22/06/10 01:34:11 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.087 s
22/06/10 01:34:11 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
22/06/10 01:34:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
22/06/10 01:34:11 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.095745 s
22/06/10 01:34:11 INFO CodeGenerator: Code generated in 11.9944 ms
+---------+
|     Date|
+---------+
| 1/5/2019|
| 3/8/2019|
| 3/3/2019|
|1/27/2019|
| 2/8/2019|
|3/25/2019|
|2/25/2019|
|2/24/2019|
|1/10/2019|
|2/20/2019|
+---------+

22/06/10 01:34:11 INFO FileSourceStrategy: Pruning directories with:
22/06/10 01:34:11 INFO FileSourceStrategy: Pushed Filters:
22/06/10 01:34:11 INFO FileSourceStrategy: Post-Scan Filters:
22/06/10 01:34:11 INFO FileSourceStrategy: Output Data Schema: struct<Date: string>
22/06/10 01:34:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/06/10 01:34:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
22/06/10 01:34:12 INFO CodeGenerator: Code generated in 11.3147 ms
22/06/10 01:34:12 INFO CodeGenerator: Code generated in 12.1279 ms
22/06/10 01:34:12 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 285.2 KiB, free 365.1 MiB)
22/06/10 01:34:12 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.1 MiB)
22/06/10 01:34:12 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:51469 (size: 23.9 KiB, free: 366.2 MiB)
22/06/10 01:34:12 INFO SparkContext: Created broadcast 5 from csv at NativeMethodAccessorImpl.java:0
22/06/10 01:34:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 01:34:12 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
22/06/10 01:34:12 INFO DAGScheduler: Registering RDD 17 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0
22/06/10 01:34:12 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/10 01:34:12 INFO DAGScheduler: Final stage: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0)
22/06/10 01:34:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
22/06/10 01:34:12 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
22/06/10 01:34:12 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[17] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 01:34:12 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 14.8 KiB, free 365.0 MiB)
22/06/10 01:34:12 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 365.0 MiB)
22/06/10 01:34:12 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:51469 (size: 7.5 KiB, free: 366.2 MiB)
22/06/10 01:34:12 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1200
22/06/10 01:34:12 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[17] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 01:34:12 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
22/06/10 01:34:12 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7770 bytes)
22/06/10 01:34:12 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
22/06/10 01:34:12 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/10 01:34:12 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1842 bytes result sent to driver
22/06/10 01:34:12 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 92 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/10 01:34:12 INFO DAGScheduler: ShuffleMapStage 2 (csv at NativeMethodAccessorImpl.java:0) finished in 0.138 s
22/06/10 01:34:12 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
22/06/10 01:34:12 INFO DAGScheduler: looking for newly runnable stages
22/06/10 01:34:12 INFO DAGScheduler: running: Set()
22/06/10 01:34:12 INFO DAGScheduler: waiting: Set(ResultStage 3)
22/06/10 01:34:12 INFO DAGScheduler: failed: Set()
22/06/10 01:34:12 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 01:34:12 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 151.2 KiB, free 364.9 MiB)
22/06/10 01:34:12 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 364.8 MiB)
22/06/10 01:34:12 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:51469 (size: 54.8 KiB, free: 366.1 MiB)
22/06/10 01:34:12 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1200
22/06/10 01:34:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 01:34:12 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
22/06/10 01:34:12 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
22/06/10 01:34:12 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
22/06/10 01:34:12 INFO ShuffleBlockFetcherIterator: Getting 1 (156.0 B) non-empty blocks including 1 (156.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
22/06/10 01:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
22/06/10 01:34:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/06/10 01:34:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
22/06/10 01:34:12 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3)
java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610013412_0003_m_000000_3\part-00000-f3fdb965-8da9-43aa-9db3-33ddc0fccd5c-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
22/06/10 01:34:12 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610013412_0003_m_000000_3\part-00000-f3fdb965-8da9-43aa-9db3-33ddc0fccd5c-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

22/06/10 01:34:12 ERROR TaskSetManager: Task 0 in stage 3.0 failed 1 times; aborting job
22/06/10 01:34:12 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
22/06/10 01:34:12 INFO TaskSchedulerImpl: Cancelling stage 3
22/06/10 01:34:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage cancelled
22/06/10 01:34:12 INFO DAGScheduler: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0) failed in 0.222 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610013412_0003_m_000000_3\part-00000-f3fdb965-8da9-43aa-9db3-33ddc0fccd5c-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
22/06/10 01:34:12 INFO DAGScheduler: Job 2 failed: csv at NativeMethodAccessorImpl.java:0, took 0.401394 s
22/06/10 01:34:12 ERROR FileFormatWriter: Aborting job afeb9e97-4f98-41e0-ad24-3854a61c8b92.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610013412_0003_m_000000_3\part-00000-f3fdb965-8da9-43aa-9db3-33ddc0fccd5c-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)
        at scala.Option.foreach(Option.scala:407)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)
        at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)
        at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)
        at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
        at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)
        at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610013412_0003_m_000000_3\part-00000-f3fdb965-8da9-43aa-9db3-33ddc0fccd5c-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        ... 1 more
Traceback (most recent call last):
  File "C:/Users/Malcolm Lewis/Desktop/Pipeline Pet Project/Spark Pipeline.py", line 36, in <module>
    output.write.csv("filtered.csv")
  File "C:\Python37\lib\site-packages\pyspark\python\lib\pyspark.zip\pyspark\sql\readwriter.py", line 1027, in csv
  File "C:\Python37\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1305, in __call__
  File "C:\Python37\lib\site-packages\pyspark\python\lib\pyspark.zip\pyspark\sql\utils.py", line 131, in deco
  File "C:\Python37\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o29.csv.
: org.apache.spark.SparkException: Job aborted.
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)
        at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)
        at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)
        at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
        at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)
        at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610013412_0003_m_000000_3\part-00000-f3fdb965-8da9-43aa-9db3-33ddc0fccd5c-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)
        at scala.Option.foreach(Option.scala:407)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)
        ... 33 more
Caused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610013412_0003_m_000000_3\part-00000-f3fdb965-8da9-43aa-9db3-33ddc0fccd5c-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        ... 1 more

22/06/10 01:34:12 INFO SparkContext: Invoking stop() from shutdown hook
22/06/10 01:34:12 INFO SparkUI: Stopped Spark web UI at http://MalcolmSurfacePro4.myfiosgateway.com:4041
22/06/10 01:34:12 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
22/06/10 01:34:12 INFO MemoryStore: MemoryStore cleared
22/06/10 01:34:12 INFO BlockManager: BlockManager stopped
22/06/10 01:34:12 INFO BlockManagerMaster: BlockManagerMaster stopped
22/06/10 01:34:12 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
22/06/10 01:34:12 INFO SparkContext: Successfully stopped SparkContext
22/06/10 01:34:12 INFO ShutdownHookManager: Shutdown hook called
22/06/10 01:34:12 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-cd36a9fb-f868-4065-9da7-d413f0197540
22/06/10 01:34:12 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-4670ed04-bd93-4773-ad2f-c4d86188e067\pyspark-16af8dda-399e-4b26-9f81-dc039702edc5
22/06/10 01:34:12 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-4670ed04-bd93-4773-ad2f-c4d86188e067

C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project>spark-submit "Spark Pipeline.py"
22/06/10 01:35:56 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2412)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2412)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
22/06/10 01:35:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
22/06/10 01:35:57 INFO SparkContext: Running Spark version 3.0.0
22/06/10 01:35:57 INFO ResourceUtils: ==============================================================
22/06/10 01:35:57 INFO ResourceUtils: Resources for spark.driver:

22/06/10 01:35:57 INFO ResourceUtils: ==============================================================
22/06/10 01:35:57 INFO SparkContext: Submitted application: Spark%20Pipeline.py
22/06/10 01:35:57 INFO SecurityManager: Changing view acls to: Malcolm Lewis
22/06/10 01:35:57 INFO SecurityManager: Changing modify acls to: Malcolm Lewis
22/06/10 01:35:57 INFO SecurityManager: Changing view acls groups to:
22/06/10 01:35:57 INFO SecurityManager: Changing modify acls groups to:
22/06/10 01:35:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Malcolm Lewis); groups with view permissions: Set(); users  with modify permissions: Set(Malcolm Lewis); groups with modify permissions: Set()
22/06/10 01:35:59 INFO Utils: Successfully started service 'sparkDriver' on port 51497.
22/06/10 01:35:59 INFO SparkEnv: Registering MapOutputTracker
22/06/10 01:35:59 INFO SparkEnv: Registering BlockManagerMaster
22/06/10 01:35:59 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
22/06/10 01:35:59 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
22/06/10 01:35:59 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
22/06/10 01:35:59 INFO DiskBlockManager: Created local directory at C:\Users\Malcolm Lewis\AppData\Local\Temp\blockmgr-3722b042-cf34-468b-9489-ad3bd9fc8e9c
22/06/10 01:35:59 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
22/06/10 01:35:59 INFO SparkEnv: Registering OutputCommitCoordinator
22/06/10 01:35:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
22/06/10 01:35:59 INFO Utils: Successfully started service 'SparkUI' on port 4041.
22/06/10 01:35:59 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://MalcolmSurfacePro4.myfiosgateway.com:4041
22/06/10 01:35:59 INFO Executor: Starting executor ID driver on host MalcolmSurfacePro4.myfiosgateway.com
22/06/10 01:35:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51520.
22/06/10 01:35:59 INFO NettyBlockTransferService: Server created on MalcolmSurfacePro4.myfiosgateway.com:51520
22/06/10 01:35:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
22/06/10 01:35:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 51520, None)
22/06/10 01:35:59 INFO BlockManagerMasterEndpoint: Registering block manager MalcolmSurfacePro4.myfiosgateway.com:51520 with 366.3 MiB RAM, BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 51520, None)
22/06/10 01:35:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 51520, None)
22/06/10 01:35:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, MalcolmSurfacePro4.myfiosgateway.com, 51520, None)
22/06/10 01:36:00 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/C:/Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/spark-warehouse').
22/06/10 01:36:00 INFO SharedState: Warehouse path is 'file:/C:/Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/spark-warehouse'.
22/06/10 01:36:01 INFO InMemoryFileIndex: It took 32 ms to list leaf files for 1 paths.
22/06/10 01:36:01 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
22/06/10 01:36:03 INFO FileSourceStrategy: Pruning directories with:
22/06/10 01:36:03 INFO FileSourceStrategy: Pushed Filters:
22/06/10 01:36:03 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
22/06/10 01:36:03 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
22/06/10 01:36:04 INFO CodeGenerator: Code generated in 266.1045 ms
22/06/10 01:36:04 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 285.2 KiB, free 366.0 MiB)
22/06/10 01:36:04 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 366.0 MiB)
22/06/10 01:36:04 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:51520 (size: 23.9 KiB, free: 366.3 MiB)
22/06/10 01:36:04 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
22/06/10 01:36:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 01:36:05 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
22/06/10 01:36:05 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/10 01:36:05 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
22/06/10 01:36:05 INFO DAGScheduler: Parents of final stage: List()
22/06/10 01:36:05 INFO DAGScheduler: Missing parents: List()
22/06/10 01:36:05 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 01:36:05 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.7 KiB, free 366.0 MiB)
22/06/10 01:36:05 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 366.0 MiB)
22/06/10 01:36:05 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:51520 (size: 5.3 KiB, free: 366.3 MiB)
22/06/10 01:36:05 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1200
22/06/10 01:36:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 01:36:05 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
22/06/10 01:36:05 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7781 bytes)
22/06/10 01:36:05 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
22/06/10 01:36:05 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/10 01:36:05 INFO CodeGenerator: Code generated in 13.6281 ms
22/06/10 01:36:05 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1673 bytes result sent to driver
22/06/10 01:36:05 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 435 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/10 01:36:05 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
22/06/10 01:36:05 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 0.608 s
22/06/10 01:36:05 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
22/06/10 01:36:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
22/06/10 01:36:05 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 0.778054 s
22/06/10 01:36:05 INFO CodeGenerator: Code generated in 11.9013 ms
22/06/10 01:36:05 INFO FileSourceStrategy: Pruning directories with:
22/06/10 01:36:05 INFO FileSourceStrategy: Pushed Filters:
22/06/10 01:36:05 INFO FileSourceStrategy: Post-Scan Filters:
22/06/10 01:36:05 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
22/06/10 01:36:05 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 285.2 KiB, free 365.7 MiB)
22/06/10 01:36:05 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.7 MiB)
22/06/10 01:36:05 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:51520 (size: 23.9 KiB, free: 366.2 MiB)
22/06/10 01:36:05 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
22/06/10 01:36:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 01:36:06 INFO FileSourceStrategy: Pruning directories with:
22/06/10 01:36:06 INFO FileSourceStrategy: Pushed Filters:
22/06/10 01:36:06 INFO FileSourceStrategy: Post-Scan Filters:
22/06/10 01:36:06 INFO FileSourceStrategy: Output Data Schema: struct<Date: string>
22/06/10 01:36:06 INFO CodeGenerator: Code generated in 9.234 ms
22/06/10 01:36:06 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 285.2 KiB, free 365.4 MiB)
22/06/10 01:36:06 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.4 MiB)
22/06/10 01:36:06 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:51520 (size: 23.9 KiB, free: 366.2 MiB)
22/06/10 01:36:06 INFO SparkContext: Created broadcast 3 from showString at NativeMethodAccessorImpl.java:0
22/06/10 01:36:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 01:36:06 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
22/06/10 01:36:06 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/10 01:36:06 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)
22/06/10 01:36:06 INFO DAGScheduler: Parents of final stage: List()
22/06/10 01:36:06 INFO DAGScheduler: Missing parents: List()
22/06/10 01:36:06 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 01:36:06 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 12.1 KiB, free 365.4 MiB)
22/06/10 01:36:06 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 365.4 MiB)
22/06/10 01:36:06 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:51520 (size: 6.2 KiB, free: 366.2 MiB)
22/06/10 01:36:06 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1200
22/06/10 01:36:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 01:36:06 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
22/06/10 01:36:06 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7781 bytes)
22/06/10 01:36:06 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
22/06/10 01:36:06 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/10 01:36:06 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1500 bytes result sent to driver
22/06/10 01:36:06 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 53 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/10 01:36:06 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
22/06/10 01:36:06 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.081 s
22/06/10 01:36:06 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
22/06/10 01:36:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
22/06/10 01:36:06 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.091071 s
22/06/10 01:36:06 INFO CodeGenerator: Code generated in 13.9135 ms
+---------+
|     Date|
+---------+
| 1/5/2019|
| 3/8/2019|
| 3/3/2019|
|1/27/2019|
| 2/8/2019|
|3/25/2019|
|2/25/2019|
|2/24/2019|
|1/10/2019|
|2/20/2019|
+---------+

22/06/10 01:36:06 INFO FileSourceStrategy: Pruning directories with:
22/06/10 01:36:06 INFO FileSourceStrategy: Pushed Filters:
22/06/10 01:36:06 INFO FileSourceStrategy: Post-Scan Filters:
22/06/10 01:36:06 INFO FileSourceStrategy: Output Data Schema: struct<Date: string>
22/06/10 01:36:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/06/10 01:36:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
22/06/10 01:36:06 INFO CodeGenerator: Code generated in 19.904 ms
22/06/10 01:36:06 INFO CodeGenerator: Code generated in 10.4899 ms
22/06/10 01:36:06 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 285.2 KiB, free 365.1 MiB)
22/06/10 01:36:06 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 365.1 MiB)
22/06/10 01:36:06 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:51520 (size: 23.9 KiB, free: 366.2 MiB)
22/06/10 01:36:06 INFO SparkContext: Created broadcast 5 from csv at NativeMethodAccessorImpl.java:0
22/06/10 01:36:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/06/10 01:36:06 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
22/06/10 01:36:06 INFO DAGScheduler: Registering RDD 17 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0
22/06/10 01:36:06 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/06/10 01:36:06 INFO DAGScheduler: Final stage: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0)
22/06/10 01:36:06 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
22/06/10 01:36:06 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
22/06/10 01:36:06 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[17] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 01:36:07 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 14.8 KiB, free 365.0 MiB)
22/06/10 01:36:07 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 365.0 MiB)
22/06/10 01:36:07 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:51520 (size: 7.5 KiB, free: 366.2 MiB)
22/06/10 01:36:07 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1200
22/06/10 01:36:07 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[17] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 01:36:07 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
22/06/10 01:36:07 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, PROCESS_LOCAL, 7770 bytes)
22/06/10 01:36:07 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
22/06/10 01:36:07 INFO FileScanRDD: Reading File path: file:///Users/Malcolm%20Lewis/Desktop/Pipeline%20Pet%20Project/supermarket_sales.csv, range: 0-131528, partition values: [empty row]
22/06/10 01:36:07 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1842 bytes result sent to driver
22/06/10 01:36:07 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 161 ms on MalcolmSurfacePro4.myfiosgateway.com (executor driver) (1/1)
22/06/10 01:36:07 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
22/06/10 01:36:07 INFO DAGScheduler: ShuffleMapStage 2 (csv at NativeMethodAccessorImpl.java:0) finished in 0.306 s
22/06/10 01:36:07 INFO DAGScheduler: looking for newly runnable stages
22/06/10 01:36:07 INFO DAGScheduler: running: Set()
22/06/10 01:36:07 INFO DAGScheduler: waiting: Set(ResultStage 3)
22/06/10 01:36:07 INFO DAGScheduler: failed: Set()
22/06/10 01:36:07 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
22/06/10 01:36:07 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 151.2 KiB, free 364.9 MiB)
22/06/10 01:36:07 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 364.8 MiB)
22/06/10 01:36:07 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on MalcolmSurfacePro4.myfiosgateway.com:51520 (size: 54.8 KiB, free: 366.1 MiB)
22/06/10 01:36:07 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1200
22/06/10 01:36:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/06/10 01:36:07 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
22/06/10 01:36:07 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
22/06/10 01:36:07 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
22/06/10 01:36:07 INFO ShuffleBlockFetcherIterator: Getting 1 (156.0 B) non-empty blocks including 1 (156.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
22/06/10 01:36:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 27 ms
22/06/10 01:36:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/06/10 01:36:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
22/06/10 01:36:07 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3)
java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610013606_0003_m_000000_3\part-00000-c1c5ddf8-93d7-4679-bb29-71843e8031e9-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
22/06/10 01:36:07 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610013606_0003_m_000000_3\part-00000-c1c5ddf8-93d7-4679-bb29-71843e8031e9-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

22/06/10 01:36:07 ERROR TaskSetManager: Task 0 in stage 3.0 failed 1 times; aborting job
22/06/10 01:36:07 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
22/06/10 01:36:07 INFO TaskSchedulerImpl: Cancelling stage 3
22/06/10 01:36:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage cancelled
22/06/10 01:36:07 INFO DAGScheduler: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0) failed in 0.318 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610013606_0003_m_000000_3\part-00000-c1c5ddf8-93d7-4679-bb29-71843e8031e9-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
22/06/10 01:36:07 INFO DAGScheduler: Job 2 failed: csv at NativeMethodAccessorImpl.java:0, took 0.708311 s
22/06/10 01:36:07 ERROR FileFormatWriter: Aborting job 4ccb206d-4e94-4e23-bda4-bce9427ce23a.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610013606_0003_m_000000_3\part-00000-c1c5ddf8-93d7-4679-bb29-71843e8031e9-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)
        at scala.Option.foreach(Option.scala:407)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)
        at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)
        at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)
        at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
        at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)
        at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610013606_0003_m_000000_3\part-00000-c1c5ddf8-93d7-4679-bb29-71843e8031e9-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        ... 1 more
Traceback (most recent call last):
  File "C:/Users/Malcolm Lewis/Desktop/Pipeline Pet Project/Spark Pipeline.py", line 36, in <module>
    output.write.csv("filtered.csv")
  File "C:\Python37\lib\site-packages\pyspark\python\lib\pyspark.zip\pyspark\sql\readwriter.py", line 1027, in csv
  File "C:\Python37\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1305, in __call__
  File "C:\Python37\lib\site-packages\pyspark\python\lib\pyspark.zip\pyspark\sql\utils.py", line 131, in deco
  File "C:\Python37\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o29.csv.
: org.apache.spark.SparkException: Job aborted.
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)
        at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)
        at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)
        at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
        at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)
        at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, MalcolmSurfacePro4.myfiosgateway.com, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610013606_0003_m_000000_3\part-00000-c1c5ddf8-93d7-4679-bb29-71843e8031e9-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)
        at scala.Option.foreach(Option.scala:407)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGSchedu22/06/10 01:36:07 INFO SparkContext: Invoking stop() from shutdown hook
ler.scala:2203)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)
        ... 33 more
Caused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project\filtered.csv\_temporary\0\_temporary\attempt_20220610013606_0003_m_000000_3\part-00000-c1c5ddf8-93d7-4679-bb29-71843e8031e9-c000.csv
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
        at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
        at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
        at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        ... 1 more

22/06/10 01:36:07 INFO SparkUI: Stopped Spark web UI at http://MalcolmSurfacePro4.myfiosgateway.com:4041
22/06/10 01:36:07 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
22/06/10 01:36:08 INFO MemoryStore: MemoryStore cleared
22/06/10 01:36:08 INFO BlockManager: BlockManager stopped
22/06/10 01:36:08 INFO BlockManagerMaster: BlockManagerMaster stopped
22/06/10 01:36:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
22/06/10 01:36:08 INFO SparkContext: Successfully stopped SparkContext
22/06/10 01:36:08 INFO ShutdownHookManager: Shutdown hook called
22/06/10 01:36:08 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-01694bc1-4ccf-4703-9406-b4fb6aa40604
22/06/10 01:36:08 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-b3616391-c62d-4349-b5e1-9ec3c87d3ff4
22/06/10 01:36:08 INFO ShutdownHookManager: Deleting directory C:\Users\Malcolm Lewis\AppData\Local\Temp\spark-b3616391-c62d-4349-b5e1-9ec3c87d3ff4\pyspark-3fb5c4db-e5b4-4fa8-83a7-0d67ed427243

C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project>^S
'‼' is not recognized as an internal or external command,
operable program or batch file.

C:\Users\Malcolm Lewis\Desktop\Pipeline Pet Project>